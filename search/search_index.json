{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Isca","text":"<p>This website provides instructions for how to run Isca on the high performance computer at St Andrews, kennedy.</p>"},{"location":"CESM/","title":"CESM","text":"<p>This overview goes through how to install CESM version 2.1.3 on Archer2, before outlining the  basics of CESM and how to run a simple simulation.</p> <p>There is a tutorial  which much of this is based on, and provides a lot further information.</p>"},{"location":"CESM/basics/","title":"Basics","text":"<p>This page just gives some useful information regarding the Community Earth System Model.</p>"},{"location":"CESM/basics/#resources","title":"Resources","text":"<ul> <li>Discussion Forum</li> <li>Tutorial (particularly useful)</li> <li>Practical</li> <li>Analysis Example</li> <li>My code to help with analysis </li> </ul>"},{"location":"CESM/basics/#paths-on-archer2","title":"Paths on ARCHER2","text":"<p>Paths pointing to different parts of the CESM model are given below.</p> <p>The paths that are not case specific should be available to use in ARCHER2 after loading the CESM module through <code>module load CESM2/2.1.3</code>.</p> <p></p> <ul> <li><code>$CESM_ROOT = /work/$GROUP/$GROUP/$USER/cesm/CESM2.1.3</code>  This is the overall directory containing all CESM stuff (for me, <code>$GROUP=n02</code> and <code>$USER=jamd</code>, giving: <code>$CESM_ROOT = /work/n02/n02/jamd/cesm/CESM2.1.3</code>)</li> <li><code>$CESMDATA = $CESM_ROOT/cesm_inputdata</code>Path to input data.</li> <li><code>$SRCROOT = $CESM_ROOT/my_cesm_sandbox</code>Path to CESM source code. <code>$CESM_LOC</code> will aslo refer to this directory.</li> <li><code>$CIMEROOT = $CESM_ROOT/my_cesm_sandbox/cime</code>Path to the  Common Infrastructure for Modeling Earth part of the source code.</li> <li><code>$CASEROOT = $CESM_ROOT/runs/$CASE/</code>Path to a particular experiment, as indicated by <code>$CASE</code>.</li> <li><code>$EXEROOT = $CESM_ROOT/runs/$CASE/bld/</code>Path to the build directories of a particular experiment.</li> <li><code>$RUNDIR = $CESM_ROOT/runs/$CASE/run/</code>Path to the run directories of a particular experiment.</li> <li><code>$DOUT_S_ROOT = $CESM_ROOT/archive/$CASE/</code>Path to the archive3d model output.</li> </ul>"},{"location":"CESM/basics/#code-components","title":"Code Components","text":"<p>CESM consists of several sub models listed below.  Output data is saved in a different location for each.</p> <p></p>"},{"location":"CESM/basics/#workflow","title":"Workflow","text":"<p>Here, I go through the general steps  for running an CESM experiment on ARCHER2.</p> <p>At any point, the file <code>$CESM_ROOT/runs/$CASE/CaseStatus</code> records commands run and whether each step has been successful.</p>"},{"location":"CESM/basics/#step-1-login","title":"Step 1 - Login","text":"<p>First, you need to login to ARCHER2 using ssh.</p>"},{"location":"CESM/basics/#step-2-load-modules","title":"Step 2 - Load modules","text":"<p>Each time you login to ARCHER2, you need to load the python and CESM modules:</p> <pre><code>module load cray-python\nmodule load CESM2/2.1.3\n</code></pre>"},{"location":"CESM/basics/#step-3-create-a-case","title":"Step 3 - Create a case","text":"<p>Create a new case using: <pre><code>create_newcase --case $CESM_ROOT/runs/CASE --compset COMPSET --res RES --project PROJECT\n</code></pre></p> <p>where for me, <code>PROJECT=n02-GLOBALEX</code>.  You may need to add <code>--run-unsupported</code> to the end of this command if you get the following error: This compset and grid combination is untested in CESM.</p> Casename convection <p>Experiment casenames have a convention:</p> <pre><code>`&lt;compset char&gt;.&lt;code base&gt;.&lt;compset shortname&gt;.&lt;res shortname&gt;[.opt_desc_string].&lt;nnn&gt;[opt_char]`\n</code></pre> <p>An example <code>$CASE = e.e20.ETEST.f19_g17.test</code>.</p> Compsets <p>Compsets are listed here and the names are  explained here:</p> <p></p> Resolution <p>Resolutions are listed here and the names are  explained here:</p> <p></p>"},{"location":"CESM/basics/#step-4-setup","title":"Step 4 - Setup","text":"<p>Navigate to <code>$CASEROOT</code> and then invoke using <code>case.setup</code>, e.g. for <code>$CASE = e.e20.ETEST.f19_g17.test</code>: <pre><code>cd $CESM_ROOT/runs/e.e20.ETEST.f19_g17.test\n./case.setup\n</code></pre></p>"},{"location":"CESM/basics/#step-5-customize-namelists","title":"Step 5 - Customize namelists","text":"<p>At this stage, you need to specify the details of the experiment by modifying the namelists and/or  customizing the output.</p>"},{"location":"CESM/basics/#step-6-build","title":"Step 6 - Build","text":"<p>Next, the executable should be built through <code>case.build</code>: <pre><code>./case.build\n</code></pre> Again, this should be run from <code>$CASEROOT</code>.</p>"},{"location":"CESM/basics/#step-7-download-input-data","title":"Step 7 - Download input data","text":"<p>Next, the required input data, from which to start the model, should be downloaded: <pre><code>./check_input_data --download\n</code></pre> Again, this should be run from <code>$CASEROOT</code>.</p>"},{"location":"CESM/basics/#step-8-run-model","title":"Step 8 - Run model","text":"<p>Finally, you can run the model with <code>case.submit</code>: <pre><code>./case.submit\n</code></pre> Again, this should be run from <code>$CASEROOT</code>. Details of the experiment may need to be changed  before submission using <code>xmlchange</code>.</p>"},{"location":"CESM/basics/#model-output","title":"Model Output","text":"<p>If the model run is successful, the CESM netcdf output  history files are automatically moved to the short term archive, located at <code>$DOUT_S_ROOT</code>. Otherwise, they are in <code>$RUNDIR</code>.</p> <p>Output files should be moved somewhere else for more long term storage. This is likely to be JASMIN, and the  files can be transferred with globus.</p> <p>Timing information  is saved as <code>$CASEROOT/timing/cesm_timing.$CASE.$date</code>. The model throughput is the estimated number  of model years that you can run in a wallclock day.</p> <p>The <code>cpl.log</code> file at <code>$CESM_ROOT/archive/$CASE/logs</code> indicates whether successful.  It should end with  <code>SUCCESSFUL TERMINATION OF CPL7-cesm</code>.</p>"},{"location":"CESM/basics/#xml-modifications","title":"XML Modifications","text":"<p>Some details of the experiment such as how long to run it, and on which queue to submit to, are  specified with xml variables.  These can be checked with <code>xmlquery</code> and modified with <code>xmlchange</code> from the  <code>$CASEROOT</code> directory. This should be done just before running the experiment.</p> <p>You can check  all variables containing the word <code>VAR</code> using <code>-p</code> for a partial match:</p> <pre><code>./xmlquery -p VAR\n</code></pre> <p>You can change the value of variable <code>VAR</code> to the new value of <code>new_val</code> using:</p> <pre><code>./xmlchange VAR=new_val\n</code></pre> Useful Variables <ul> <li><code>JOB_WALLCLOCK_TIME</code> - Max time to run job for. Must be less than max walltime of chosen <code>JOB_QUEUE</code>. This is listed on ARCHER2 website.</li> <li><code>STOP_N</code> - Experiment will end after <code>STOP_N</code> <code>STOP_OPTION</code>.</li> <li><code>STOP_OPTION</code> - Unit of simulation time to indicate how long to run for e.g. <code>nmonths</code>, <code>nyears</code> or <code>ndays</code>.</li> <li><code>JOB_QUEUE</code> - Which queue to submit to. Most common on ARCHER2  are <code>standard</code> or <code>short</code>.</li> <li><code>CONTINUE_RUN</code> - <code>TRUE</code> to continue run from last restart file.</li> <li><code>RESUBMIT</code> - Need to use  if experiment takes longer than max job time on partition being used.</li> </ul>"},{"location":"CESM/basics/#restarting","title":"Restarting","text":"<p>Restart files  are written according to the <code>REST_OPTION</code> and <code>REST_N</code> settings.  By default, this is set to be the same as <code>$STOP_OPTION</code> and <code>$STOP_N</code> i.e. one restart file per run.</p> <p>The restart files are saved as <code>$DOUT_S_ROOT/rest/yyyy-mm-dd-ssss/</code>.</p> <p>To carry on running a model  from a restart file, you need to set <code>CONTINUE_RUN=TRUE</code> using <code>xmlchange</code>. By default, it is <code>FALSE</code>, in which  case the experiment would just be run from the beginning again.</p> <p>Rather than conitnuing a run from when the last job finished, if you want to  restart from a specific point,  you can move the restart file into the <code>$RUNDIR</code>.</p>"},{"location":"CESM/basics/#namelists","title":"Namelists","text":"<p>Namelists  can be modified through the <code>user_nl_xxx</code> files in <code>$CASEROOT</code>:</p> <p></p> <p>This is where you modify details of the simulation e.g. \\(CO_2\\) concentration.</p>"},{"location":"CESM/basics/#customizing-output","title":"Customizing Output","text":"<p>By default, the simulation will just output  the monthly average of default variables.</p> <p>Within the <code>user_nl_xxx</code> files, there are three namelist variables which allow you to  change output frequency  (<code>nhtfrq</code>) e.g. to daily average, as well as add extra variables or history files  (<code>fincl</code>).</p> <p>The <code>print_ds_var_list</code> function is quite useful  for checking which variables are in the CESM default output, and thus to decide which to output at a different  frequency.</p> Example <p>Below I go through how to run an experiment called <code>e.e20.ETEST.f19_g17.test_daily_output</code> for 40 days while outputting the daily average of the following in <code>h1</code> history files which contain 10 days each:</p> <ul> <li><code>T</code>: Temperature</li> <li><code>TS</code>: Surface temperature</li> <li><code>Q</code>: Specific humidity</li> <li><code>Z3</code>: Geopotential height</li> <li><code>LHFLX</code>: Surface latent heat flux</li> <li><code>SHFLX</code>: Surface sensible heat flux</li> <li><code>FSNS</code>: Net solar flux at the surface</li> <li><code>FLNS</code>: Net longwave flux at the surface</li> <li><code>U10</code>: 10m wind speed</li> </ul> <p>Go through up to step 5 as normal: <pre><code>$CIMEROOT/scripts/create_newcase --case $CESM_ROOT/runs/e.e20.ETEST.f19_g17.test_daily_output --compset ETEST --res f19_g17 --project n02-GLOBALEX --run-unsupported\ncd $CESM_ROOT/runs/e.e20.ETEST.f19_g17.test_daily_output\n./case.setup\n</code></pre></p> <p>Now customize <code>$CASE_ROOT/user_nl_cam</code> to include the variables: <pre><code>! Users should add all user specific namelist changes below in the form of \n! namelist_var = new_namelist_value \n\nnhtfrq = 0, -24         ! Monthly average for default h0 file, daily average for h1 file\nmfilt = 1, 10           ! 1 file per month for h0, 1 file per 10 days for h1 file\nfincl2 = 'T', 'Q', 'Z3', 'TS', 'LHFLX', 'SHFLX', 'FSNS', 'FLNS', 'U10' ! Variables to save daily in h1\n</code></pre></p> <p>Continue the rest of the pipeline as normal: <pre><code>./case.build\n./check_input_data --download\n./xmlchange STOP_N=40\n./xmlchange STOP_OPTION=ndays\n./case.submit\n</code></pre></p> <p>This produces files such as <code>e.e20.ETEST.f19_g17.test_daily_output.cam.h1.0001-01-01-00000.nc</code>  in <code>$DOUT_S_ROOT</code> with the <code>h1</code> history file indicator containing the output for daily data.</p> <p>This example only changes atmospheric variables, but you can do similar things  for the other components.</p> <p>The namelist files should be edited after setup but before build. Note that the  <code>_in</code> files only appear in <code>$CASEROOT</code> after <code>./case.build</code> and these should not be edited.</p> <p>Optionally, can run <code>./preview_namelists</code> from <code>$CASEROOT</code> after editing namelists, but this is done anyway in  <code>./case.build</code>. But if you have changed the namelists and then want to continue the same run, you can just run <code>./preview_namelists</code> followed by <code>./case.submit</code> with <code>CONTINUE_RUN=TRUE</code> to continue an experiment  with modified namelists.</p> <p>Warning</p> <p>Note that you cannot change history options (i.e. customize output) on a restart and instead must do a branch run.</p>"},{"location":"CESM/basics/#branch-run","title":"Branch Run","text":"<p>This section describes how to do a branch run.  This will take an experiment you have already run at a particular point  in time as a starting condition, then modify the experiment somehow e.g. change \\(CO_2\\) concentration, and continue the run.</p> <p>The workflow for this is exactly the same as you carried out for the experiment you want to branch off  from up until Step 5, except the case name should be different. E.g. if the initial experiment was called <code>e.e20.E1850TEST.f09_g17.test</code>, the branched experiment may be called <code>e.e20.E1850TEST.f09_g17.branch</code>.</p> <p>Step 5 is where you make this experiment different from the experiment you have already run, e.g. change \\(CO_2\\) concentration through the variable <code>co2vmr</code> or <code>co2_vmr_rad</code> in the <code>user_nml_cam</code> namelist file.</p> <p>Next, you should build the executable as usual, but you can skip downloading the input data because you are starting from a restart file, not input data.</p>"},{"location":"CESM/basics/#branch-point","title":"Branch Point","text":"<p>At this stage, you need to  specify the branch point  by moving the relevant restart files into <code>$RUNDIR</code>.</p> <p>To restart from the date <code>yyyy-mm-dd-ssss</code>, all the files in the directory <code>$DOUT_S_ROOT/rest/yyyy-mm-dd-ssss/</code> of the initial experiment should be moved to <code>$RUNDIR</code> of the new experiment</p> Example <p>If I want to create a branch called <code>e.e20.E1850TEST.f09_g17.branch</code> from 1st January Year 11 of experiment called <code>e.e20.E1850TEST.f09_g17.test</code>, then I would move all the files in the directory  <code>$CESM_ROOT/archive/e.e20.E1850TEST.f09_g17.test/rest/0011-01-01-00000/</code> to <code>$RUNDIR = $CESM_ROOT/runs/e.e20.E1850TEST.f09_g17.branch/run/</code>.</p> <p>The <code>$RUNDIR</code> before and after this transfer is shown below. Afterwards, there are .nc and rcpointer files in the <code>$RUNDIR</code>.</p> BeforeAfter <p></p> <p></p> <p>Once the restart files have been transferred, <code>xmlchange</code> must be used to indicate that this experiment is a branch run. If we are branching off from an experiment with casename <code>old_case</code> at the date <code>yyyy-mm-dd</code>, then you should run:</p> <pre><code>./xmlchange RUN_TYPE=branch\n./xmlchange RUN_REFCASE=old_case\n./xmlchange RUN_REFDATE=yyyy-mm-dd\n./xmlchange GET_REFCASE=FALSE\n</code></pre> Example <p>Continuing from our previous example, you would run:</p> <pre><code>./xmlchange RUN_TYPE=branch\n./xmlchange RUN_REFCASE=e.e20.E1850TEST.f09_g17.test\n./xmlchange RUN_REFDATE=0011-01-01\n./xmlchange GET_REFCASE=FALSE\n</code></pre> <p>After this, the branch job can be submitted as normal, remembering to  specify the run duration etc.</p>"},{"location":"CESM/first_run/","title":"First Run","text":"<p>Here I go through a step by step example of running a slab ocean experiment, with present day initialization. I  use the <code>ETEST</code> compset with <code>f19_g17</code> resolution.</p> Details of <code>ETEST</code> and <code>f19_g17</code> ETESTf19_g17 <p></p> <p></p>"},{"location":"CESM/first_run/#step-1-login","title":"Step 1 - Login","text":"<p>After logging in to ARCHER2, you should get a welcome message.</p> Terminal Output <pre><code>Last login: Tue Nov  5 15:59:18 2024 from 2.98.194.178\n#######################################################################################\n\n        @@@@@@@@@\n     @@@         @@@            _      ____     ____   _   _   _____   ____    ____\n   @@@    @@@@@    @@@         / \\    |  _ \\   / ___| | | | | | ____| |  _ \\  |___ \\\n  @@@   @@     @@   @@@       / _ \\   | |_) | | |     | |_| | |  _|   | |_) |   __) |\n  @@   @@  @@@  @@   @@      / ___ \\  |  _ &lt;  | |___  |  _  | | |___  |  _ &lt;   / __/\n  @@   @@  @@@  @@   @@     /_/   \\_\\ |_| \\_\\  \\____| |_| |_| |_____| |_| \\_\\ |_____|\n  @@@   @@     @@   @@@\n   @@@    @@@@@    @@@       https://www.archer2.ac.uk/support-access/\n     @@@         @@@\n        @@@@@@@@@\n\n -         U K R I         -        E P C C        -         H P E   C r a y         -\n\nHostname:     ln02\nDistribution: SLES 15.4 4\nCPUS:         256\nMemory:       515.3GB\nConfigured:   2024-07-04\n\n######################################################################################\n---------------------------------Welcome to ARCHER2-----------------------------------\n######################################################################################\n\n/usr/bin/manpath: can't set the locale; make sure $LC_* and $LANG are correct\n</code></pre>"},{"location":"CESM/first_run/#step-2-load-modules","title":"Step 2 - Load modules","text":"<p>After successfully loading the modules, there are a few messages printed to terminal.</p> Terminal Output <pre><code>jamd@ln02:~&gt; module load cray-python\njamd@ln02:~&gt; module load CESM2/2.1.3\n\nLmod is automatically replacing \"cce/15.0.0\" with \"gcc/11.2.0\".\n\n\nLmod is automatically replacing \"PrgEnv-cray/8.3.3\" with \"PrgEnv-gnu/8.3.3\".\n\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-mpich/8.1.23\n</code></pre> <p>After this, I make <code>$CESM_ROOT</code> the current directory:</p> <pre><code>jamd@ln02:~&gt; cd $CESM_ROOT\njamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3&gt; \n</code></pre>"},{"location":"CESM/first_run/#step-3-create-a-case","title":"Step 3 - Create a case","text":"<p>To create the case with <code>ETEST</code> compset and <code>f19_g17</code> resolution, I run:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3&gt; $CIMEROOT/scripts/create_newcase --case $CESM_ROOT/runs/e.e20.ETEST.f19_g17.test --compset ETEST --res f19_g17 --project n02-GLOBALEX --run-unsupported\n</code></pre> <p>where I have used the case name of <code>e.e20.ETEST.f19_g17.test</code> following the  convection with the descriptive string <code>test</code>.</p> Terminal Output <p>Following this command, a bunch of stuff will be printed to terminal, the last of which should be: <pre><code>Pes comments: none\n Compset is: 2000_CAM60_CLM50%SP_CICE_DOCN%SOM_MOSART_SGLC_SWAV_TEST \n Grid is: a%1.9x2.5_l%1.9x2.5_oi%gx1v7_r%r05_g%null_w%null_m%gx1v7 \n Components in compset are: ['cam', 'clm', 'cice', 'docn', 'mosart', 'sglc', 'swav', 'sesp', 'drv', 'dart'] \nNo charge_account info available, using value from PROJECT\nNo project info available\ncesm model version found: cesm2.1.3-rc.01\nBatch_system_type is slurm\njob is case.run USER_REQUESTED_WALLTIME None USER_REQUESTED_QUEUE None WALLTIME_FORMAT %H:%M:%S\njob is case.st_archive USER_REQUESTED_WALLTIME None USER_REQUESTED_QUEUE None WALLTIME_FORMAT %H:%M:%S\n Creating Case directory /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test\n</code></pre></p>"},{"location":"CESM/first_run/#step-4-setup","title":"Step 4 - Setup","text":"<p>Now the experiment has been created, we need to make <code>$CESM_ROOT/runs/e.e20.ETEST.f19_g17.test</code> the current directory, before running <code>case.setup</code>:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3&gt; cd $CESM_ROOT/runs/e.e20.ETEST.f19_g17.test\njamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./case.setup\n</code></pre> Terminal Output <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./case.setup\nSetting resource.RLIMIT_STACK to -1 from (8388608, -1)\n/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/env_mach_specific.xml already exists, delete to replace\njob is case.run USER_REQUESTED_WALLTIME None USER_REQUESTED_QUEUE None WALLTIME_FORMAT %H:%M:%S\nCreating batch scripts\nWriting case.run script from input template /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/config/cesm/machines/template.case.run\nCreating file .case.run\nWriting case.st_archive script from input template /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/config/cesm/machines/template.st_archive\nCreating file case.st_archive\nCreating user_nl_xxx files for components and cpl\nIf an old case build already exists, might want to run 'case.build --clean' before building\nYou can now run './preview_run' to get more info on how your case will be run\n</code></pre>"},{"location":"CESM/first_run/#step-5-customize-namelists","title":"Step 5 - Customize namelists","text":"<p>For this example, we keep the default experiment parameters so don't need to  change the namelist files.</p>"},{"location":"CESM/first_run/#step-6-build","title":"Step 6 - Build","text":"<p>Now we build the executable:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./case.build\n</code></pre> <p>This step takes a while (about 5 minutes for this example), but should end with the following message:</p> <pre><code>Time spent not building: 6.407066 sec\nTime spent building: 253.041738 sec\nMODEL BUILD HAS FINISHED SUCCESSFULLY\n</code></pre> Terminal Output <p>A lot is printed to terminal at this stage, the last of which is shown below.</p> <pre><code>Building atm with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/atm.bldlog.241105-175344\nBuilding ice with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/ice.bldlog.241105-175344\nBuilding ocn with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/ocn.bldlog.241105-175344\nBuilding rof with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/rof.bldlog.241105-175344\nBuilding glc with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/glc.bldlog.241105-175344\nBuilding wav with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/wav.bldlog.241105-175344\nBuilding esp with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/esp.bldlog.241105-175344\nsesp built in 4.198223 seconds\nsglc built in 4.202698 seconds\nswav built in 4.218285 seconds\ndocn built in 5.041561 seconds\nComponent rof build complete with 4 warnings\nmosart built in 10.101223 seconds\nComponent ice build complete with 10 warnings\ncice built in 25.621810 seconds\nComponent atm build complete with 109 warnings\ncam built in 101.564669 seconds\nBuilding cesm with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/cesm.bldlog.241105-175344 \nTime spent not building: 6.407066 sec\nTime spent building: 253.041738 sec\nMODEL BUILD HAS FINISHED SUCCESSFULLY\n</code></pre>"},{"location":"CESM/first_run/#step-7-download-input-data","title":"Step 7 - Download Input data","text":"<p>The input data is downloaded with the command:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./check_input_data --download\n</code></pre> Terminal Output <p>In this step, you will get a lot of messages of the form  <code>Model missing file...Trying to download file...using WGET protocol...SUCCESS</code>. </p> <p>An few examples are given below:</p> <pre><code>  Model cam missing file srf_emis_specifier for SOAG = '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_anthro_surface_2000climo_0.9x1.25_c20170608.nc'\nTrying to download file: 'atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_anthro_surface_2000climo_0.9x1.25_c20170608.nc' to path '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_anthro_surface_2000climo_0.9x1.25_c20170608.nc' using WGET protocol.\nSUCCESS\n\n  Model cam missing file srf_emis_specifier for SOAG = '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_bb_surface_2000climo_0.9x1.25_c20170322.nc'\nTrying to download file: 'atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_bb_surface_2000climo_0.9x1.25_c20170322.nc' to path '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_bb_surface_2000climo_0.9x1.25_c20170322.nc' using WGET protocol.\nSUCCESS\n\n  Model cam missing file srf_emis_specifier for SOAG = '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_biogenic_surface_2000climo_0.9x1.25_c20170322.nc'\nTrying to download file: 'atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_biogenic_surface_2000climo_0.9x1.25_c20170322.nc' to path '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_biogenic_surface_2000climo_0.9x1.25_c20170322.nc' using WGET protocol.\nSUCCESS\n</code></pre> <p>This stage will take a while if no input data already exists, on the order of 30 minutes for 1 degree resolution. If the input data was downloaded successfully, running <code>./check_input_data</code> should show the following:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./check_input_data           \nSetting resource.RLIMIT_STACK to -1 from (8388608, -1)\nLoading input file list: 'Buildconf/cice.input_data_list'\nLoading input file list: 'Buildconf/mosart.input_data_list'\nLoading input file list: 'Buildconf/docn.input_data_list'\nLoading input file list: 'Buildconf/cpl.input_data_list'\nLoading input file list: 'Buildconf/clm.input_data_list'\nLoading input file list: 'Buildconf/cam.input_data_list'\n</code></pre>"},{"location":"CESM/first_run/#step-8-run-model","title":"Step 8 - Run model","text":"<p>Before running the model, you need change  the duration of the simulation (default is 5 days),  and specify which partition to submit the job to (default is <code>standard</code>).  Here, I change it to 1 month on the <code>short</code> partition which has a max walltime of 20 minutes:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlchange JOB_QUEUE=short      \njamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlchange JOB_WALLCLOCK_TIME=20:00   \njamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlchange STOP_N=1       \njamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlchange STOP_OPTION=nmonths\n</code></pre> Checking using <code>xmlquery</code> <p>After the above changes, you can use <code>./xmlquery -p JOB</code> to check the <code>JOB_QUEUE</code> and <code>JOB_WALLCLOCK_TIME</code> have  changed: <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlquery -p JOB\n\nResults in group case.run\n        JOB_QUEUE: short\n        JOB_WALLCLOCK_TIME: 20:00\n\nResults in group case.st_archive\n        JOB_QUEUE: short\n        JOB_WALLCLOCK_TIME: 20:00\n\nResults in group run_begin_stop_restart\n        JOB_IDS: \n        JOB_PRIORITY: regular\n</code></pre></p> <p>and <code>./xmlquery -p STOP</code> to check <code>STOP_N</code> and <code>STOP_OPTION</code>:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlquery -p STOP\n\nResults in group run_begin_stop_restart\n        STOP_DATE: -999\n        STOP_N: 1\n        STOP_OPTION: nmonths\n</code></pre> <p>Now we can run the job:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./case.submit \n</code></pre> <p>If successful, you should get information printed to terminal about the job id:</p> <pre><code>Submitted job id is 7977850\nSubmitted job case.run with id 7977849\nSubmitted job case.st_archive with id 7977850\n</code></pre> Terminal Output <p>The full terminal output is:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./case.submit \nSetting resource.RLIMIT_STACK to -1 from (8388608, -1)\nCreating component namelists\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/components/cam//cime_config/buildnml\nCAM namelist copy: file1 /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/Buildconf/camconf/atm_in file2 /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/run/atm_in \n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/components/clm//cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/components/cice//cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/src/components/data_comps/docn/cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/components/mosart//cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/src/components/stub_comps/sglc/cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/src/components/stub_comps/swav/cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/src/components/stub_comps/sesp/cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/src/drivers/mct/cime_config/buildnml\nFinished creating component namelists\nChecking that inputdata is available as part of case submission\nSetting resource.RLIMIT_STACK to -1 from (-1, -1)\nLoading input file list: 'Buildconf/cice.input_data_list'\nLoading input file list: 'Buildconf/mosart.input_data_list'\nLoading input file list: 'Buildconf/docn.input_data_list'\nLoading input file list: 'Buildconf/cpl.input_data_list'\nLoading input file list: 'Buildconf/clm.input_data_list'\nLoading input file list: 'Buildconf/cam.input_data_list'\nCheck case OK\nsubmit_jobs case.run\nSubmit job case.run\nSubmitting job script sbatch --time 20:00 -q short --account n02-GLOBALEX --export=ALL /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/.case.run --resubmit\nSubmitted job id is 7977849\nSubmit job case.st_archive\nSubmitting job script sbatch --time 20:00 -q short --account n02-GLOBALEX --export=ALL  --dependency=afterok:7977849 /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/case.st_archive --resubmit\nSubmitted job id is 7977850\nSubmitted job case.run with id 7977849\nSubmitted job case.st_archive with id 7977850\n</code></pre> <p>The progress of the job can be monitored and managed with the usual slurm commands:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; squeue -u jamd\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           7977850  standard st_archi     jamd PD       0:00      1 (Dependency)\n           7977849  standard run.e.e2     jamd  R       1:52      6 nid[006831,006833,006836,006844-006845,006849]\n</code></pre>"},{"location":"CESM/first_run/#model-output","title":"Model Output","text":"<p>The model output should be located in  <code>$DOUT_S_ROOT = /work/n02/n02/jamd/cesm/CESM2.1.3/archive/e.e20.ETEST.f19_g17.test/</code> if successful with files  corresponding to each of the model components, as well as restart and log files:</p> <p></p>"},{"location":"CESM/first_run/#timing","title":"Timing","text":"<p>Timing information can be found at <code>$CASEROOT/timing/cesm_timing.$CASE.$date</code>. For this experiment,  the file is: </p> <p><code>/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/timing/cesm_timing.e.e20.ETEST.f19_g17.test.7977849.241105-182147</code></p> Timing File <p>The timing file is quite long, the first section is shown below.  The most useful information is in the Overall Metrics section.</p> <pre><code>---------------- TIMING PROFILE ---------------------\n  Case        : e.e20.ETEST.f19_g17.test\n  LID         : 7977849.241105-182147\n  Machine     : archer2\n  Caseroot    : /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test\n  Timeroot    : /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/Tools\n  User        : jamd\n  Curr Date   : Tue Nov  5 18:31:13 2024\n  grid        : a%1.9x2.5_l%1.9x2.5_oi%gx1v7_r%r05_g%null_w%null_m%gx1v7\n  compset     : 2000_CAM60_CLM50%SP_CICE_DOCN%SOM_MOSART_SGLC_SWAV_TEST\n  run_type    : startup, continue_run = FALSE (inittype = TRUE)\n  stop_option : nmonths, stop_n = 1\n  run_length  : 31 days (30.979166666666668 for ocean)\n\n  component       comp_pes    root_pe   tasks  x threads instances (stride) \n  ---------        ------     -------   ------   ------  ---------  ------  \n  cpl = cpl        512         0        512    x 1       1      (1     ) \n  atm = cam        512         0        512    x 1       1      (1     ) \n  lnd = clm        256         0        256    x 1       1      (1     ) \n  ice = cice       256         256      256    x 1       1      (1     ) \n  ocn = docn       256         512      256    x 1       1      (1     ) \n  rof = mosart     256         0        256    x 1       1      (1     ) \n  glc = sglc       128         0        128    x 1       1      (1     ) \n  wav = swav       128         0        128    x 1       1      (1     ) \n  esp = sesp       1           0        1      x 1       1      (1     ) \n\n  total pes active           : 768 \n  mpi tasks per node               : 128 \n  pe count for cost estimate : 768 \n\n  Overall Metrics: \n    Model Cost:             798.38   pe-hrs/simulated_year \n    Model Throughput:        23.09   simulated_years/day \n\n    Init Time   :     234.999 seconds \n    Run Time    :     317.848 seconds       10.253 seconds/day \n    Final Time  :       0.004 seconds \n\n    Actual Ocn Init Wait Time     :     308.436 seconds \n    Estimated Ocn Init Run Time   :       0.001 seconds \n    Estimated Run Time Correction :       0.000 seconds \n      (This correction has been applied to the ocean and total run times)\n</code></pre>"},{"location":"CESM/first_run/#log","title":"Log","text":"<p>The <code>cpl.log</code> file can be found at <code>$CESM_ROOT/archive/$CASE/logs</code>. For this example, the path is:</p> <p><code>/work/n02/n02/jamd/cesm/CESM2.1.3/archive/e.e20.ETEST.f19_g17.test/logs/cpl.log.7977849.241105-182147</code></p> <p>This should end with <code>SUCCESSFUL TERMINATION OF CPL7-cesm.</code> if the simulation ran correctly.</p> Log File <p>The file is very long, but the last SUCCESSFUL part is given below for this experiment:</p> <pre><code>(seq_mct_drv): ===============          SUCCESSFUL TERMINATION OF CPL7-cesm ===============\n(seq_mct_drv): ===============        at YMD,TOD =   00010201       0         ===============\n(seq_mct_drv): ===============  # simulated days (this run) =       31.000  ===============\n(seq_mct_drv): ===============  compute time (hrs)          =        0.088  ===============\n(seq_mct_drv): ===============  # simulated years / cmp-day =       23.087  ===============\n(seq_mct_drv): ===============  pes min memory highwater  (MB)     890.039  ===============\n(seq_mct_drv): ===============  pes max memory highwater  (MB)    7045.794  ===============\n(seq_mct_drv): ===============  pes min memory last usage (MB)      62.108  ===============\n(seq_mct_drv): ===============  pes max memory last usage (MB)     660.740  ===============\n</code></pre>"},{"location":"CESM/first_run/#first-plot","title":"First Plot","text":"<p>After transferring the output data in <code>$DOUT_S_ROOT</code> to JASMIN, the <code>load_dataset</code> function can be used to load the dataset, and  the <code>print_ds_var_list</code> function to find out  what variables are contained in the output data.</p> <p>Below, I load the atmospheric dataset, find the temperature related variables in the Dataset and plot the  surface temperature averaged over the first month.</p> CodeOutputPlot <pre><code>import sys\nsys.path.append('/home/users/jamd1/Isca/')\nimport isca_tools\n# var_keep = ['TS', 'FSNT', 'FLNT', 'gw', 'LHFLX', 'SHFLX', 'FLNS', 'FSNS', 'PRECSC', 'PRECSL']\nexp_name = 'e.e20.ETEST.f19_g17.test'\nds = isca_tools.cesm.load_dataset(exp_name, decode_times=False).load()\nisca_tools.utils.print_ds_var_list(ds, 'temp')\nds.TS.plot()\n</code></pre> <pre><code>RTPTHLP_CLUBB: Temp. Moist. Covariance\nSTEND_CLUBB: Temperature tendency\nT: Temperature\nTHLP2_CLUBB: Temperature Variance\nTREFHT: Reference height temperature\nTS: Surface temperature (radiative)\nTSMN: Minimum surface temperature over output period\nTSMX: Maximum surface temperature over output period\n</code></pre> <p></p> <p>Note that for this to work, the <code>isca_tools</code> directory has the path <code>/home/users/jamd1/Isca/isca_tools</code> on JASMIN.</p>"},{"location":"CESM/installation/","title":"Installation on Archer2","text":"<p>Here, I go through the procedure for installing CESM version 2.1.3 on ARCHER2. These are mainly based on the instructions on the ARCHER2 website  but correct them, so it actually works.</p>"},{"location":"CESM/installation/#step-1-archer2-account","title":"Step 1 - ARCHER2 Account","text":"<p>First, you need to setup an ARCHER2 account and  then connect using ssh.</p>"},{"location":"CESM/installation/#step-2-downloading-cesm-213-and-setting-up-the-directory-structure","title":"Step 2 - Downloading CESM 2.1.3 And Setting Up The Directory Structure","text":"<p>This step is the same as on the ARCHER2 website, i.e. you need to run:</p> <pre><code>module load cray-python\nsource /work/n02/shared/CESM2/setup_cesm213.sh\n</code></pre> <p>This script will create a directory at <code>$CESM_ROOT</code>, defaulting to <code>/work/$GROUP/$GROUP/$USER/cesm/CESM2.1.3</code>, so for me  with <code>$GROUP=n02</code>, and <code>$USER=jamd</code>, <code>$CESM_ROOT = /work/n02/n02/jamd/cesm/CESM2.1.3/</code>. </p> <p>Note <code>$CESM_LOC = $CESM_ROOT/my_cesm_sandbox</code> in what follows.</p>"},{"location":"CESM/installation/#step-3-changes-to-externals-configuration","title":"Step 3 - Changes to Externals Configuration","text":"<p>A couple of sections in the file <code>$CESM_ROOT/my_cesm_sandbox/Externals.cfg</code> need modifying.  First, change the CIME section from:</p> Editing files on ARCHER2 <p>You can open or edit a file using the nano.</p> <ul> <li><code>nano index.html</code> opens the <code>index.html</code> file, which can then be edited. </li> <li><code>Ctrl + O</code> to save the file, and confirm with <code>Enter</code></li> <li><code>Ctrl + X</code> to close <code>nano</code></li> </ul> <p>Alternatively, globus can be used to transfer the file to your local laptop, where you can make the necessary edits, before sending it back to overwrite the file on ARCHER2.</p> <pre><code>[cime]\ntag = cime5.6.32\nprotocol = git\nrepo_url = https://github.com/ESMCI/cime\nlocal_path = cime\nrequired = True\n</code></pre> <p>to (note when I did this, I did not include the <code>externals = Externals_cime.cfg</code> addition):</p> <pre><code>[cime]\nbranch = maint-5.6\nprotocol = git\nrepo_url = https://github.com/ESMCI/cime\nlocal_path = cime\nexternals = Externals_cime.cfg\nrequired = True\n</code></pre> <p>The CAM section in the same file also needs changing. This is the step that was missing from the ARCHER2 website,  but I think it is there  now. This needs changing from:</p> <pre><code>[cam]\ntag = cam_cesm2_1_rel_41\nprotocol = git\nrepo_url = https://github.com/ESCOMP/CAM\nlocal_path = components/cam\nexternals = Externals_CAM.cfg\nrequired = True\n</code></pre> <p>to:</p> <pre><code>[cam]\ntag = cam_cesm2_1_rel\nprotocol = git\nrepo_url = https://github.com/ESCOMP/CAM\nlocal_path = components/cam\nexternals = Externals_CAM.cfg\nrequired = True\n</code></pre>"},{"location":"CESM/installation/#step-4-downloading-components","title":"Step 4 - Downloading Components","text":"<p>Now you can download the external components by running the following commands:</p> <pre><code>cd $CESM_ROOT\n./manage_externals/checkout_externals\n</code></pre> Confirming successful download <p>To confirm a successful download of all components, you can run checkout_externals with the status flag to show the  status of the externals:</p> <pre><code>./manage_externals/checkout_externals -S\n</code></pre> <p>This should show a clean status for all externals, with no characters in the first two columns of output,  as in this example:</p> <p></p> <p>This comes from the CESM website  (but the screenshot is from my installation).</p>"},{"location":"CESM/installation/#step-5-building-cprnc","title":"Step 5 - Building cprnc","text":"<p>cprnc is a generic tool for analyzing a netcdf file or comparing two netcdf files. It is used in various places by CESM  and the source is included with cime. My procedure for building it differs slightly from that on the  ARCHER2 website.</p> <p>First, load <code>CESM2/2.1.3</code> and navigate to <code>$CIMEROOT/tools/cprnc</code>:</p> <pre><code>module load CESM2/2.1.3\ncd $CIMEROOT/tools/\n</code></pre> <p>where <code>$CIMEROOT = $CESM_ROOT/my_cesm_sandbox/cime</code>.  For me, I am in the directory <code>/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/tools</code> at this stage.</p> <p>If the directory <code>$CIMEROOT/tools/cprnc</code> does not exist (it did not for me) then create and enter it:</p> <pre><code>cd $CIMEROOT/tools/\nmkdir cprnc\ncd ./cprnc\n</code></pre> <p>From the <code>$CIMEROOT/tools/cprnc</code> directory, run the following three commands (one after the other):</p> <pre><code>../configure --macros-format=Makefile--mpilib=mpi-serial\nsed -i '/}}/d' .env_mach_specific.sh\nsource ./.env_mach_specific.sh &amp;&amp; make\n</code></pre> <p>I received the following message after the last of these commands, which is expected:</p> <pre><code>The following dependent module(s) are not currently loaded: cray-hdf5-parallel (required by: CESM2/2.1.3), cray-netcdf-hdf5parallel (required by: CESM2/2.1.3), cray-parallel-netodf (required by: CESM2/2.1.3)\n</code></pre> <p>Note that this step is not essential, so you can proceed to the next step if you encountered some issues here.</p>"},{"location":"CESM/installation/#step-6-changing-input-data-configuration","title":"Step 6 - Changing Input Data Configuration","text":"<p>When I first installed CESM on ARCHER2, I kept getting errors when trying to  download input data. These errors arose due to a problem with globus.</p> <p>I managed to fix it by changing the order so the <code>wget</code> protocol is used instead. This is done by modifying the  <code>$CIMEROOT/config/cesm/config_inputdata.xml</code> file as indicated below:</p> BeforeAfter <pre><code>&lt;?xml version=\"1.0\"?&gt;\n\n&lt;inputdata&gt;\n  &lt;!-- server precidence is order in this file.  Highest preference at top --&gt;\n  &lt;!-- If the client doesn't have the protocol it will be skipped --&gt;\n  &lt;!-- chksum verification of inputfiles is possible.  If a file with name --&gt;\n  &lt;!-- inputdata_chksum.dat is found on the server in the directory above inputdata --&gt;\n  &lt;!-- it will be searched for filename and chksum of each downloaded file.  --&gt;\n  &lt;!-- see the file ftp://ftp.cgd.ucar.edu/cesm/inputdata_chksum.dat for proper format. --&gt;\n  &lt;server&gt;\n    &lt;comment&gt;grid ftp requires the globus-url-copy tool on the client side &lt;/comment&gt;\n    &lt;protocol&gt;gftp&lt;/protocol&gt;\n    &lt;address&gt;ftp://gridanon.cgd.ucar.edu:2811/cesm/inputdata/&lt;/address&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;protocol&gt;wget&lt;/protocol&gt;\n    &lt;address&gt;ftp://ftp.cgd.ucar.edu/cesm/inputdata/&lt;/address&gt;\n    &lt;user&gt;anonymous&lt;/user&gt;\n    &lt;password&gt;user@example.edu&lt;/password&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;comment&gt; ftp requires the python package ftplib &lt;/comment&gt;\n    &lt;protocol&gt;ftp&lt;/protocol&gt;\n    &lt;address&gt;ftp.cgd.ucar.edu/cesm/inputdata&lt;/address&gt;\n    &lt;user&gt;anonymous&lt;/user&gt;\n    &lt;password&gt;user@example.edu&lt;/password&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;protocol&gt;svn&lt;/protocol&gt;\n    &lt;address&gt;https://svn-ccsm-inputdata.cgd.ucar.edu/trunk/inputdata&lt;/address&gt;\n  &lt;/server&gt;\n\n&lt;/inputdata&gt;\n</code></pre> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n\n&lt;inputdata&gt;\n  &lt;!-- server precidence is order in this file.  Highest preference at top --&gt;\n  &lt;!-- If the client doesn't have the protocol it will be skipped --&gt;\n  &lt;!-- chksum verification of inputfiles is possible.  If a file with name --&gt;\n  &lt;!-- inputdata_chksum.dat is found on the server in the directory above inputdata --&gt;\n  &lt;!-- it will be searched for filename and chksum of each downloaded file.  --&gt;\n  &lt;!-- see the file ftp://ftp.cgd.ucar.edu/cesm/inputdata_chksum.dat for proper format. --&gt;\n  &lt;server&gt;\n    &lt;protocol&gt;wget&lt;/protocol&gt;\n    &lt;address&gt;ftp://ftp.cgd.ucar.edu/cesm/inputdata/&lt;/address&gt;\n    &lt;user&gt;anonymous&lt;/user&gt;\n    &lt;password&gt;user@example.edu&lt;/password&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;comment&gt; ftp requires the python package ftplib &lt;/comment&gt;\n    &lt;protocol&gt;ftp&lt;/protocol&gt;\n    &lt;address&gt;ftp.cgd.ucar.edu/cesm/inputdata&lt;/address&gt;\n    &lt;user&gt;anonymous&lt;/user&gt;\n    &lt;password&gt;user@example.edu&lt;/password&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;protocol&gt;svn&lt;/protocol&gt;\n    &lt;address&gt;https://svn-ccsm-inputdata.cgd.ucar.edu/trunk/inputdata&lt;/address&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;comment&gt;grid ftp requires the globus-url-copy tool on the client side &lt;/comment&gt;\n    &lt;protocol&gt;gftp&lt;/protocol&gt;\n    &lt;address&gt;ftp://gridanon.cgd.ucar.edu:2811/cesm/inputdata/&lt;/address&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n&lt;/inputdata&gt;\n</code></pre> <p>Once this step has been completed, you are ready to run a simple test case.</p>"},{"location":"Isca/benchmarking/","title":"Benchmarking","text":"<p>To see how the number of cores per node and the resolution of the simulation affect how long the experiments  take to run, you can run the  same simulation with different parameters.</p>"},{"location":"Isca/benchmarking/#scripts","title":"Scripts","text":"<p>To do this, I created the following files and added them to the folder <code>/gpfs1/home/jamd1/isca_jobs/benchmarking</code>:</p> benchmark_run.pybenchmark.shrecord_time.pyexperiment/held_suarez_test_case.py <pre><code>import os\nfrom os import system\nimport sys\nimport numpy as np\nimport re\nfrom typing import Tuple\n\n# This script runs a single experiment with different resolution and number of cores. It then records the time\n# taken for each.    \n\n# Things to change\nexperiment = 'held_suarez_test_case.py'  # python script to call is this in the benchmarking/experiment folder\nn_months = 1  # duration of simulation\nn_nodes = 1  # how many nodes to run on\nn_cores_list = [8, 16, 32]  # cores per node to iterate over, all would be [8, 16, 32]\nres_list = ['T21', 'T42', 'T85']  # horizontal resolution to iterate over, all would be ['T21', 'T42', 'T85']\ncsv_file = 'task_times.csv'  # where to save how long each task took.\n\n\ndef extract_number(f: str) -&gt; Tuple[int, str]:\n    # returns last number of string. If is no number, will return -1.\n    s = re.findall(r'\\d+', f)\n    return int(s[-1]) if s else -1, f\n\n\ndef get_max_folder_number(dir: str) -&gt; int:\n    # Finds maximum number at end of folder within the directory dir.\n    # Will be -1 if no folders end in a number or if directory does not exist.\n    if os.path.exists(dir):\n        max_index = extract_number(max(os.listdir(dir), key=extract_number))[0]\n    else:\n        max_index = -1\n    return max_index\n\n\n# get index of first iteration so does not overwrite previous data\n# first index would be 1 if experiment not yet run\nshell_script = 'benchmark.sh'  # bash script to call experiment\nexperiment = experiment.replace('.py', '')  # make sure experiment name does not have .py suffix.\npython_script = os.path.join('experiments', experiment+'.py')\n# error .txt files saved as output/experiment/error{ind}.txt and similar for the output file\nstarting_ind = int(np.clip(get_max_folder_number(f'output/{experiment}') + 1, 1, np.inf))\n\nif n_nodes == 1 and n_months &lt;= 3:\n    # If 1 node use debug partition as quicker\n    partition = 'debug'\nelse:\n    # If more than 1 node, need to use other partition but will take much longer to run as need to queue.\n    partition = 'parallel'\n\n# Iterate over all n_cores and resolutions provided\nind = starting_ind\nfor n_cores in n_cores_list:\n    for res in res_list:\n        output_file = f'output/{experiment}/output_run{ind}.txt'  # name of file containing stuff printed to console\n        error_file = f'output/{experiment}/error_run{ind}.txt'  # name of file containing errors printed to console\n        system(f'bash {shell_script} {python_script} {ind} {partition} {n_nodes} {n_cores} {res} {n_months} {csv_file} '\n               f'{output_file} {error_file}')\n        ind += 1\n</code></pre> <pre><code>#!/bin/bash\nsbatch &lt;&lt;EOT\n#!/bin/bash\n#SBATCH --job-name=$(basename \"$1\" .py)  # make job name be the same as the python script without prefix and suffix.\n#SBATCH --output=$9  # output to console saved as text file\n#SBATCH --error=${10}    # errors to console saved as text file\n#SBATCH --time=02:00:00 # maximum walltime for the job\n#SBATCH --nodes=$4 # specify number of nodes\n#SBATCH --ntasks-per-node=$5 # specify number of processors per node\n#SBATCH --mail-type=END # send email at job completion\n#SBATCH --mail-user=jamd1@st-andrews.ac.uk # email address\n#SBATCH --partition=$3 # queue to run on\n\n# Input parameters\n# $1 - python script indicating experiment to run e.g. experiments/held_suarez_test_case.py\n# $2 - index giving the run number for this experiment\n# $3 - partition indicating which queue to run experiment on e.g. 'debug' or 'singlenode'\n# $4 - n_nodes indicating number of nodes to run experiment on.\n# $5 - n_cores indicating the number of cores to use per node.\n# $6 - res indicating horizontal resolution to use for the experiment e.g. 'T42'\n# $7 - n_months indicates duration of the simulation in months.\n# $8 - csv file to save task times e.g. 'task_times.csv'\n# $9 - name of txt file where stuff printed to console saved e.g. console_output.txt\n# ${10} - name of txt file where errors printed to console saved e.g. console_error.txt\n\n# Save some of input parameters for use in python scripts\nexport NMONTHS=$7\nexport RES=$6\nexport RUN_NO=$2\n\n# Run python script for experiment and record how long it takes\ncd /home/jamd1/isca_jobs/benchmarking/\npython record_time.py $8 START\npython $1\npython record_time.py $8 END\n\nexit 0\nEOT\n</code></pre> <pre><code>import sys as sys\nimport os\nimport pandas as pd\nfrom datetime import datetime\n\n# This records the time and environmental variables into a csv file for a given Isca experiment.\n\nreformatted_GMT_timestamp = datetime.utcnow().strftime('[%Y,%m,%d,%H,%M,%S]')\nfile_name = sys.argv[1]    # where csv file with task info is saved\ntask_status = sys.argv[2]  # either start or end\n\nif task_status.lower() == 'start':\n    # Initialise dictionary with all things that we want to save\n    task_dict = {'Job Name': [os.environ['SLURM_JOB_NAME']],\n                 'Run Number': [int(os.environ['RUN_NO'])],\n                 'Partition': [os.environ['SLURM_JOB_PARTITION']],\n                 'Resolution': [os.environ['RES']],\n                 'Number of Nodes': [int(os.environ['SLURM_NNODES'])],\n                 'Tasks Per Node': [int(os.environ['SLURM_NTASKS_PER_NODE'])],\n                 'Simulation Duration (Months)': [int(os.environ['NMONTHS'])],\n                 'Start Time (Y,M,D,H,M,S)': [reformatted_GMT_timestamp],\n                 'End Time (Y,M,D,H,M,S)': [None],  # Set when call with 'end'\n                 'Time/Seconds': [None]}            # Set when call with 'end'\n    if os.path.exists(file_name):\n        # If file exists, append to existing dataframe.\n        df = pd.read_csv(file_name)\n        df = pd.concat([df, pd.DataFrame(data=task_dict)], ignore_index=True)\n    else:\n        # If no file, create new dataframe.\n        df = pd.DataFrame(data=task_dict)\nelse:\n    # If task finished, only record end time and duration of task\n    df = pd.read_csv(file_name)\n    df.loc[df.shape[0] - 1, 'End Time (Y,M,D,H,M,S)'] = reformatted_GMT_timestamp\n    start_time = datetime.strptime(df.loc[df.shape[0] - 1, 'Start Time (Y,M,D,H,M,S)'], \"[%Y,%m,%d,%H,%M,%S]\")\n    end_time = datetime.strptime(reformatted_GMT_timestamp, \"[%Y,%m,%d,%H,%M,%S]\")\n    df.loc[df.shape[0] - 1, 'Time/Seconds'] = int((end_time-start_time).total_seconds())\ndf.to_csv(file_name, index=False)  # save dataframe to a .csv file\n</code></pre> <pre><code>import numpy as np\nimport os\nfrom isca import DryCodeBase, DiagTable, Experiment, Namelist, GFDL_BASE\n\n# MODIFIED version of held_suarez_test_case.py script to allow for variables to be inherited\n# from the environment and to run for specified length of time.\n\n# Things that were set in benchmark.sh script - MODIFIED\nNCORES = int(os.environ[\"SLURM_NTASKS_PER_NODE\"])\nNMONTHS = int(os.environ['NMONTHS']) + 1  # +1 because does first month automatically\nRESOLUTION = os.environ['RES'], 25  # horizontal resolution, 25 levels in pressure\n\n# a CodeBase can be a directory on the computer,\n# useful for iterative development\ncb = DryCodeBase.from_directory(GFDL_BASE)\n\n# or it can point to a specific git repo and commit id.\n# This method should ensure future, independent, reproducibility of results.\n# cb = DryCodeBase.from_repo(repo='https://github.com/isca/isca', commit='isca1.1')\n\n# compilation depends on computer specific settings.  The $GFDL_ENV\n# environment variable is used to determine which `$GFDL_BASE/src/extra/env` file\n# is used to load the correct compilers.  The env file is always loaded from\n# $GFDL_BASE and not the checked out git repo.\n\ncb.compile()  # compile the source code to working directory $GFDL_WORK/codebase\n\n# create an Experiment object to handle the configuration of model parameters\n# and output diagnostics\n\nexp_name = os.environ['SLURM_JOB_NAME']  # MODIFIED - this always used to be held_suarez_default\nexp = Experiment(exp_name, codebase=cb)\n\n#Tell model how to write diagnostics\ndiag = DiagTable()\ndiag.add_file('atmos_monthly', 30, 'days', time_units='days')\n\n#Tell model which diagnostics to write\ndiag.add_field('dynamics', 'ps', time_avg=True)\ndiag.add_field('dynamics', 'bk')\ndiag.add_field('dynamics', 'pk')\ndiag.add_field('dynamics', 'ucomp', time_avg=True)\ndiag.add_field('dynamics', 'vcomp', time_avg=True)\ndiag.add_field('dynamics', 'temp', time_avg=True)\ndiag.add_field('dynamics', 'vor', time_avg=True)\ndiag.add_field('dynamics', 'div', time_avg=True)\n\nexp.diag_table = diag\n\n# define namelist values as python dictionary\n# wrapped as a namelist object.\nnamelist = Namelist({\n    'main_nml': {\n        'dt_atmos': 600,\n        'days': 30,\n        'calendar': 'thirty_day',\n        'current_date': [2000,1,1,0,0,0]\n    },\n\n    'atmosphere_nml': {\n        'idealized_moist_model': False  # False for Newtonian Cooling.  True for Isca/Frierson\n    },\n\n    'spectral_dynamics_nml': {\n        'damping_order'           : 4,                      # default: 2\n        'water_correction_limit'  : 200.e2,                 # default: 0\n        'reference_sea_level_press': 1.0e5,                  # default: 101325\n        'valid_range_t'           : [100., 800.],           # default: (100, 500)\n        'initial_sphum'           : 0.0,                  # default: 0\n        'vert_coord_option'       : 'uneven_sigma',         # default: 'even_sigma'\n        'scale_heights': 6.0,\n        'exponent': 7.5,\n        'surf_res': 0.5\n    },\n\n    # configure the relaxation profile\n    'hs_forcing_nml': {\n        't_zero': 315.,    # temperature at reference pressure at equator (default 315K)\n        't_strat': 200.,   # stratosphere temperature (default 200K)\n        'delh': 60.,       # equator-pole temp gradient (default 60K)\n        'delv': 10.,       # lapse rate (default 10K)\n        'eps': 0.,         # stratospheric latitudinal variation (default 0K)\n        'sigma_b': 0.7,    # boundary layer friction height (default p/ps = sigma = 0.7)\n\n        # negative sign is a flag indicating that the units are days\n        'ka':   -40.,      # Constant Newtonian cooling timescale (default 40 days)\n        'ks':    -4.,      # Boundary layer dependent cooling timescale (default 4 days)\n        'kf':   -1.,       # BL momentum frictional timescale (default 1 days)\n\n        'do_conserve_energy':   True,  # convert dissipated momentum into heat (default True)\n    },\n\n    'diag_manager_nml': {\n        'mix_snapshot_average_fields': False\n    },\n\n    'fms_nml': {\n        'domains_stack_size': 600000                        # default: 0\n    },\n\n    'fms_io_nml': {\n        'threading_write': 'single',                         # default: multi\n        'fileset_write': 'single',                           # default: multi\n    }\n})\n\nexp.namelist = namelist\nexp.set_resolution(*RESOLUTION)\n\n#Lets do a run!\nif __name__ == '__main__':\n    # MODIFIED - used to be for 1 year with overwrite_data=False\n    # But we are running same experiment multiple times so need to overwrite.\n    exp.run(1, num_cores=NCORES, use_restart=False, overwrite_data=True)\n    for i in range(2, NMONTHS):\n        exp.run(i, num_cores=NCORES, overwrite_data=True)  # use the restart i-1 by default\n</code></pre>"},{"location":"Isca/benchmarking/#experiment-script","title":"Experiment script","text":"<p>The <code>held_suarez_test_case</code> script had to be modified from its  original version to vary the <code>NCORES</code> and <code>RESOLUTION</code> parameters which are hard coded.</p> <p>All changes are indicated by a comment containing <code># MODIFIED</code>. The other changes are to change the duration of the simulation and overwrite any existing data in the folder  <code>/gpfs1/scratch/jamd1/isca_output/held_suarez_test_case/run0001</code>.</p> <p>Other test scripts also need to be changed in a  similar fashion to benchmark different experiments.</p>"},{"location":"Isca/benchmarking/#running","title":"Running","text":"<p>To run the benchmarking on the Held Suarez experiment, you login to kennedy and then run the following: <pre><code>conda activate isca_env\ncd isca_jobs/benchmarking\npython benchmarking.py\n</code></pre></p> <p>This should then produce a series of <code>output</code> and <code>error</code> text files in the folder  <code>/gpfs1/home/jamd1/isca_jobs/benchmarking/output/held_suarez_test_case</code>.</p> <p>The variables corresponding to each index, as well as the times, are saved in the file  <code>/gpfs1/home/jamd1/isca_jobs/benchmarking/task_times.csv</code>.</p>"},{"location":"Isca/benchmarking/#results","title":"Results","text":"<p>I ran the benchmarking for the Held Suarez and  Frierson  test experiments, as well as a test using SOCRATES  for 1 month on 1 node (using the debug partition).</p> <p>The SOCRATES experiment was the same as the  simple one used when setting up SOCRATES.</p> <p>The time taken for the simulations are shown below:</p> <p></p> <p>The 64 cores result is made up of two nodes, each of 32 cores and was run on the parallel partition.  All other results were run on one node, using the debug partition.</p> T21 resolution with 32 cores per node <p>The combination of 32 cores per node with T21 resolution does not appear in the plots because it  produces the error  <pre><code>MPP_DEFINE_DOMAINS(mpp_compute_extent): domain extents must be positive definite.\n</code></pre> This comes from the file  <code>mpp_domains_define.inc</code> and the issue is likely to be that you can't have a resolution lower than the number of cores.  This is probably because, it causes problems when trying to splitting longitude or latitude domains between cores.</p>"},{"location":"Isca/getting_started/","title":"Getting Started","text":"<p>Isca needs to be run on a powerful computer, so to use it, you first need to <code>ssh</code> into a high performance computer e.g. kennedy.</p>"},{"location":"Isca/getting_started/#installation","title":"Installation","text":""},{"location":"Isca/getting_started/#copy-source-code-step-2","title":"Copy Source Code (Step 2)","text":"<ul> <li>Once logged into the high performance computer, run  <code>git clone https://github.com/ExeClim/Isca</code></li> <li>Then change the current directory  <code>cd Isca</code></li> </ul>"},{"location":"Isca/getting_started/#create-conda-environment-step-3","title":"Create CONDA environment (Step 3)","text":"<ul> <li>In terminal, run <code>conda env create -f ci/environment-py3.9.yml</code>. This takes around 5 minutes.</li> <li> <p>Then, run <code>chmod u+x /gpfs1/apps/conda/$USER/conda/envs/*/bin/*</code>  to make sure it is using the correct python version.</p> </li> <li> <p>Now you should be able to activate the CONDA environment: <code>conda activate isca_env</code>. I would probably double check it is using the correct python version by running <code>which python</code>.  It should return <code>/gpfs1/apps/conda/jamd1/conda/envs/isca_env/bin/python</code>.</p> </li> </ul>"},{"location":"Isca/getting_started/#install-in-development-mode-step-4","title":"Install in development mode (Step 4)","text":"<ul> <li>Change directory to where the setup.py file is:  <code>cd /gpfs1/home/jamd1/Isca/src/extra/python</code> </li> <li>Install the <code>isca</code> python module in development mode:  <code>pip install -e .</code></li> </ul>"},{"location":"Isca/getting_started/#set-environment-and-where-isca-saves-data","title":"Set environment and where Isca saves data","text":"<ul> <li>Using FileZilla, create the directories  /gpfs1/scratch/other/$USER/isca_work and /gpfs1/scratch/other/$USER/isca_output.</li> <li>Add the following to the /gpfs1/home/jamd1/.bashrc file, to indicate the Isca environment and where to  save data.  <pre><code># directory of the Isca source code\nexport GFDL_BASE=/gpfs1/home/$USER/Isca\n# \"environment\" configuration for emps-gv4\nexport GFDL_ENV=ubuntu_conda\n# temporary working directory used in running the model\nexport GFDL_WORK=/gpfs1/scratch/other/$USER/isca_work\n# directory for storing model output\nexport GFDL_DATA=/gpfs1/scratch/other/$USER/isca_output\n</code></pre></li> <li>The full .bashrc file now looks like this:  </li> <li>Exit the <code>ssh</code> and log back in for the new .bashrc script to take effect.</li> <li>This is close to the  Compiling for the first time section.</li> </ul>"},{"location":"Isca/getting_started/#add-fortran-compiler-flags","title":"Add Fortran compiler flags","text":"<ul> <li>Check version of <code>gfortran</code> installed using <code>conda list -n isca_env</code>:  <pre><code>gfortran                  10.4.0              h0c96582_10    conda-forge\ngfortran_impl_linux-64    10.4.0              h44b2e72_16    conda-forge\ngfortran_linux-64         10.4.0              h69d5af5_10    conda-forge\n</code></pre></li> <li>If version is 10 or greater, the file  /gpfs1/home/$USER/Isca/src/extra/python/isca/templates/mkmf.template.ubuntu_conda  needs to be changed, with the following:  <code>-w -fallow-argument-mismatch -fallow-invalid-boz</code>  added to the existing FFlags.</li> <li>The final file should look like this:  </li> </ul>"},{"location":"Isca/getting_started/#held-suarez","title":"Held Suarez","text":"<p>A simple experiment to run to check that the installation has worked is the  Held Suarez  experiment.</p> <p>To run this, you can do the following:</p> <ul> <li>Login to kennedy.</li> <li>Run <code>conda activate isca_env</code> to activate the Isca CONDA environment.</li> <li>Create the following script, titled <code>held_suarez_run.sh</code>, and transfer it to a suitable location on kennedy. <pre><code>#!/bin/bash\n#SBATCH --job-name=held_suarez_test\n#SBATCH --output=\"held_suarez_test_output.txt\"\n#SBATCH --error=\"held_suarez_test_error.txt\"\n#SBATCH --time=02:00:00 # maximum walltime for the job\n#SBATCH --nodes=1 # specify number of nodes\n#SBATCH --ntasks-per-node=16 # specify number of processors per node\n#SBATCH --mail-type=END # send email at job completion\n#SBATCH --mail-user=$USER@st-andrews.ac.uk # email address\n#SBATCH --partition=debug # queue to run on\n#SBATCH --nodelist=kennedy20 # node to run on\n\npython $GFDL_BASE/exp/test_cases/held_suarez/held_suarez_test_case.py\n</code></pre><ul> <li>This will save the things printed to the console in <code>held_suarez_test_error.txt</code>, which will just be added to the same directory that the script is in.  You may want to change this to put it in another location.</li> <li>This will use the debug queue as it should only take around 20 minutes to run.</li> </ul> </li> <li>Submit the script: <code>sbatch held_suarez_run.sh</code> (make sure you are in the same directory as the file first).</li> </ul> <p>As well as the <code>held_suarez_test_output.txt</code> and <code>held_suarez_test_error.txt</code> files, this should also generate some output data in the folder  <code>/gpfs1/scratch/jamd1/isca_output/held_suarez_default/run0001</code>:</p> <p></p>"},{"location":"Isca/getting_started/#running-a-general-experiment","title":"Running a General Experiment","text":""},{"location":"Isca/getting_started/#required-files","title":"Required Files","text":"<p>To run a general experiment, you need to create two files, a namelist nml file specifying the  configuration options for the experiment and a diagnostic table file, specifying what  diagnostics to save for the experiment.</p> <p>The corresponding files for the Held Suarez experiment are given below:</p> namelist_hs.nmldiag_table_hs <pre><code>! This section gives info to give to SLURM when running experiment\n&amp;experiment_details\n   name = 'held_suarez'         ! Name of experiment\n   input_dir = '/gpfs1/home/jamd1/Isca/jobs/held_suarez'\n   n_months_total = 12\n   n_months_job = 12\n   n_nodes = 1\n   n_cores = 16\n   resolution = 'T42'\n   partition = 'debug'          ! Queue to submit job to (Slurm info).\n   overwrite_data = .false.\n   compile = .false.\n   max_walltime = '01:00:00'\n   delete_restart_files = .true.\n   nodelist = 'kennedy20'\n/\n\n&amp;atmosphere_nml\n    idealized_moist_model = .false.\n/\n\n&amp;diag_manager_nml\n    mix_snapshot_average_fields = .false.\n/\n\n&amp;fms_io_nml\n    fileset_write = 'single'\n    threading_write = 'single'\n/\n\n&amp;fms_nml\n    domains_stack_size = 600000\n/\n\n&amp;hs_forcing_nml\n    delh = 60.0\n    delv = 10.0\n    do_conserve_energy = .true.\n    eps = 0.0\n    ka = -40.0\n    kf = -1.0\n    ks = -4.0\n    sigma_b = 0.7\n    t_strat = 200.0\n    t_zero = 315.0\n/\n\n&amp;main_nml\n    calendar = 'thirty_day'\n    current_date = 2000, 1, 1, 0, 0, 0\n    days = 30\n    dt_atmos = 600\n/\n\n&amp;spectral_dynamics_nml\n    damping_order = 4\n    exponent = 7.5\n    initial_sphum = 0.0\n    reference_sea_level_press = 100000.0\n    scale_heights = 6.0\n    surf_res = 0.5\n    valid_range_t = 100.0, 800.0\n    vert_coord_option = 'uneven_sigma'\n    water_correction_limit = 20000.0\n    lon_max = 256\n    lat_max = 128\n    num_fourier = 85\n    num_spherical = 86\n    num_levels = 25\n/\n</code></pre> <pre><code>\"FMS Model results\"\n0001 1 1 0 0 0\n# = output files =\n# file_name, output_freq, output_units, format, time_units, long_name\n\n\"atmos_monthly\", 30, \"days\", 1, \"days\", \"time\",\n\n# = diagnostic field entries =\n# module_name, field_name, output_name, file_name, time_sampling, time_avg, other_opts, precision\n\n\n\"dynamics\", \"ps\", \"ps\", \"atmos_monthly\", \"all\", .true., \"none\", 2,\n\"dynamics\", \"bk\", \"bk\", \"atmos_monthly\", \"all\", .false., \"none\", 2,\n\"dynamics\", \"pk\", \"pk\", \"atmos_monthly\", \"all\", .false., \"none\", 2,\n\"dynamics\", \"ucomp\", \"ucomp\", \"atmos_monthly\", \"all\", .true., \"none\", 2,\n\"dynamics\", \"vcomp\", \"vcomp\", \"atmos_monthly\", \"all\", .true., \"none\", 2,\n\"dynamics\", \"temp\", \"temp\", \"atmos_monthly\", \"all\", .true., \"none\", 2,\n\"dynamics\", \"vor\", \"vor\", \"atmos_monthly\", \"all\", .true., \"none\", 2,\n\"dynamics\", \"div\", \"div\", \"atmos_monthly\", \"all\", .true., \"none\", 2,\n</code></pre> <p>The <code>namelist_hs.nml</code> file specifies all the  namelist options  for the experiment. There is also an additional <code>experiment_details</code>  section which contains information on how to run  the simulation.</p> nml files <p>If a parameter is a boolean e.g. <code>overwrite_data</code> or <code>compile</code>, the format must be <code>.false.</code> or <code>.true.</code>. If a parameter is a string e.g. <code>max_walltime</code>, <code>resolution</code> or <code>name</code> then quotation marks (<code>''</code>) must be used i.e. <code>name = 'held_suarez'</code>.</p> <p><code>!</code> indicates a comment and will not be read in.</p> <p>The <code>diag_table_hs</code> file specifies all the  diagnostics  to save to the output directory.</p>"},{"location":"Isca/getting_started/#running","title":"Running","text":"<p>To run a general experiment, download the <code>isca_tools</code> folder from the  Github repository and transfer it  to your home directory on kennedy.</p> <p>Once you have done this, and have created a namelist file with path  <code>isca_jobs/experiment/namelist.nml</code> and diagnostic table file with path <code>isca_jobs/experiment/diag_table</code>, you can do the following:</p> <ul> <li>Login to kennedy.</li> <li>Run <code>conda activate isca_env</code> to activate the Isca CONDA environment.</li> <li>Run:  <pre><code>python isca_tools isca_jobs/experiment/namelist.nml isca_jobs/experiment/diag_table True\n</code></pre></li> </ul> <p>The <code>True</code> in the above line of code indicates that the jobs should be submitted to Slurm. If this third parameter  is <code>False</code> or not given, then it won't use Slurm but just run on kennedy. This may be useful for debugging small jobs.</p> <p>What this last line of code does is call the function  <code>run_experiment</code>.</p> <p>Rather than submitting the job using terminal, you can also do it through running the following python script: <pre><code>import os\nfrom isca_tools import run_experiment\n\njobs_dir = os.path.join(os.environ['HOME'], 'isca_jobs')  # all jobs saved here\nexp_dir = os.path.join(jobs_dir, 'experiment')            # specific experiment - CHANGE FOR EACH EXPERIMENT\n\nnamelist_file = os.path.join(exp_dir, 'namelist.nml')\ndiag_table_file = os.path.join(exp_dir, 'diag_table')\nrun_experiment(namelist_file, diag_table_file, slurm=True)\n</code></pre></p>"},{"location":"Isca/getting_started/#output-data","title":"Output Data","text":"<p>This will save data to the folder <code>/gpfs1/scratch/jamd1/isca_output/name/</code> where <code>name</code> is the value of <code>name</code> indicated in the <code>experiment_details</code> section of the <code>namelist.nml</code> file.</p> <p>There will be a folder for each month e.g. <code>run0003</code> will contain the data for the third month.</p> <p>If Slurm is used, there will also be a folder titled <code>console_output</code>. This which will contain a file titled <code>error1.txt</code>  containing all the stuff Isca printed to the console for the job starting on month 1, as well as a file titled <code>time1.txt</code>, which contains the time taken for that job to run. If the experiment was split over multiple jobs, there will be multiple <code>error.txt</code> and <code>time.txt</code> files, each indexed with the month  that they started on.</p>"},{"location":"Isca/land/","title":"Land","text":"<p>To run an experiment with land, a .nc file needs to be created indicating where the land is and the  topography.</p> <p>This can be done using the  <code>write_land</code> function.</p>"},{"location":"Isca/land/#square-example","title":"Square Example","text":"<p>To produce a file called <code>land.nc</code> within the <code>input_dir</code> for an  experiment that has a single block of land in the range \\(-10 \\leq \\phi \\leq 10\\); \\(150 \\leq \\lambda \\leq 210\\) with  a single \\(8km\\) high mountain in the center, I would run the following:</p> Code<code>namelist.nml</code> <pre><code>from isca_tools import write_land\nfrom isca_tools.plot import show_land\nimport numpy as np\nwrite_land('land.nc', '/gpfs1/home/jamd1/Isca/jobs/experiment/namelist.nml', 'square', [-10, 10, 180-30, 180+30],\n           topography='gaussian', topography_gauss=[0, 180, 20, 10, 8000])\nshow_land('/gpfs1/home/jamd1/Isca/jobs/experiment/land.nc')\n</code></pre> <pre><code>&amp;experiment_details\n    name = 'experiment'             \n    input_dir = '/gpfs1/home/jamd1/Isca/jobs/experiment/'   \n    n_months_total = 120          ! 10 year simulation\n    n_months_job = 12             ! Each job is 1 year\n    n_nodes = 1                 \n    n_cores = 16                 \n    resolution = 'T42'           \n    partition = 'debug'         \n    overwrite_data = .false.    \n    compile = .false.           \n    max_walltime = '01:00:00'  \n    delete_restart_files = .true.\n/\n\n&amp;main_nml\n    calendar = 'thirty_day'\n    current_date = 1, 1, 1, 0, 0, 0\n    days = 30\n/\n\n&amp;atmosphere_nml\n    idealized_moist_model = .true.\n/\n\n&amp;idealized_moist_phys_nml\n    land_option = 'input'\n    land_file_name = 'INPUT/land.nc'\n    land_roughness_prefactor = 10\n/\n\n&amp;surface_flux_nml\n    land_humidity_prefactor = 0.7                   \n    land_evap_prefactor = 0.7                       \n/\n\n&amp;mixed_layer_nml\n    land_option = 'input'       \n    land_albedo_prefactor = 1.3   \n    land_h_capacity_prefactor = 0.1                \n/\n\n&amp;spectral_init_cond_nml\n    topography_option = 'input'\n    topog_file_name = 'land.nc'\n/\n</code></pre> <p>The <code>land.nc</code> file will contain two variables: <code>land_mask</code> is set to \\(1\\) if a given latitude/longitude coordinate  corresponds to land, otherwise it will be \\(0\\). <code>zsurf</code> indicates the height in meters at each  latitude/longitude coordinate.</p> <p>The <code>show_land</code> function is useful for checking that the  location of the land and topography are as expected. The green outline indicates where land is and the colormap indicates the topography:</p> <p></p> <p>Once the <code>land.nc</code> file has been created and looks correct, the experiment can be run as normal.</p>"},{"location":"Isca/land/#namelist-options","title":"Namelist options","text":"<p>The <code>namelist.nml</code> file in the <code>square</code> example only indicates the options related to land. In Isca, land can be  specified to differ from ocean in four ways:</p> <ol> <li>Roughness length - Land tends to be rougher than ocean.</li> <li>Evaporative flux - It is easier to evaporate water from ocean than land.</li> <li>Albedo - Land tends to reflect more sunlight than ocean.</li> <li>Heat capacity - Land has a smaller heat capacity than ocean and so responds more quickly to external forcing.</li> </ol> <p>Each of the land options specified in the <code>namelist.nml</code> file are related to one of these:</p> <ul> <li><code>land_option</code> - Indicates that the experiment uses land. This option must also be specified in the  <code>mixed_layer_nml</code> namelist.</li> <li><code>land_file_name</code> - Name of land .nc file used.</li> <li><code>land_roughness_prefactor</code> - (1)</li> <li><code>land_humidity_prefactor</code> - (2)</li> <li><code>land_evap_prefactor</code> - (2)</li> <li><code>land_albedo_prefactor</code> - (3)</li> <li><code>land_h_capacity_prefactor</code> - (4)</li> <li><code>topography_option</code> - Indicates that the experiment uses  topography.</li> <li><code>topog_file_name</code> - Name of land .nc file used.</li> </ul>"},{"location":"Isca/socrates/","title":"SOCRATES","text":"<p>To run an experiment with the SOCRATES radiation scheme, you need to get the SOCRATES source code because it is not included with Isca.</p> <p>The following goes through how to modify Isca, so it can be used with SOCRATES and then how to run a simple  experiment. It is based on the instructions on the  Isca website.</p>"},{"location":"Isca/socrates/#adding-socrates-to-isca","title":"Adding SOCRATES to Isca","text":""},{"location":"Isca/socrates/#get-source-code","title":"Get source code","text":"<p>To get the source code, you require a Met Office Science Repository Service (MOSRS) account.  To get one of these, email the radiation code owner which, as of November 2022, was  Dr James Manners: james.manners@metoffice.gov.uk.</p> <p>You should then be sent a username and then be asked to create a password.</p> <p>Once you have an account, download the latest version of the SOCRATES source code from the  MOSRS website (this was 22.07 as of November 2022).</p>"},{"location":"Isca/socrates/#put-source-code-in-isca-directory","title":"Put source code in Isca directory","text":"<p> The SOCRATES source code that you have just downloaded should look something like the folder on the left.</p> <p>Move everything contained in this folder to: <code>$GFDL_BASE/src/atmos_param/socrates/src/trunk/</code>  so that it is within the Isca source code (you may need to create the <code>trunk</code> folder first).</p> <p>I would also add the following line to the  .bashrc file: <pre><code>export GFDL_SOC=/gpfs1/home/$USER/Isca/src/atmos_param/socrates/src/trunk/\n</code></pre> to indicate where the SOCRATES source code is located.  The <code>/gpfs1/home/$USER/Isca/</code> part should be whatever <code>GFDL_BASE</code> is in your .bashrc file.</p>"},{"location":"Isca/socrates/#edit-the-number-of-angles-in-the-phase-function","title":"Edit the number of angles in the phase function","text":"<p>This is just step 3 from the  Isca instructions.</p> <p>Open the file  <code>$GFDL_BASE/src/atmos_param/socrates/src/trunk/src/modules_core/dimensions_spec_ucf.F90</code> and make the following changes:</p> <p><pre><code>npd_k_term=14\nnpd_scale_variable = 4\nnpd_continuum = 2\nnpd_drop_type = 5\nnpd_cloud_parameter = 30\nnpd_phase_term = 1\n</code></pre> The final one of these is the most important, as a large value for this term significantly  increases Socrates' memory usage, and will make Isca much slower.</p>"},{"location":"Isca/socrates/#change-the-path_names-file","title":"Change the <code>path_names</code> file","text":"<p>As indicated in step 6 of the Isca instructions, Isca's Python front-end uses a static list of file names to be compiled for the SOCRATES version of Isca.</p> <p>These are indicated in the following file:</p> <p><code>$GFDL_BASE/src/extra/model/socrates/path_names</code></p> <p>However, the list was compiled from a version of SOCRATES around \\(17\\).  Thus, it is now out of date and needs updating:</p> <ul> <li>Take the original <code>path_names</code> file and delete all names which include the directory  <code>atmos_param/socrates/src/trunk/src/radiation_control</code>.</li> <li>Delete all names which include the directory  <code>atmos_param/socrates/src/trunk/src/radiance_core</code>.</li> <li>Using the code below (<code>jamd1</code> should be replaced with your username),  find all files in <code>radiance_core</code> directory of the SOCRATES source code and add them to the <code>path_names</code> file: <pre><code>import os\nfor root, dirs, files in os.walk(os.path.abspath('/gpfs1/home/jamd1/Isca/src/atmos_param/socrates/src/trunk/src/radiance_core')):\n    for file in files:\n        print(os.path.join(root, file).replace('/gpfs1/home/jamd1/Isca/src/', ''))\n</code></pre></li> <li>At this stage, I would run a SOCRATES test experiment with  <code>compile=True</code> to see what errors you get. You will probably get an opening file error due to a file being specified in <code>path_names</code> which does not exist e.g.  If you get this, just delete the name of this file from the <code>path_names</code> folder. For me, with v22.07 of SOCRATES,  this only occured for the file  <code>atmos_param/socrates/src/trunk/src/aux/interpolate_p.f</code>.</li> </ul>"},{"location":"Isca/socrates/#single-column","title":"Single Column","text":"<p>The <code>path_names</code> for the single column version of SOCRATES in the file  <code>$GFDL_BASE/src/extra/model/socrates_column/path_names</code> should also be changed as above,  with a few modifications to get the compilation to work:</p> <p>As well as <code>atmos_param/socrates/src/trunk/src/aux/interpolate_p.f</code>, I had to delete  <code>atmos_param/socrates/src/trunk/src/um/out_nml.f90</code> and  <code>atmos_param/socrates/src/trunk/src/um/def_um_nml.f90</code>.</p> <p>I had to add <code>atmos_param/socrates/src/trunk/src/scatter/polynomialroots.f90</code>.</p> <p>I had to edit the <code>SWAP</code> interface in this <code>polynomialroots.f90</code> file:</p> OriginalUpdated <pre><code>  INTERFACE Swap\n    MODULE PROCEDURE SwapDouble, SwapSingle\n  END INTERFACE\n</code></pre> <pre><code>  INTERFACE Swap\n    MODULE PROCEDURE SwapDouble   !, SwapSingle\n  END INTERFACE\n</code></pre> <p>Otherwise I got an error:</p> <p><code>Error: Ambiguous interfaces in generic interface 'swap' for 'swapdouble' at (1) and  'swapsingle' at (2)</code></p>"},{"location":"Isca/socrates/#running-a-simple-experiment","title":"Running a simple experiment","text":"<p>The simple SOCRATES experiment that I ran, was just taking the  Frierson test experiment and making the following modifications:</p> <ul> <li>Set <code>albedo</code> to \\(0.38\\) to reflect value used  in <code>socrates_aquaplanet.py</code> example script.</li> <li>Set <code>depth</code> to \\(5m\\).</li> <li>Set up the <code>socrates_nml</code> namelist set up with the values used in the  <code>socrates_aquaplanet.py</code>  example script. To do this, you will need to add the  <code>ozone_1990.nc</code> file into the <code>input_dir</code>. The <code>lw_spectral_filename</code> and <code>sw_spectral_filename</code> options may need altering as well, to indicate the correct files  e.g. in the <code>namelist.nml</code> file given below, you would have to use your username rather than <code>jamd1</code>.</li> </ul> Files used <p>The <code>namelist.nml</code> and <code>diag_table</code> files used are indicated below:</p> <code>namelist.nml</code><code>diag_table</code> <pre><code>! This experiment is the same as frierson_test_case but with socrates radiation as specified\n! in the socrates_aquaplanet.py example and with a mixed layer depth of 5m rather 2.5m in the mixed_layer_nml namelist.\n! Albedo is also set to 0.38 as used for the byrne radiation.\n! It is also run for 10 years.\n\n! This section gives info to give to Slurm when running experiment\n&amp;experiment_details\n   name = 'test/socrates'                ! Name of experiment e.g. data saved to folder $GFDL_DATA/name.\n   input_dir = '/gpfs1/home/jamd1/isca_jobs/test/socrates/'\n   n_months_total = 1           ! Total duration of simulation in months.\n   n_months_job = 1             ! Approximate duration of each job of the simulation in months.\n                                ! If n_months_total=12 and n_months_job=6, would have 2 jobs, each of length 6 months.\n   n_nodes = 1                  ! Number of nodes to run job on (Slurm info).\n   n_cores = 32                  ! Number of cores for each node to run job on (Slurm info).\n   resolution = 'T42'           ! Horizontal resolution of experiment ('T21', 'T42' or 'T85').\n   partition = 'debug'          ! Queue to submit job to (Slurm info).\n   overwrite_data = .false.     ! If .true. and data already exists in $GFDL_DATA/name, it will be overwritten.\n   compile = .false.            ! If .true. it will recompile the codebase before running the experiment.\n   max_walltime = '02:00:00'    ! Maximum time that job can run e.g. '01:00:00' would be 1 hour (Slurm info).\n   delete_restart_files = .true.    ! Only want to save 1 restart file\n/\n\n&amp;main_nml\n    calendar = 'thirty_day'\n    current_date = 1, 1, 1, 0, 0, 0\n    days = 30\n    dt_atmos = 720\n    hours = 0\n    minutes = 0\n    seconds = 0\n/\n\n&amp;idealized_moist_phys_nml\n    do_damping = .true.\n    turb = .true.\n    mixed_layer_bc = .true.\n    do_virtual = .false.\n    do_simple = .true.\n    roughness_heat = 3.21e-05\n    roughness_moist = 3.21e-05\n    roughness_mom = 3.21e-05\n    two_stream_gray = .false.\n    do_socrates_radiation = .true.\n    convection_scheme = 'SIMPLE_BETTS_MILLER'\n/\n\n&amp;vert_turb_driver_nml\n    do_mellor_yamada = .false.      ! default is True\n    do_diffusivity = .true.         ! default is False\n    do_simple = .true.              ! default is False\n    constant_gust = 0.0             ! default is 1.0\n    use_tau = .false.\n/\n\n&amp;diffusivity_nml\n    do_entrain = .false.\n    do_simple = .true.\n/\n\n&amp;surface_flux_nml\n    do_simple = .true.\n    old_dtaudv = .true.\n    use_virtual_temp = .false.\n/\n\n&amp;atmosphere_nml\n    idealized_moist_model = .true.\n/\n\n&amp;mixed_layer_nml\n    albedo_value = 0.38                 ! Frierson was 0.31, need to increase for it to converge with byrne.\n                                        ! socrates_aquaplanet.py also has 0.38 so keeping this value.\n    depth = 5.0                         ! different from frierson_test_case value of 2.5\n    evaporation = .true.\n    prescribe_initial_dist = .true.\n    tconst = 285.0\n/\n\n&amp;qe_moist_convection_nml\n    tmax = 350.0\n    tmin = 160.0\n    rhbm = 0.7\n/\n\n! Not sure why we need this namelist as we are doing simple betts miller which uses qe_moist_convection_nml\n&amp;betts_miller_nml\n    rhbm = 0.7\n    do_simp = .false.\n    do_shallower = .true.\n/\n\n&amp;lscale_cond_nml\n    do_evap = .true.\n    do_simple = .true.\n/\n\n&amp;sat_vapor_pres_nml\n    do_simple = .true.\n/\n\n&amp;damping_driver_nml\n    do_conserve_energy = .true.\n    do_rayleigh = .true.\n    sponge_pbottom = 5000.0         ! Bottom of the model's sponge down to 50hPa (units are Pa)\n    trayfric = -0.25\n/\n\n&amp;diag_manager_nml\n    mix_snapshot_average_fields = .false.       ! time avg fields are labelled with time in middle of window\n/\n\n\n&amp;fms_io_nml\n    fileset_write = 'single'        ! default is multi\n    threading_write = 'single'      ! default is multi\n/\n\n&amp;fms_nml\n    domains_stack_size = 600000     ! default is 0\n/\n\n&amp;spectral_dynamics_nml\n    damping_order = 4\n    water_correction_limit = 200.e2\n    reference_sea_level_press = 1.0e5\n    num_levels = 25                         ! How many pressure levels to use\n    valid_range_t = 100.0, 800.0\n    initial_sphum = 2e-06\n    vert_coord_option = 'input'             ! Use the vertical levels from Frierson 2006\n    surf_res = 0.5\n    scale_heights = 11.0\n    exponent = 7.0\n    robert_coeff = 0.03\n/\n\n! Specify vertical levels from Frierson 2006\n&amp;vert_coordinate_nml\n    bk = 0.000000, 0.0117665, 0.0196679, 0.0315244, 0.0485411, 0.0719344, 0.1027829, 0.1418581, 0.1894648, 0.2453219, 0.3085103, 0.3775033, 0.4502789, 0.5244989, 0.5977253, 0.6676441, 0.7322627, 0.7900587, 0.8400683, 0.8819111, 0.9157609, 0.9422770, 0.9625127, 0.9778177, 0.9897489, 1.0000000\n    pk = 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000\n/\n\n! Specify socrates radiation using parameters given in socrates_aquaplanet.py script.\n&amp;socrates_rad_nml\n    stellar_constant = 1370\n    lw_spectral_filename = '/gpfs1/home/jamd1/Isca/src/atmos_param/socrates/src/trunk/data/spectra/ga7/sp_lw_ga7'\n    sw_spectral_filename = '/gpfs1/home/jamd1/Isca/src/atmos_param/socrates/src/trunk/data/spectra/ga7/sp_sw_ga7'\n    do_read_ozone = .true.\n    ozone_file_name = 'ozone_1990'\n    ozone_field_name = 'ozone_1990'\n    dt_rad = 3600\n    store_intermediate_rad = .true.\n    chunk_size = 16\n    use_pressure_interp_for_half_levels = .false.\n    tidally_locked = .false.\n    solday = 90\n/\n</code></pre> <pre><code>\"FMS Model results\"\n0001 1 1 0 0 0\n# = output files =\n# file_name, output_freq, output_units, format, time_units, long_name\n\n\"atmos_monthly\", 30, \"days\", 1, \"days\", \"time\",\n\n# = diagnostic field entries =\n# module_name, field_name, output_name, file_name, time_sampling, time_avg, other_opts, precision\n\n\"mixed_layer\", \"t_surf\", \"t_surf\", \"atmos_monthly\", \"all\", .true., \"none\", 2,\n\"socrates\", \"soc_olr\", \"soc_olr\", \"atmos_monthly\", \"all\", .true., \"none\", 2,\n\"socrates\", \"soc_toa_sw\", \"soc_toa_sw\", \"atmos_monthly\", \"all\", .true., \"none\", 2,\n</code></pre> <p>The first time you run it, you need to compile the SOCRATES code, which is achieved by setting <code>compile=True</code> (set to <code>False</code> again after it has worked once). You also probably want a very short simulation duration when debugging to check that SOCRATES is working.  I did this by setting <code>days=3</code>.</p>"},{"location":"Isca/time_series/","title":"Time Series","text":"<p>To run an experiment with a variable e.g. \\(CO_2\\) concentration that varies during the simulation,  a timeseries .nc file needs to be created indicating the value of the variable at a given time, pressure,  latitude and longitude.</p> <p>This can be done using the  <code>create_time_series_file</code> function.</p>"},{"location":"Isca/time_series/#carbon-dioxide-example","title":"Carbon dioxide Example","text":"<p>To produce a file called <code>co2.nc</code> within the <code>input_dir</code> for an  experiment that had \\(CO_2\\) concentration of \\(300 ppmv\\) for the first  \\(5\\) years and \\(600 ppmv\\) thereafter, I would run the following:</p> Code<code>namelist.nml</code> <pre><code>from isca_tools import create_time_series_file\nimport numpy as np\n\ndef co2_func(days, pressure, lat, lon):\n    co2_val = np.ones((days.shape[0], pressure.shape[0],\n                       lat.shape[0], lon.shape[0])) * 600\n    co2_val[days &lt; 360 * 5] = 300\n    return co2_val\n\ncreate_time_series_file('co2.nc', \n                        '/gpfs1/home/jamd1/Isca/jobs/experiment/namelist.nml',\n                        'co2',co2_func, 360)\n</code></pre> <pre><code>&amp;experiment_details\nname = 'experiment'             \ninput_dir = '/gpfs1/home/jamd1/Isca/jobs/experiment/'   \nn_months_total = 120          ! 10 year simulation\nn_months_job = 12             ! Each job is 1 year\nn_nodes = 1                 \nn_cores = 16                 \nresolution = 'T21'           \npartition = 'debug'         \noverwrite_data = .false.    \ncompile = .false.           \nmax_walltime = '01:00:00'  \ndelete_restart_files = .true.\n/\n\n&amp;main_nml\n    calendar = 'thirty_day'\n    current_date = 1, 1, 1, 0, 0, 0\n    days = 30\n/\n\n&amp;atmosphere_nml\n    idealized_moist_model = .true.\n/\n\n&amp;idealized_moist_phys_nml\n    two_stream_gray = .true.\n/\n\n&amp;two_stream_gray_rad_nml\n    rad_scheme = 'byrne'            !Must be 'byrne' or 'geen' otherwise optical depth independent of CO2\n    do_read_co2 = .true.            !Read in CO2 timeseries from input file\n    co2_file = 'co2'                !Tell model name of co2 input file\n/\n</code></pre> <p>The <code>co2_func</code> function given as an input to the <code>create_time_series_file</code> must take as arguments, time (in days), pressure, latitude and longitude. It then must output an  \\(n_{time} \\times n_{pressure} \\times n_{latitude} \\times n_{longitude}\\) numpy array giving the \\(CO_2\\) concentration at each location for each time.</p> Pressure values <p>The <code>create_time_series_file</code> function loads in pressure coordinates from the <code>t_{res}_grid.nc</code> files  in <code>isca_tools/time_series/grid_files</code>. These files only contain 2 pressure values so if more resolution is required in the vertical, new <code>t_{res}_grid.nc</code> files need to be created.</p> <p>To do this, modify the <code>gridfile_namelist.nml</code> file so that  <code>num_levels</code> indicates the number of desired pressure values.</p> <p>Then run the experiment using this updated file. Copy the <code>atmos_monthly.nc</code> file that this creates and place it in the <code>isca_tools/time_series/grid_files</code> folder with the name <code>t_{res}_exp.nc</code>. Now you can run  <code>create_grid_file</code>  to get the <code>t_{res}_grid.nc</code> file.</p> <p>Note that if the variable is <code>co2</code> for the  <code>two_stream_gray_rad_nml</code> namelist,  no spatial variation is allowed and the max value at each time step is read in.</p> <p>The <code>namelist.nml</code> file indicates the bare minimum variables that need to be specified to run this time varying \\(CO_2\\) experiment. Because the calendar is <code>thirty_day</code>, each year is \\(360\\) days hence why \\(360\\) appears in <code>co2_func</code>.</p> <p>The <code>co2_file</code> option in the namelist indicates the name of  the file within the <code>input_dir</code>. It must be specified without  the <code>.nc</code> suffix.</p> <p>To allow for varying \\(CO_2\\) concentration to have any effect,  <code>rad_scheme</code> must be either <code>byrne</code> or <code>geen</code>. Otherwise, the optical depth is prescribed and independent of \\(CO_2\\) concentration.</p> <p>The same <code>co2.nc</code> file can be used with the Rapid Radiative Transfer Model/SOCRATES radiation scheme too, by including  <code>rrtm_radiation_nml</code>/<code>socrates_rad_nml</code>  in the namelist file:</p> RRTMSOCRATES <pre><code>&amp;rrtm_radiation_nml\n    do_read_co2 = .true.            !Read in CO2 timeseries from input file\n    co2_file = 'co2'                !Tell model name of co2 input file\n/\n</code></pre> <pre><code>&amp;socrates_rad_nml\n    do_read_co2 = .true.            !Read in CO2 timeseries from input file\n    co2_file = 'co2'                !Tell model name of co2 input file\n/\n</code></pre>"},{"location":"code/cesm/base/","title":"Base","text":""},{"location":"code/cesm/base/#isca_tools.cesm.base.create_per_job_nml","title":"<code>create_per_job_nml(input_file_path, files_per_job=1, years_per_job=None, exist_ok=None)</code>","text":"<p>Splits up list of all input files (or years) into separate lists of no more than <code>files_per_job</code> (<code>years_per_job</code>) in each. A <code>nml</code> file is then created for each of these with name same as <code>input_file_path</code> but with first file (year) in job as a suffix.</p> <p>E.g. <code>input_nml</code> becomes <code>input0.nml</code> with <code>input_info['script_info']['ind_files']</code> set to file indices to run for that job.</p> <p>E.g. <code>input_nml</code> becomes <code>input1985.nml</code> with <code>input_info['script_info']['year_files']</code> set to years to run for that job.</p> <p>Parameters:</p> Name Type Description Default <code>input_file_path</code> <code>str</code> <p>Path to <code>nml</code> file for experiment.</p> required <code>files_per_job</code> <code>int</code> <p>Number of files to consider for each job.</p> <code>1</code> <code>years_per_job</code> <code>Optional[int]</code> <p>Number of years to run for each job. Overrides <code>files_per_job</code> if provided.</p> <code>None</code> <code>exist_ok</code> <code>Optional[bool]</code> <p>If <code>True</code>, do not raise exception if any file to be created already exists. If <code>False</code>, will overwrite it. If <code>None</code> leaves the existing file unchanged.</p> <code>None</code> <p>Returns:</p> Type Description <code>List</code> <p>List of paths to nml files created e.g. <code>['/Users/.../input1985.nml', '/Users/.../input1985.nml']</code></p> Source code in <code>isca_tools/cesm/base.py</code> <pre><code>def create_per_job_nml(input_file_path: str, files_per_job: int = 1, years_per_job: Optional[int] = None,\n                       exist_ok: Optional[bool] = None) -&gt; List:\n    \"\"\"\n    Splits up list of all input files (or years) into separate lists of no more than `files_per_job` (`years_per_job`) in each.\n    A `nml` file is then created for each of these with name same as `input_file_path` but with first file (year)\n    in job as a suffix.\n\n    E.g. `input_nml` becomes `input0.nml` with `input_info['script_info']['ind_files']` set to file\n    indices to run for that job.\n\n    E.g. `input_nml` becomes `input1985.nml` with `input_info['script_info']['year_files']` set to\n    years to run for that job.\n\n    Args:\n        input_file_path: Path to `nml` file for experiment.\n        files_per_job: Number of files to consider for each job.\n        years_per_job: Number of years to run for each job. Overrides `files_per_job` if provided.\n        exist_ok: If `True`, do not raise exception if any file to be created already exists.\n            If `False`, will overwrite it. If `None` leaves the existing file unchanged.\n\n    Returns:\n        List of paths to nml files created e.g. `['/Users/.../input1985.nml', '/Users/.../input1985.nml']`\n    \"\"\"\n    input_info = f90nml.read(input_file_path)\n\n    # Determine which years to get data for\n    file_dates = get_exp_file_dates(input_info['script_info']['exp_name'], 'atm',\n                                    input_info['script_info']['archive_dir'], input_info['script_info']['hist_file'])\n    year_files_all = np.unique(file_dates.dt.year).tolist()\n    file_ind_all = np.arange(len(file_dates)).tolist()\n    if years_per_job is not None:\n        if input_info['script_info']['year_files'] is None:\n            file_consider = year_files_all\n        else:\n            file_consider = parse_int_list(input_info['script_info']['year_files'], lambda x: int(x),\n                                            all_values=year_files_all)\n    else:\n        if input_info['script_info']['ind_files'] is None:\n            file_consider = file_ind_all\n        else:\n            file_consider = parse_int_list(input_info['script_info']['ind_files'], lambda x: int(x),\n                                           all_values=file_ind_all)\n    n_digit = np.max([len(str(val)) for val in file_consider])          # how many digits to use for file name\n    file_jobs = split_list_max_n(file_consider, years_per_job if years_per_job is not None else files_per_job)\n    out_file_names = []\n    for ind in file_jobs:\n        if years_per_job is not None:\n            input_info['script_info']['year_files'] = ind\n        else:\n            input_info['script_info']['ind_files'] = ind\n        out_file_names.append(input_file_path.replace('.nml', f'{ind[0]:{n_digit}d}.nml'))\n        if os.path.exists(out_file_names[-1]):\n            if exist_ok is None:\n                print(f'{ind}: Output nml file already exists. Leaving unchanged')\n                continue\n        input_info.write(out_file_names[-1], force=exist_ok)\n        print(f'{ind}: Output nml file created')\n    return out_file_names\n</code></pre>"},{"location":"code/cesm/base/#isca_tools.cesm.base.get_pressure","title":"<code>get_pressure(ps, p0, hya, hyb)</code>","text":"<p>Calculates pressure at the hybrid levels, similar to NCAR function.</p> <p>Parameters:</p> Name Type Description Default <code>ps</code> <code>Union[ndarray, DataArray]</code> <p><code>float [n_time x n_lat x n_lon]</code> Array of surface pressures in units of Pa. <code>PS</code> in CESM atmospheric output.</p> required <code>p0</code> <code>float</code> <p>Surface reference pressure in Pa. <code>P0</code> in CESM atmospheric output.</p> required <code>hya</code> <code>Union[ndarray, DataArray]</code> <p><code>float [n_levels]</code> Hybrid A coefficients. <code>hyam</code> in CESM atmospheric output.</p> required <code>hyb</code> <code>Union[ndarray, DataArray]</code> <p><code>float [n_levels]</code> Hybrid B coefficients. <code>hybm</code> in CESM atmospheric output.</p> required <p>Returns:</p> Type Description <code>Union[ndarray, DataArray]</code> <p><code>float [n_time x n_levels x n_lat x n_lon]</code> Pressure at the hybrid levels in Pa.</p> Source code in <code>isca_tools/cesm/base.py</code> <pre><code>def get_pressure(ps: Union[np.ndarray, xr.DataArray], p0: float, hya: Union[np.ndarray, xr.DataArray],\n                 hyb: Union[np.ndarray, xr.DataArray]) -&gt; Union[np.ndarray, xr.DataArray]:\n    \"\"\"\n    Calculates pressure at the hybrid levels, similar to\n    [NCAR function](https://www.ncl.ucar.edu/Document/Functions/Built-in/pres_hybrid_ccm.shtml).\n\n    Args:\n        ps: `float [n_time x n_lat x n_lon]`&lt;/br&gt;\n            Array of surface pressures in units of *Pa*.&lt;/br&gt;\n            `PS` in CESM atmospheric output.\n        p0: Surface reference pressure in *Pa*.&lt;/br&gt;\n            `P0` in CESM atmospheric output.\n        hya: `float [n_levels]`&lt;/br&gt;\n            Hybrid A coefficients.&lt;/br&gt;\n            `hyam` in CESM atmospheric output.\n        hyb: `float [n_levels]`&lt;/br&gt;\n            Hybrid B coefficients.&lt;/br&gt;\n            `hybm` in CESM atmospheric output.\n\n    Returns:\n        `float [n_time x n_levels x n_lat x n_lon]`\n            Pressure at the hybrid levels in *Pa*.\n    \"\"\"\n    return hya * p0 + hyb * ps\n</code></pre>"},{"location":"code/cesm/load/","title":"Load","text":""},{"location":"code/cesm/load/#isca_tools.cesm.load.ds_month_shift","title":"<code>ds_month_shift(ds, decode_times=True)</code>","text":"<p>When loading CESM data, for some reason the first month is marked as February, so this function shifts the time variable to correct it to January.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset to apply the shift to. It should have been loaded with <code>decode_times=False</code>.</p> required <code>decode_times</code> <code>bool</code> <p>If <code>True</code>, will convert time to actual date.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset with first months shifted by -1 so now first month is January.</p> Source code in <code>isca_tools/cesm/load.py</code> <pre><code>def ds_month_shift(ds: xr.Dataset, decode_times: bool = True) -&gt; xr.Dataset:\n    \"\"\"\n    When loading CESM data, for some reason the first month is marked as February, so this function\n    shifts the time variable to correct it to January.\n\n    Args:\n        ds: Dataset to apply the shift to.\n            It should have been loaded with `decode_times=False`.\n        decode_times: If `True`, will convert time to actual date.\n\n    Returns:\n        Dataset with first months shifted by -1 so now first month is January.\n    \"\"\"\n    n_day_month = np.asarray(\n        [cftime.DatetimeNoLeap(1, i + 1, 1).daysinmonth for i in range(12)])  # number of days in each month\n    months_in_ds = np.arange(1, 13)                             # TODO may have to get months from ds e.g. ds.time.dt.month\n    n_day_month = n_day_month[np.asarray(months_in_ds) - 1]\n    n_months_in_ds = ds.time.size\n    n_years_in_ds = int(np.floor(n_months_in_ds / 12))\n    month_shift_array = np.concatenate((np.tile(n_day_month, n_years_in_ds),\n                                        n_day_month[:n_months_in_ds % 12]))\n    ds_new = ds.assign_coords({'time': ('time', ds.time.values - month_shift_array, ds.time.attrs)})\n    if decode_times:\n        ds_new = xr.decode_cf(ds_new)\n    return ds_new\n</code></pre>"},{"location":"code/cesm/load/#isca_tools.cesm.load.get_exp_dir","title":"<code>get_exp_dir(exp_name, comp='atm', archive_dir=jasmin_archive_dir)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>exp_name</code> <code>str</code> <p>Name of folder in <code>archive_dir</code> where data for this experiment was saved.</p> required <code>comp</code> <code>str</code> <p>Component of CESM to load data from. Options are:</p> <ul> <li><code>atm</code>: atmosphere</li> <li><code>ice</code>: ice</li> <li><code>lnd</code>: land</li> <li><code>rof</code>: river</li> </ul> <code>'atm'</code> <code>archive_dir</code> <code>str</code> <p>Directory where CESM archive data saved.</p> <code>jasmin_archive_dir</code> <p>Returns:</p> Source code in <code>isca_tools/cesm/load.py</code> <pre><code>def get_exp_dir(exp_name: str, comp: str = 'atm', archive_dir: str = jasmin_archive_dir):\n    \"\"\"\n\n    Args:\n        exp_name: Name of folder in `archive_dir` where data for this experiment was saved.\n        comp: Component of CESM to load data from.&lt;/br&gt;\n            Options are:\n\n            * `atm`: atmosphere\n            * `ice`: ice\n            * `lnd`: land\n            * `rof`: river\n        archive_dir: Directory where CESM archive data saved.\n\n    Returns:\n\n    \"\"\"\n    # LHS of comp_id_dict is name of directory containing hist files for the component\n    # RHS of comp_id_dict is the string indicating the component in the individual .nc files within this directory\n    comp_id_dict = {'atm': 'cam',  # atmosphere\n                    'ice': 'cice',  # ice\n                    'lnd': 'clm2',  # land\n                    'rof': 'mosart'}  # river\n    if comp not in comp_id_dict:\n        # Generate inverse dict to comp_id_dict\n        comp_id_dict_reverse = {key: list(comp_id_dict.keys())[i] for i, key in enumerate(comp_id_dict.values())}\n        if comp in comp_id_dict_reverse:\n            # Deal with case where give comp_file not comp_dir i.e. 'cam' rather than 'atm'\n            comp_dir = os.path.join(archive_dir, exp_name, comp_id_dict_reverse[comp])\n            comp_id = comp\n        else:\n            raise ValueError(f'comp must be one of {list(comp_id_dict.keys())} but got {comp}')\n    else:\n        comp_dir = os.path.join(archive_dir, exp_name, comp)\n        comp_id = comp_id_dict[comp]\n    return os.path.join(comp_dir, 'hist'), comp_id\n</code></pre>"},{"location":"code/cesm/load/#isca_tools.cesm.load.get_exp_file_dates","title":"<code>get_exp_file_dates(exp_name, comp='atm', archive_dir=jasmin_archive_dir, hist_file=0)</code>","text":"<p>Get dates indicated in file names of a particular experiment.</p> <p>Parameters:</p> Name Type Description Default <code>exp_name</code> <code>str</code> <p>Name of folder in <code>archive_dir</code> where data for this experiment was saved.</p> required <code>comp</code> <code>str</code> <p>Component of CESM to load data from. Options are:</p> <ul> <li><code>atm</code>: atmosphere</li> <li><code>ice</code>: ice</li> <li><code>lnd</code>: land</li> <li><code>rof</code>: river</li> </ul> <code>'atm'</code> <code>archive_dir</code> <code>str</code> <p>Directory where CESM archive data saved.</p> <code>jasmin_archive_dir</code> <code>hist_file</code> <code>int</code> <p>Which history file to load, <code>0</code> is the default monthly averaged data set.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>DataArray of dates indicated in file names of <code>exp_name</code>.</p> Source code in <code>isca_tools/cesm/load.py</code> <pre><code>def get_exp_file_dates(exp_name: str, comp: str = 'atm', archive_dir: str = jasmin_archive_dir,\n                       hist_file: int = 0) -&gt; xr.DataArray:\n    \"\"\"\n    Get dates indicated in file names of a particular experiment.\n\n    Args:\n        exp_name: Name of folder in `archive_dir` where data for this experiment was saved.\n        comp: Component of CESM to load data from.&lt;/br&gt;\n            Options are:\n\n            * `atm`: atmosphere\n            * `ice`: ice\n            * `lnd`: land\n            * `rof`: river\n        archive_dir: Directory where CESM archive data saved.\n        hist_file: Which history file to load, `0` is the default monthly averaged data set.\n\n    Returns:\n        DataArray of dates indicated in file names of `exp_name`.\n    \"\"\"\n    exp_dir, comp_id = get_exp_dir(exp_name, comp, archive_dir)\n    # Only load in specific years and/or months\n    data_files_all = os.listdir(exp_dir)\n    # only keep files of correct format\n    file_dates = []\n    for file in data_files_all:\n        date = re.search(rf'h{hist_file}\\.(.*?)\\.nc', file)\n        if not date:\n            continue\n        file_dates.append(parse_cesm_datetime(date.group(1)))\n    try:\n        return xr.DataArray(np.array(file_dates, dtype='datetime64[D]'), dims=\"time\", name=\"time\")\n    except OutOfBoundsDatetime as e:\n        warnings.warn(f\"Got out of bounds error, re-trying with NoLeap Calendar and cftime\\n{e}\")\n        cftime_dates = [cftime.DatetimeNoLeap(dt.year, dt.month, dt.day) for dt in file_dates]\n        return xr.DataArray(CFTimeIndex(cftime_dates, calendar=\"noleap\"), dims=\"time\", name=\"time\")\n</code></pre>"},{"location":"code/cesm/load/#isca_tools.cesm.load.load_dataset","title":"<code>load_dataset(exp_name, comp='atm', archive_dir=jasmin_archive_dir, hist_file=0, chunks=None, combine='nested', concat_dim='time', decode_times=True, parallel=False, preprocess=None, year_files=None, month_files=None, ind_files=None, apply_month_shift_fix=True, logger=None)</code>","text":"<p>This loads a dataset of a given component produced by CESM.</p> <p>Parameters:</p> Name Type Description Default <code>exp_name</code> <code>str</code> <p>Name of folder in <code>archive_dir</code> where data for this experiment was saved.</p> required <code>comp</code> <code>str</code> <p>Component of CESM to load data from. Options are:</p> <ul> <li><code>atm</code>: atmosphere</li> <li><code>ice</code>: ice</li> <li><code>lnd</code>: land</li> <li><code>rof</code>: river</li> </ul> <code>'atm'</code> <code>archive_dir</code> <code>str</code> <p>Directory where CESM archive data saved.</p> <code>jasmin_archive_dir</code> <code>hist_file</code> <code>int</code> <p>Which history file to load, <code>0</code> is the default monthly averaged data set.</p> <code>0</code> <code>chunks</code> <code>Optional[Union[dict, Literal['auto'], int]]</code> <p>Dictionary with keys given by dimension names and values given by chunk sizes e.g. <code>{\"time\": 365, \"lat\": 50, \"lon\": 100}</code>. Has big impact on memory usage. If <code>None</code>, no chunking is performed.</p> <code>None</code> <code>combine</code> <code>Literal['by_coords', 'nested']</code> <p>Whether <code>xarray.combine_by_coords</code> or <code>xarray.combine_nested</code> is used to combine all the data.</p> <code>'nested'</code> <code>concat_dim</code> <code>str</code> <p>Dimensions to concatenate files along. You only need to provide this argument if combine='nested'.</p> <code>'time'</code> <code>parallel</code> <code>bool</code> <p>Whether parallel loading is performed.</p> <code>False</code> <code>preprocess</code> <code>Optional[Callable]</code> <p>Function to preprocess the data before loading.</p> <code>None</code> <code>decode_times</code> <code>bool</code> <p>If <code>True</code>, will convert time to actual date.</p> <code>True</code> <code>year_files</code> <code>Optional[Union[int, List, str]]</code> <p>Only files with these years in their name will be loaded. Leave as <code>None</code> to load all years. As well as integer or list of integers, there are three string options: * <code>'1975:1979'</code> will load in all years between 1975 and 1979 inclusive. * <code>first5</code> will load in the first 5 years. * <code>last5</code> will load in the last 5 years.</p> <code>None</code> <code>month_files</code> <code>Optional[Union[int, List, str]]</code> <p>Only files with these months (1 is Jan) in their names will be loaded. Leave as <code>None</code> to load all months. Accepts same format as <code>year_files</code>.</p> <code>None</code> <code>ind_files</code> <code>Optional[Union[int, List, str]]</code> <p>Will save the files with these indices, when ordered by date. Takes precident over <code>year_files</code> and <code>month_files</code>. Accepts same format as <code>year_files</code>.</p> <code>None</code> <code>apply_month_shift_fix</code> <code>bool</code> <p>If <code>True</code>, will apply <code>ds_month_shift</code> before returning dataset. Only used for monthly averaged data i.e. <code>hist_file=0</code>.</p> <code>True</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing all diagnostics specified for the experiment.</p> Source code in <code>isca_tools/cesm/load.py</code> <pre><code>def load_dataset(exp_name: str, comp: str = 'atm',\n                 archive_dir: str = jasmin_archive_dir,\n                 hist_file: int = 0,\n                 chunks: Optional[Union[dict, Literal[\"auto\"], int]] = None,\n                 combine: Literal[\"by_coords\", \"nested\"] = 'nested',\n                 concat_dim: str = 'time',\n                 decode_times: bool = True,\n                 parallel: bool = False,\n                 preprocess: Optional[Callable] = None,\n                 year_files: Optional[Union[int, List, str]] = None,\n                 month_files: Optional[Union[int, List, str]] = None,\n                 ind_files: Optional[Union[int, List, str]] = None,\n                 apply_month_shift_fix: bool = True,\n                 logger: Optional[logging.Logger] = None) -&gt; xr.Dataset:\n    \"\"\"\n    This loads a dataset of a given component produced by CESM.\n\n    Args:\n        exp_name: Name of folder in `archive_dir` where data for this experiment was saved.\n        comp: Component of CESM to load data from.&lt;/br&gt;\n            Options are:\n\n            * `atm`: atmosphere\n            * `ice`: ice\n            * `lnd`: land\n            * `rof`: river\n        archive_dir: Directory where CESM archive data saved.\n        hist_file: Which history file to load, `0` is the default monthly averaged data set.\n        chunks: Dictionary with keys given by dimension names and values given by chunk sizes\n            e.g. `{\"time\": 365, \"lat\": 50, \"lon\": 100}`.&lt;/br&gt;\n            Has big impact on memory usage. If `None`, no chunking is performed.\n        combine: Whether `xarray.combine_by_coords` or `xarray.combine_nested` is used to combine all the data.\n        concat_dim: Dimensions to concatenate files along.\n            You only need to provide this argument if combine='nested'.\n        parallel: Whether parallel loading is performed.\n        preprocess: Function to preprocess the data before loading.\n        decode_times: If `True`, will convert time to actual date.\n        year_files: Only files with these years in their name will be loaded. Leave as `None` to load all years.&lt;/br&gt;\n            As well as integer or list of integers, there are three string options:\n            * `'1975:1979'` will load in all years between 1975 and 1979 inclusive.\n            * `first5` will load in the first 5 years.\n            * `last5` will load in the last 5 years.\n        month_files: Only files with these months (1 is Jan) in their names will be loaded.\n            Leave as `None` to load all months.&lt;/br&gt;\n            Accepts same format as `year_files`.\n        ind_files: Will save the files with these indices, when ordered by date. Takes precident over\n            `year_files` and `month_files`.&lt;/br&gt;\n            Accepts same format as `year_files`.\n        apply_month_shift_fix: If `True`, will apply `ds_month_shift` before returning dataset.&lt;/br&gt;\n            Only used for monthly averaged data i.e. `hist_file=0`.\n        logger: Optional logger.\n\n    Returns:\n        Dataset containing all diagnostics specified for the experiment.\n    \"\"\"\n    exp_dir, comp_id = get_exp_dir(exp_name, comp, archive_dir)\n    # Only load in specific years and/or months\n    data_files_all = os.listdir(exp_dir)\n    # only keep files of correct format\n    data_files_all = [file for file in data_files_all if\n                      fnmatch.fnmatch(file, f'{exp_name}.{comp_id}.h{hist_file}.*.nc')]\n    n_files_all = len(data_files_all)\n    if (year_files is None) and (month_files is None) and (ind_files is None):\n        # Load all data in folder\n        # * indicates where date index info is, so we combine all datasets\n        data_files_load = os.path.join(exp_dir, f'{exp_name}.{comp_id}.h{hist_file}.*.nc')\n    elif ind_files is not None:\n        ind_files = parse_int_list(ind_files, format_func=lambda x: int(x), all_values=np.arange(n_files_all).tolist())\n        data_files_load = [os.path.join(exp_dir, file) for i, file in enumerate(data_files_all) if\n                           i in ind_files]\n    else:\n        if hist_file != 0 and month_files is not None:\n            warnings.warn(f'If h{hist_file} files not saved monthly then will not have a file for each month so '\n                          f'using months_keep={month_files} will miss out different days in different years.')\n        file_dates = get_exp_file_dates(exp_name, comp, archive_dir, hist_file)\n        year_files_all = np.unique(file_dates.dt.year).tolist()\n        if year_files is None:\n            year_files = year_files_all         # all possible years\n        else:\n            year_files = parse_int_list(year_files, format_func=lambda x: int(x), all_values=year_files_all)\n            years_request_missing = [x for x in year_files if x not in year_files_all]\n            if len(years_request_missing) &gt; 0:\n                warnings.warn(f'The requested years = {years_request_missing}\\n'\n                              f'are missing from the available years = {year_files_all}')\n        month_files_all = np.unique(file_dates.dt.month).tolist()\n        if month_files is None:\n            month_files = month_files_all       # all possible months\n        else:\n            month_files = parse_int_list(month_files, format_func= lambda x: int(x))\n            month_request_missing = [x for x in month_files if x not in month_files_all]\n            if len(month_request_missing) &gt; 0:\n                warnings.warn(f'The requested months = {month_request_missing}\\n'\n                              f'are missing from the available months = {month_files_all}')\n\n        file_ind_keep = [i for i in range(file_dates.size) if (file_dates.dt.year[i] in year_files)\n                         and (file_dates.dt.month[i] in month_files)]\n        if len(file_ind_keep) == 0:\n            raise ValueError(f'No files with requested years and months in file name\\n'\n                             f'Available years: {np.unique(file_dates.dt.year).tolist()}\\n'\n                             f'Available months: {np.unique(file_dates.dt.month).tolist()}\\n'\n                             f'Requested years: {year_files}\\n'\n                             f'Requested months: {month_files}\\n')\n\n        # only keep files with requested years and months in file name\n        data_files_load = [os.path.join(exp_dir, file) for i, file in enumerate(data_files_all) if\n                           i in file_ind_keep]\n    if logger:\n        if isinstance(data_files_load, str):\n            logger.info(f'Loading data from all {n_files_all} files: {data_files_load}')\n        else:\n            files_str = \"\\n\".join(data_files_load)\n            logger.info(f'Loading data from {len(data_files_load)}/{n_files_all} files:\\n{files_str}')\n    if apply_month_shift_fix and hist_file == 0:\n        ds = xr.open_mfdataset(data_files_load, decode_times=False, concat_dim=concat_dim,\n                               combine=combine, chunks=chunks, parallel=parallel, preprocess=preprocess)\n        return ds_month_shift(ds, decode_times)\n    else:\n        return xr.open_mfdataset(data_files_load, decode_times=decode_times,\n                                 concat_dim=concat_dim, combine=combine, chunks=chunks, parallel=parallel,\n                                 preprocess=preprocess)\n</code></pre>"},{"location":"code/cesm/load/#isca_tools.cesm.load.load_z2m","title":"<code>load_z2m(surf_geopotential_file=jasmin_surf_geopotential_file, var_reindex_like=None)</code>","text":"<p>Returns 2m geopotential height for CESM simulation.</p> <p>Parameters:</p> Name Type Description Default <code>surf_geopotential_file</code> <code>str</code> <p>File location of input data containing the geopotential at the surface: <code>PHIS</code>.</p> <code>jasmin_surf_geopotential_file</code> <code>var_reindex_like</code> <code>Optional[DataArray]</code> <p>Can provide a variable so <code>z2m</code> will have the same lat-lon as this variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>2m geopotential height in units of meters.</p> Source code in <code>isca_tools/cesm/load.py</code> <pre><code>def load_z2m(surf_geopotential_file: str = jasmin_surf_geopotential_file,\n             var_reindex_like: Optional[xr.DataArray] = None) -&gt; xr.DataArray:\n    \"\"\"\n    Returns 2m geopotential height for CESM simulation.\n\n    Args:\n        surf_geopotential_file: File location of input data containing the geopotential at the surface: `PHIS`.\n        var_reindex_like: Can provide a variable so `z2m` will have the same lat-lon as this variable.\n\n    Returns:\n        2m geopotential height in units of meters.\n    \"\"\"\n    # PHIS is the geopotential at the surface, so to get Z at reference height, divide by g and add 2\n    ds_z2m = xr.open_dataset(surf_geopotential_file)[['PHIS']]\n    z_refht = 2   # reference height is at 2m\n    ds_z2m['ZREFHT'] = ds_z2m['PHIS'] / g + z_refht               # PHIS is geopotential in m2/s2 so need to convert\n    del ds_z2m['PHIS']\n    if var_reindex_like is not None:\n        ds_z2m = ds_z2m.reindex_like(var_reindex_like, method=\"nearest\", tolerance=0.01)\n    ds_z2m = set_attrs(ds_z2m.ZREFHT, long_name='Geopotential height at reference height (2m)', units='m')\n    return ds_z2m\n</code></pre>"},{"location":"code/cesm/load/#isca_tools.cesm.load.parse_cesm_datetime","title":"<code>parse_cesm_datetime(time_str)</code>","text":"<p>Given a time string in the form either 'YYYY-MM' or 'YYYY-MM-DD-sssss' where <code>sssss</code> are the seconds since midnight, this will return the datetime object corresponding to that time.</p> <p>Parameters:</p> Name Type Description Default <code>time_str</code> <p>String to convert to datetime object.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>Datetime object corresponding to <code>time_str</code>.</p> Source code in <code>isca_tools/cesm/load.py</code> <pre><code>def parse_cesm_datetime(time_str) -&gt; datetime:\n    \"\"\"\n    Given a time string in the form either 'YYYY-MM' or 'YYYY-MM-DD-sssss' where `sssss` are the seconds since midnight,\n    this will return the datetime object corresponding to that time.\n\n    Args:\n        time_str: String to convert to datetime object.\n\n    Returns:\n        Datetime object corresponding to `time_str`.\n    \"\"\"\n    date_part = time_str[:10]               # 'YYYY-MM-DD'\n    if len(date_part) == 7:\n        return datetime.strptime(date_part, '%Y-%m')\n    else:\n        seconds_since_midnight = int(time_str[11:])  # e.g., 00000 \u2192 0 seconds, 04320 \u2192 01:12:00 (1 hour, 12 minutes)\n        base_date = datetime.strptime(date_part, '%Y-%m-%d')\n        return base_date + timedelta(seconds=seconds_since_midnight)\n</code></pre>"},{"location":"code/cesm/load/#isca_tools.cesm.load.select_months","title":"<code>select_months(ds, month_nh, month_sh=None)</code>","text":"<p>In dataset, keep only <code>month_nh</code> months in the northern hemisphere, and <code>month_sh</code> in the southern hemisphere.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset to select months from.</p> required <code>month_nh</code> <code>Union[ndarray, List[int]]</code> <p>List of months to keep in northern hemisphere.</p> required <code>month_sh</code> <code>Optional[Union[ndarray, List[int]]]</code> <p>List of months to keep in southern hemisphere. If <code>None</code>, will be the same as <code>month_nh</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset with months selected.</p> Source code in <code>isca_tools/cesm/load.py</code> <pre><code>def select_months(ds: xr.Dataset, month_nh: Union[np.ndarray, List[int]],\n                  month_sh: Optional[Union[np.ndarray, List[int]]] = None) -&gt; xr.Dataset:\n    \"\"\"\n    In dataset, keep only `month_nh` months in the northern hemisphere, and `month_sh` in the southern hemisphere.\n\n    Args:\n        ds: Dataset to select months from.\n        month_nh: List of months to keep in northern hemisphere.\n        month_sh: List of months to keep in southern hemisphere. If `None`, will be the same as `month_nh`.\n\n    Returns:\n        Dataset with months selected.\n    \"\"\"\n    # Select months for NH\n    if month_sh is None:\n        mask = ds.time.dt.month.isin(month_nh)\n    else:\n        mask_nh = (ds.lat &gt;= 0) &amp; (ds.time.dt.month.isin(month_nh))\n        mask_sh = (ds.lat &lt; 0) &amp; (ds.time.dt.month.isin(month_sh))\n        mask = mask_nh | mask_sh\n    return ds.where(mask)\n</code></pre>"},{"location":"code/convection/base/","title":"Base","text":""},{"location":"code/convection/base/#isca_tools.convection.base.convection_neutral_profile","title":"<code>convection_neutral_profile(temp_start, p_start, temp_lcl, p_levels)</code>","text":"<p>This returns the temperature of an air parcel at the given pressure levels, assuming it follows the <code>dry_profile</code> up until the <code>lcl_temp</code> has been reached, followed by the <code>moist_profile</code>.</p> <p>Parameters:</p> Name Type Description Default <code>temp_start</code> <code>float</code> <p>Starting temperature of parcel. Units: Kelvin.</p> required <code>p_start</code> <code>float</code> <p>Starting pressure of parcel. Units: Pa.</p> required <code>temp_lcl</code> <code>float</code> <p>Temperature of LCL in K.</p> required <code>p_levels</code> <code>ndarray</code> <p><code>float [n_p_levels]</code>. Pressure levels to find the temperature of the parcel at. Assumes p_levels is ascending i.e. starts in space. Units: Pa.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_p_levels]</code>. Temperature at each pressure level indicated by <code>p_levels</code>.</p> Source code in <code>isca_tools/convection/base.py</code> <pre><code>def convection_neutral_profile(temp_start: float, p_start: float, temp_lcl: float,\n                               p_levels: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    This returns the temperature of an air parcel at the given pressure levels, assuming it follows the\n    `dry_profile` up until the `lcl_temp` has been reached, followed by the `moist_profile`.\n\n    Args:\n        temp_start: Starting temperature of parcel. Units: *Kelvin*.\n        p_start: Starting pressure of parcel. Units: *Pa*.\n        temp_lcl: Temperature of *LCL* in *K*.\n        p_levels: `float [n_p_levels]`.&lt;/br&gt;\n            Pressure levels to find the temperature of the parcel at.\n            Assumes p_levels is ascending i.e. starts in space.\n            Units: *Pa*.\n\n    Returns:\n        `float [n_p_levels]`.&lt;/br&gt;\n            Temperature at each pressure level indicated by `p_levels`.\n\n    \"\"\"\n    p_lcl = dry_profile_pressure(temp_start, p_start, temp_lcl)\n    temp_dry = dry_profile_temp(temp_start, p_start, p_levels)       # Compute dry profile for all pressure values\n    temp_moist = moist_profile(temp_lcl, p_lcl, p_levels[p_levels &lt; p_lcl])\n    temp_dry[p_levels &lt; p_lcl] = temp_moist     # Replace dry temperature with moist for pressure below p_lcl\n    return temp_dry\n</code></pre>"},{"location":"code/convection/base/#isca_tools.convection.base.dry_profile_pressure","title":"<code>dry_profile_pressure(temp_start, p_start, temp_levels)</code>","text":"<p>Given a series of <code>temp_levels</code>, this function returns the pressure corresponding to each temperature, assuming it follows a dry adiabat.</p> <p>Can also use the pressure of the LCL if the LCL temperature is given as <code>temp_levels</code>.</p> <p>Parameters:</p> Name Type Description Default <code>temp_start</code> <code>float</code> <p>Starting temperature of parcel. Units: Kelvin.</p> required <code>p_start</code> <code>float</code> <p>Starting pressure of parcel. Units: Pa.</p> required <code>temp_levels</code> <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code>. Temperatures of parcel where we want to findthe corresponding pressure. Units: K.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code>. Pressure at each temperature indicated by <code>temp_levels</code>.</p> Source code in <code>isca_tools/convection/base.py</code> <pre><code>def dry_profile_pressure(temp_start: float, p_start: float,\n                         temp_levels: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Given a series of `temp_levels`, this function returns the pressure corresponding to each temperature, assuming\n    it follows a dry adiabat.\n\n    Can also use the pressure of the LCL if the LCL temperature is given as `temp_levels`.\n\n    Args:\n        temp_start: Starting temperature of parcel. Units: *Kelvin*.\n        p_start: Starting pressure of parcel. Units: *Pa*.\n        temp_levels: `float [n_p_levels]`.&lt;/br&gt;\n            Temperatures of parcel where we want to findthe corresponding pressure. Units: *K*.\n\n    Returns:\n        `float [n_p_levels]`.&lt;/br&gt;\n            Pressure at each temperature indicated by `temp_levels`.\n    \"\"\"\n    return p_start * (temp_levels / temp_start) ** (1/kappa)\n</code></pre>"},{"location":"code/convection/base/#isca_tools.convection.base.dry_profile_temp","title":"<code>dry_profile_temp(temp_start, p_start, p_levels)</code>","text":"<p>Returns the temperature of an air parcel at the given pressure levels, assuming it follows the dry adiabat.</p> <p>Parameters:</p> Name Type Description Default <code>temp_start</code> <code>float</code> <p>Starting temperature of parcel. Units: Kelvin.</p> required <code>p_start</code> <code>float</code> <p>Starting pressure of parcel. Units: Pa.</p> required <code>p_levels</code> <code>ndarray</code> <p><code>float [n_p_levels]</code>. Pressure levels to find the temperature of the parcel at. Units: Pa.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_p_levels]</code>. Temperature at each pressure level indicated by <code>p_levels</code>.</p> Source code in <code>isca_tools/convection/base.py</code> <pre><code>def dry_profile_temp(temp_start: float, p_start: float, p_levels: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Returns the temperature of an air parcel at the given pressure levels, assuming it follows the dry adiabat.\n\n    Args:\n        temp_start: Starting temperature of parcel. Units: *Kelvin*.\n        p_start: Starting pressure of parcel. Units: *Pa*.\n        p_levels: `float [n_p_levels]`.&lt;/br&gt;\n            Pressure levels to find the temperature of the parcel at. Units: *Pa*.\n\n    Returns:\n        `float [n_p_levels]`.&lt;/br&gt;\n            Temperature at each pressure level indicated by `p_levels`.\n    \"\"\"\n    return temp_start * (p_levels/p_start)**kappa\n</code></pre>"},{"location":"code/convection/base/#isca_tools.convection.base.equivalent_potential_temp","title":"<code>equivalent_potential_temp(temp, pressure, sphum=None, p_ref=100000.0)</code>","text":"<p>Returns the virtual potential temperature using equation 9.40 of Holton 2004 textbook.</p> <p>For a saturated parcel, this is \\(\\theta_e = \\theta \\exp (L_v q_s/c_pT)\\).</p> <p>For an unsaturated parcel (used if <code>sphum</code> given), it is \\(\\theta_e = \\theta \\exp (L_v q/c_pT_{LCL})\\).</p> <p>where \\(q\\) (\\(q_s\\)) is the (saturation) mixing ratio, \\(T_{LCL}\\) is the lifting condensation level temperature (using equation 10 from Bolton 1980) and \\(\\theta\\) is the potential temperature.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code> Temperature in K to find virtual potential temperature at.</p> required <code>pressure</code> <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code> Pressure levels in Pa corresponding to the temperatures given.</p> required <code>sphum</code> <code>Optional[float]</code> <p><code>float [n_p_levels]</code> Specific humidity of parcel at each pressure level in kg/kg. If not given, assumes saturated.</p> <code>None</code> <code>p_ref</code> <code>float</code> <p>Reference pressure in Pa used to compute the potential temperature</p> <code>100000.0</code> <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code> Equivalent potential temperature in K at given pressure levels.</p> Source code in <code>isca_tools/convection/base.py</code> <pre><code>def equivalent_potential_temp(temp: Union[float, np.ndarray], pressure: Union[float, np.ndarray],\n                              sphum: Optional[float] = None, p_ref: float = 1e5) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Returns the virtual potential temperature using equation 9.40 of *Holton 2004* textbook.\n\n    For a saturated parcel, this is $\\\\theta_e = \\\\theta \\\\exp (L_v q_s/c_pT)$.\n\n    For an unsaturated parcel (used if `sphum` given), it is $\\\\theta_e = \\\\theta \\\\exp (L_v q/c_pT_{LCL})$.\n\n    where $q$ ($q_s$) is the (saturation) mixing ratio, $T_{LCL}$ is the lifting condensation level temperature\n    (using equation 10 from *Bolton 1980*) and $\\\\theta$ is the potential temperature.\n\n    Args:\n        temp: `float [n_p_levels]`\n            Temperature in *K* to find virtual potential temperature at.\n        pressure: `float [n_p_levels]`\n            Pressure levels in *Pa* corresponding to the temperatures given.\n        sphum: `float [n_p_levels]`\n            Specific humidity of parcel at each pressure level in *kg/kg*. If not given, assumes saturated.\n        p_ref: Reference pressure in *Pa* used to compute the potential temperature\n\n    Returns:\n        `float [n_p_levels]`\n            Equivalent potential temperature in *K* at given pressure levels.\n    \"\"\"\n    if sphum is None:\n        mix_ratio = mixing_ratio_from_sphum(sphum_sat(temp, pressure))\n        temp_lcl = temp\n    else:\n        mix_ratio = mixing_ratio_from_sphum(sphum)\n        temp_lcl = lcl_temp_bolton(temp, rh_from_sphum(sphum, temp, pressure))\n    temp_pot = potential_temp(temp, pressure, p_ref)\n    return temp_pot * np.exp(L_v * mix_ratio / (c_p * temp_lcl))\n</code></pre>"},{"location":"code/convection/base/#isca_tools.convection.base.lapse_moist","title":"<code>lapse_moist(temp, total_pressure, pressure_coords=False)</code>","text":"<p>Returns the saturated moist adiabatic lapse rate, \\(\\Gamma_s = -dT/dz\\), at a given temperature.</p> <p>Comes from equation D.10 in Holton 2004.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>Union[float, ndarray]</code> <p>Temperature to compute lapse rate at. Units: Kelvin.</p> required <code>total_pressure</code> <code>Union[float, ndarray]</code> <p>Atmospheric pressure at altitude considered, \\(p\\), in Pa.</p> required <code>pressure_coords</code> <code>bool</code> <p>If <code>True</code>, will return \\(dT/dp\\), otherwise will return \\(-dT/dz\\).</p> <code>False</code> <p>Returns:     Saturated moist adiabatic lapse rate. Units: \\(Km^{-1}\\).</p> Source code in <code>isca_tools/convection/base.py</code> <pre><code>def lapse_moist(temp: Union[float, np.ndarray], total_pressure: Union[float, np.ndarray],\n                pressure_coords: bool = False) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Returns the saturated moist adiabatic lapse rate, $\\Gamma_s = -dT/dz$, at a given temperature.\n\n    Comes from equation D.10 in Holton 2004.\n\n    Args:\n        temp: Temperature to compute lapse rate at. Units: *Kelvin*.\n        total_pressure: Atmospheric pressure at altitude considered, $p$, in *Pa*.\n        pressure_coords: If `True`, will return $dT/dp$, otherwise will return $-dT/dz$.\n    Returns:\n        Saturated moist adiabatic lapse rate. Units: $Km^{-1}$.\n    \"\"\"\n    e_s = saturation_vapor_pressure(temp)\n    w_s = mixing_ratio_from_partial_pressure(e_s, total_pressure)   # saturation mixing ratio\n    neg_dT_dz = lapse_dry * (1 + L_v*w_s / (R * temp)) / (1 + epsilon * L_v**2*w_s/(c_p * R * temp**2))\n    if pressure_coords:\n        return R * temp * neg_dT_dz / (total_pressure * g)\n    else:\n        return neg_dT_dz\n</code></pre>"},{"location":"code/convection/base/#isca_tools.convection.base.lcl_metpy","title":"<code>lcl_metpy(temp_2m, sphum_2m, pressure_surf)</code>","text":"<p>Returns the pressure and temperature of the lifting condensation level, LCL.</p> <p>Parameters:</p> Name Type Description Default <code>temp_2m</code> <code>Union[float, ndarray, DataArray]</code> <p>Near-surface (2m) air temperature in Kelvin.</p> required <code>sphum_2m</code> <code>Union[float, ndarray, DataArray]</code> <p>Near-surface (2m) specific humidity in kg/kg.</p> required <code>pressure_surf</code> <code>Union[float, ndarray, DataArray]</code> <p>Surface pressure in Pa.</p> required <p>Returns:</p> Name Type Description <code>p_lcl</code> <code>Union[float, ndarray, DataArray]</code> <p>LCL pressure in Pa.</p> <code>temp_lcl</code> <code>Union[float, ndarray, DataArray]</code> <p>LCL temperature in Kelvin.</p> Source code in <code>isca_tools/convection/base.py</code> <pre><code>def lcl_metpy(temp_2m: Union[float, np.ndarray, xr.DataArray], sphum_2m: Union[float, np.ndarray, xr.DataArray],\n              pressure_surf: Union[float, np.ndarray, xr.DataArray]) -&gt; Tuple[Union[float, np.ndarray, xr.DataArray],\nUnion[float, np.ndarray, xr.DataArray]]:\n    \"\"\"\n    Returns the pressure and temperature of the lifting condensation level, LCL.\n\n    Args:\n        temp_2m: Near-surface (2m) air temperature in *Kelvin*.\n        sphum_2m: Near-surface (2m) specific humidity in *kg/kg*.\n        pressure_surf: Surface pressure in *Pa*.\n\n    Returns:\n        p_lcl: LCL pressure in *Pa*.\n        temp_lcl: LCL temperature in *Kelvin*.\n    \"\"\"\n    # Extract and convert to units\n    dims = None\n    coords = None\n    if isinstance(temp_2m, xr.DataArray):\n        dims = temp_2m.dims\n        coords = temp_2m.coords\n        temp_2m = temp_2m.values\n    if isinstance(sphum_2m, xr.DataArray):\n        if dims is None:\n            dims = sphum_2m.dims\n        if coords is None:\n            coords = sphum_2m.coords\n        sphum_2m = sphum_2m.values\n    if isinstance(pressure_surf, xr.DataArray):\n        if dims is None:\n            dims = pressure_surf.dims\n        if coords is None:\n            coords = pressure_surf.coords\n        pressure_surf = pressure_surf.values\n    q = sphum_2m * metpy.units.units('kg/kg')\n    T = temp_2m * metpy.units.units.kelvin\n    P = pressure_surf  * metpy.units.units.pascal\n\n    # Calculate dewpoint from mixing ratio\n    dewpoint = metpy.calc.dewpoint_from_specific_humidity(P, q)\n    lcl_pressure, lcl_temperature = metpy.calc.lcl(P, T, dewpoint)\n    if dims is None:\n        lcl_pressure = lcl_pressure.magnitude\n        lcl_temperature = lcl_temperature.magnitude\n    else:\n        # Convert MetPy Quantities to numpy arrays and units to attributes\n        lcl_pressure = xr.DataArray(\n            data=lcl_pressure.magnitude,\n            dims=dims,\n            coords=coords,\n            name='lcl_pressure',\n            attrs={'units': str(lcl_pressure.units), 'long_name': 'LCL pressure'}\n        )\n\n        lcl_temperature = xr.DataArray(\n            data=lcl_temperature.magnitude,\n            dims=dims,\n            coords=coords,\n            name='lcl_temperature',\n            attrs={'units': str(lcl_temperature.units), 'long_name': 'LCL temperature'}\n        )\n    return lcl_pressure, lcl_temperature\n</code></pre>"},{"location":"code/convection/base/#isca_tools.convection.base.lcl_temp_bolton","title":"<code>lcl_temp_bolton(temp_surf, rh_surf)</code>","text":"<p>Returns the temperature of the lifting condensation level, LCL, given the surface temperature and relative humidity.</p> <p>Equation comes from Equation 22 in Bolton 1980.</p> <p>Parameters:</p> Name Type Description Default <code>temp_surf</code> <code>ndarray</code> <p>Surface temperature in Kelvin.</p> required <code>rh_surf</code> <code>ndarray</code> <p>Percentage surface relative humidity (\\(0 &lt; rh &lt; 100\\)).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Temperature of LCL in Kelvin.</p> Source code in <code>isca_tools/convection/base.py</code> <pre><code>def lcl_temp_bolton(temp_surf: np.ndarray, rh_surf: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Returns the temperature of the lifting condensation level, *LCL*, given the surface\n    temperature and relative humidity.\n\n    Equation comes from *Equation 22* in *Bolton 1980*.\n\n    Args:\n        temp_surf: Surface temperature in *Kelvin*.\n        rh_surf: Percentage surface relative humidity ($0 &lt; rh &lt; 100$).\n\n    Returns:\n        Temperature of *LCL* in *Kelvin*.\n    \"\"\"\n    return 1 / (1/(temp_surf-55) - np.log(rh_surf/100)/2840) + 55\n</code></pre>"},{"location":"code/convection/base/#isca_tools.convection.base.moist_profile","title":"<code>moist_profile(temp_start, p_start, p_levels)</code>","text":"<p>Returns the temperature of an air parcel at the given pressure levels, assuming it follows the saturated moist adiabat.</p> <p>Parameters:</p> Name Type Description Default <code>temp_start</code> <code>float</code> <p>Starting temperature of parcel. Units: Kelvin.</p> required <code>p_start</code> <code>float</code> <p>Starting pressure of parcel. Units: Pa.</p> required <code>p_levels</code> <code>ndarray</code> <p><code>float [n_p_levels]</code>. Pressure levels to find the temperature of the parcel at. Assumes p_levels is ascending i.e. starts in space. Units: Pa.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_p_levels]</code>.</p> <code>ndarray</code> <p>Temperature at each pressure level indicated by <code>p_levels</code>.</p> Source code in <code>isca_tools/convection/base.py</code> <pre><code>def moist_profile(temp_start: float, p_start: float, p_levels: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Returns the temperature of an air parcel at the given pressure levels, assuming it follows the saturated moist\n    adiabat.\n\n    Args:\n        temp_start: Starting temperature of parcel. Units: *Kelvin*.\n        p_start: Starting pressure of parcel. Units: *Pa*.\n        p_levels: `float [n_p_levels]`.&lt;/br&gt;\n            Pressure levels to find the temperature of the parcel at.\n            Assumes p_levels is ascending i.e. starts in space.\n            Units: *Pa*.\n\n    Returns:\n        `float [n_p_levels]`.&lt;/br&gt;\n        Temperature at each pressure level indicated by `p_levels`.\n    \"\"\"\n\n    # Solve for all pressure levels with lower value than starting pressure\n    p_lower = p_levels[p_levels &lt;= p_start][::-1]\n    if len(p_lower) &gt; 0:\n        if p_start not in p_lower:\n            added_p_start = True\n            p_lower = np.concatenate(([p_start], p_lower))\n        else:\n            added_p_start = False\n        temp_lower = odeint(lapse_moist, temp_start, p_lower, args=(True,)).flatten()\n        if added_p_start:\n            # If had to add starting pressure, remove it so only have temperatures corresponding to pressures in\n            # p_levels.\n            temp_lower = temp_lower[1:]\n            p_lower = p_lower[1:]\n    else:\n        temp_lower = np.array([])\n\n    # Solve for all pressure levels with higher value than starting pressure\n    p_higher = p_levels[p_levels &gt; p_start]\n    if len(p_higher) &gt; 0:\n        if p_start not in p_higher:\n            added_p_start = True\n            p_higher = np.concatenate(([p_start], p_higher))\n        else:\n            added_p_start = False\n        temp_higher = odeint(lapse_moist, temp_start, p_higher, args=(True,)).flatten()\n        if added_p_start:\n            # If had to add starting pressure, remove it so only have temperatures corresponding to pressures in\n            # p_levels.\n            temp_higher = temp_higher[1:]\n            p_higher = p_higher[1:]\n    else:\n        temp_higher = np.array([])\n\n    # Combine all temperature values found\n    p_descend = np.concatenate((p_higher, p_lower))\n    temp_descend = np.concatenate((temp_higher, temp_lower))\n    # Rearrange temperature values so match how p_levels ordered in input.\n    temp_final = temp_descend[numpy_indexed.indices(p_descend, p_levels)].flatten()\n\n    # Old manual way of doing it\n    # p_done = np.asarray([p_start]).reshape(-1, 1)\n    # p_todo = p_levels\n    # temp_levels = np.zeros_like(p_levels)\n    # # For each pressure level, compute lapse rate based on pressure level closest to it and then update\n    # # temperature of that level assuming lapse rate stays constant between the levels.\n    # while len(p_todo) &gt; 0:\n    #     # Find the two pressure levels with the minimum distance between them such that on is in the set p_done\n    #     # and one is in the set p_todo.\n    #     # Want two closest as assuming lapse rate is constant between the levels\n    #     diff = np.abs(p_done - p_todo)\n    #     done_ind, todo_ind = np.argwhere(diff == np.min(diff))[0]\n    #     if done_ind == 0:\n    #         # First index is the inputted start values\n    #         p_ref = p_start\n    #         temp_ref = temp_start\n    #     else:\n    #         p_ind = np.where(p_levels == p_done[done_ind])[0]\n    #         p_ref = p_levels[p_ind]\n    #         temp_ref = temp_levels[p_ind]\n    #\n    #     # Compute lapse rate based on temperature and pressure of level in the done set.\n    #     dT_dp = lapse_moist(temp_ref, p_ref, True)\n    #     temp_level_ind = np.where(p_levels == p_todo[todo_ind])[0]\n    #\n    #     # Update temperature at level considering - assume constant lapse rate between levels.\n    #     temp_levels[temp_level_ind] = temp_ref + (p_levels[temp_level_ind]-p_ref) * dT_dp\n    #\n    #     # Transfer pressure level from p_todo to p_done for next iteration\n    #     p_done = np.append(p_done, np.asarray(p_todo[todo_ind]).reshape(-1, 1), axis=0)\n    #     p_todo = np.delete(p_todo, todo_ind)\n    return temp_final\n</code></pre>"},{"location":"code/convection/base/#isca_tools.convection.base.potential_temp","title":"<code>potential_temp(temp, pressure, p_ref=100000.0)</code>","text":"<p>Returns the potential temperature: \\(\\theta = T\\left(\\frac{p_{ref}}{p}\\right)^{\\kappa}\\)</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code> Temperature in K to find potential temperature at.</p> required <code>pressure</code> <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code> Pressure levels in Pa corresponding to the temperatures given.</p> required <code>p_ref</code> <code>float</code> <p>Reference pressure in Pa used to compute the potential temperature</p> <code>100000.0</code> <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code> Potential temperature in K at each pressure level.</p> Source code in <code>isca_tools/convection/base.py</code> <pre><code>def potential_temp(temp: Union[float, np.ndarray], pressure: Union[float, np.ndarray],\n                   p_ref: float = 1e5) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Returns the potential temperature: $\\\\theta = T\\\\left(\\\\frac{p_{ref}}{p}\\\\right)^{\\\\kappa}$\n\n    Args:\n        temp: `float [n_p_levels]`\n            Temperature in *K* to find potential temperature at.\n        pressure: `float [n_p_levels]`\n            Pressure levels in *Pa* corresponding to the temperatures given.\n        p_ref: Reference pressure in *Pa* used to compute the potential temperature\n\n    Returns:\n        `float [n_p_levels]`\n            Potential temperature in *K* at each pressure level.\n    \"\"\"\n    return temp * (p_ref / pressure)**kappa\n</code></pre>"},{"location":"code/convection/simple_betts_miller/","title":"Simple Betts-Miller","text":""},{"location":"code/convection/simple_betts_miller/#isca_tools.convection.simple_betts_miller.get_temp_ref","title":"<code>get_temp_ref(temp_start, p_start, sphum_start, p_full, p_ref=100000.0)</code>","text":"<p>This replicates the way the reference temperature profile is computed in Isca with the Simple Betts-Miller convection scheme.</p> <p>Below the LCL, it is set to a dry adiabat, and above it the <code>ref_temp_above_lcl</code> function is used, which is very similar to the moist adiabat.</p> <p>As well as the reference temperature profile, the LCL temperature and pressure are also returned.</p> <p>Parameters:</p> Name Type Description Default <code>temp_start</code> <code>float</code> <p>Starting temperature of parcel, \\(T_{start}\\). Units: Kelvin.</p> required <code>p_start</code> <code>float</code> <p>Pressure, \\(p_{start}\\), in Pa corresponding to starting point of dry ascent i.e. near surface pressure.</p> required <code>sphum_start</code> <code>float</code> <p>Starting specific humidity of parcel, \\(q_{start}\\). Units: kg/kg.</p> required <code>p_full</code> <code>ndarray</code> <p><code>float [n_p_levels]</code>. Full model pressure levels in ascending order. <code>p_full[0]</code> represents space and <code>p_full[-1]</code> is the surface.Units: Pa.</p> required <code>p_ref</code> <code>float</code> <p>Reference pressure, \\(p_{ref}\\) in Pa. It is a parameter in the <code>qe_moist_convection</code> namelist.</p> <code>100000.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>temp_ref</code>: <code>float [n_p_levels]</code> The reference temperature at each pressure level in <code>p_full</code>.</p> <code>float</code> <p><code>temp_lcl</code>: Temperature of LCL in K.</p> <code>float</code> <p><code>p_lcl</code>: Pressure of LCL in Pa.</p> Source code in <code>isca_tools/convection/simple_betts_miller.py</code> <pre><code>def get_temp_ref(temp_start: float, p_start: float, sphum_start: float,\n                 p_full: np.ndarray, p_ref: float = 1e5) -&gt; Tuple[np.ndarray, float, float]:\n    \"\"\"\n    This replicates the way the reference temperature profile is computed in Isca with the\n    [Simple Betts-Miller](https://jduffield65.github.io/Isca/namelists/convection/qe_moist_convection/)\n    convection scheme.\n\n    Below the LCL, it is set to a dry adiabat, and above it the `ref_temp_above_lcl` function is used, which is very\n    similar to the moist adiabat.\n\n    As well as the reference temperature profile, the LCL temperature and pressure are also returned.\n\n    Args:\n        temp_start: Starting temperature of parcel, $T_{start}$. Units: *Kelvin*.\n        p_start: Pressure, $p_{start}$, in *Pa* corresponding to starting point of dry ascent i.e. near surface\n            pressure.\n        sphum_start: Starting specific humidity of parcel, $q_{start}$. Units: *kg/kg*.\n        p_full: `float [n_p_levels]`.&lt;/br&gt;\n            Full model pressure levels in ascending order. `p_full[0]` represents space and `p_full[-1]` is\n            the surface.&lt;/br&gt;Units: *Pa*.\n        p_ref: Reference pressure, $p_{ref}$ in *Pa*. It is a parameter in the `qe_moist_convection` namelist.\n\n    Returns:\n        `temp_ref`: `float [n_p_levels]`&lt;/br&gt;\n            The reference temperature at each pressure level in `p_full`.\n        `temp_lcl`: Temperature of *LCL* in *K*.\n        `p_lcl`: Pressure of *LCL* in *Pa*.\n    \"\"\"\n    # Important that all variables are of correct datatype\n    if not isinstance(temp_start, float):\n        warnings.warn('Changing temp_start to a float')\n        temp_start = float(temp_start)\n    if not isinstance(p_start, float):\n        warnings.warn('Changing p_start to a float')\n        p_start = float(p_start)\n    if not isinstance(sphum_start, float):\n        warnings.warn('Changing sphum_start to a float')\n        sphum_start = float(sphum_start)\n    if not isinstance(p_ref, float):\n        warnings.warn('Changing p_ref to a float')\n        p_ref = float(p_ref)\n    if not isinstance(p_full, np.ndarray):\n        warnings.warn('Changing p_full to a numpy array')\n        p_full = np.asarray(p_full)\n\n    temp_lcl = lcl_temp(temp_start, p_start, sphum_start, p_ref)\n    p_lcl = dry_profile_pressure(temp_start, p_start, temp_lcl)\n    temp_ref = np.zeros(len(p_full))\n    temp_ref[p_full &gt;= p_lcl] = dry_profile_temp(temp_start, p_start, p_full[p_full &gt;= p_lcl])\n    temp_ref[p_full &lt; p_lcl] = ref_temp_above_lcl(temp_lcl, p_lcl, p_full[p_full &lt; p_lcl])\n    return temp_ref, temp_lcl, p_lcl\n</code></pre>"},{"location":"code/convection/simple_betts_miller/#isca_tools.convection.simple_betts_miller.lcl_temp","title":"<code>lcl_temp(temp_start, p_start, sphum_start, p_ref=100000.0)</code>","text":"<p>Function to replicate the way LCL temperature is computed in Isca with the Simple Betts-Miller convection scheme.</p> <p>It is based on two properties of dry ascent:</p> <p>Potential temperature is conserved so surface potential temperature = potential temperature at the \\(LCL\\):</p> \\[\\theta = \\theta_{start}(T_{start}, p_{start}) = T_{start}\\bigg(\\frac{p_{ref}}{p_{start}}\\bigg)^{\\kappa} = \\theta_{LCL}(T_{LCL}, p_{LCL}) = T_{LCL}\\bigg(\\frac{p_{ref}}{p_{LCL}}\\bigg)^{\\kappa}\\] <p>Mixing ratio, \\(w\\), is conserved in unsaturated adiabatic ascent because there is no precipitation, and at the \\(LCL\\), \\(w_{LCL} = w_{sat}\\) because by definition at the \\(LCL\\), the air is saturated:</p> \\[w = w_{start} = \\frac{q_{start}}{1-q_{start}} = w_{sat} = \\frac{\\epsilon e_s(T_{LCL})}{p_{LCL}-e_s(T_{LCL})}\\] <p>\\(q\\) is specific humidity, \\(\\epsilon = R_{dry}/R_v\\) is the ratio of gas constant for dry air to vapour and \\(\\kappa = R_{dry}/c_p\\). \\(p_{ref}\\) is taken to be \\(100,000 Pa\\) to be consistent with the value used in Isca.</p> <p>So we have two equations for two unknowns, \\(T_{LCL}\\) and \\(p_{LCL}\\). By eliminating \\(p_{LCL}\\), we can get an equation where RHS is just a function of \\(T_{LCL}\\) and the LHS consists only of known quantities:</p> \\[\\theta(T_{start})^{\\kappa}p_{ref} \\frac{w(q_{start})}{w(q_{start}) + \\epsilon} = T_{LCL}^{-1/\\kappa}e_s(T_{LCL})\\] <p>This can then be solved using Newton iteration to get the value of \\(T_{LCL}\\).</p> <p>Parameters:</p> Name Type Description Default <code>temp_start</code> <code>float</code> <p>Starting temperature of parcel, \\(T_{start}\\). Units: Kelvin.</p> required <code>p_start</code> <code>float</code> <p>Pressure, \\(p_{start}\\), in Pa corresponding to starting point of dry ascent i.e. near surface pressure.</p> required <code>sphum_start</code> <code>float</code> <p>Starting specific humidity of parcel, \\(q_{start}\\). Units: kg/kg.</p> required <code>p_ref</code> <code>float</code> <p>Reference pressure, \\(p_{ref}\\) in Pa. It is a parameter in the <code>qe_moist_convection</code> namelist.</p> <code>100000.0</code> <p>Returns:</p> Type Description <code>float</code> <p>Temperature of LCL in K.</p> Source code in <code>isca_tools/convection/simple_betts_miller.py</code> <pre><code>def lcl_temp(temp_start: float, p_start: float, sphum_start: float, p_ref: float = 1e5) -&gt; float:\n    \"\"\"\n    Function to replicate the way\n    [LCL temperature is computed](https://github.com/ExeClim/Isca/blob/9560521e1ba5ce27a13786ffdcb16578d0bd00da/src/\n    atmos_param/qe_moist_convection/qe_moist_convection.F90#L1092-L1130)\n    in *Isca* with the\n    [Simple Betts-Miller](https://jduffield65.github.io/Isca/namelists/convection/qe_moist_convection/)\n    convection scheme.\n\n    It is based on two properties of dry ascent:\n\n    Potential temperature is conserved so surface potential temperature = potential temperature at the $LCL$:\n\n    $$\\\\theta = \\\\theta_{start}(T_{start}, p_{start}) = T_{start}\\\\bigg(\\\\frac{p_{ref}}{p_{start}}\\\\bigg)^{\\kappa} =\n    \\\\theta_{LCL}(T_{LCL}, p_{LCL}) = T_{LCL}\\\\bigg(\\\\frac{p_{ref}}{p_{LCL}}\\\\bigg)^{\\kappa}$$\n\n    Mixing ratio, $w$, is conserved in unsaturated adiabatic ascent because there is no precipitation,\n    and at the $LCL$, $w_{LCL} = w_{sat}$ because by definition at the $LCL$, the air is saturated:\n\n    $$w = w_{start} = \\\\frac{q_{start}}{1-q_{start}} = w_{sat} = \\\\frac{\\epsilon e_s(T_{LCL})}{p_{LCL}-e_s(T_{LCL})}$$\n\n    $q$ is specific humidity, $\\epsilon = R_{dry}/R_v$ is the ratio of gas constant for dry air to vapour and\n    $\\kappa = R_{dry}/c_p$. $p_{ref}$ is taken to be $100,000 Pa$ to be consistent with the\n    [value used in Isca](https://github.com/ExeClim/Isca/blob/9560521e1ba5ce27a13786ffdcb16578d0bd00da/src/\n    atmos_param/qe_moist_convection/qe_moist_convection.F90#L74-L75).\n\n    So we have two equations for two unknowns, $T_{LCL}$ and $p_{LCL}$. By eliminating $p_{LCL}$,\n    we can get an equation where RHS is just a function of $T_{LCL}$ and the LHS consists only of known quantities:\n\n    $$\\\\theta(T_{start})^{\\kappa}p_{ref} \\\\frac{w(q_{start})}{w(q_{start}) + \\epsilon} =\n    T_{LCL}^{-1/\\kappa}e_s(T_{LCL})$$\n\n    This can then be solved using Newton iteration to get the value of $T_{LCL}$.\n\n    Args:\n        temp_start: Starting temperature of parcel, $T_{start}$. Units: *Kelvin*.\n        p_start: Pressure, $p_{start}$, in *Pa* corresponding to starting point of dry ascent i.e. near surface\n            pressure.\n        sphum_start: Starting specific humidity of parcel, $q_{start}$. Units: *kg/kg*.\n        p_ref: Reference pressure, $p_{ref}$ in *Pa*. It is a parameter in the `qe_moist_convection` namelist.\n\n    Returns:\n        Temperature of *LCL* in *K*.\n    \"\"\"\n    def lcl_opt_func(temp_lcl, p_start, temp_start, sphum_start, p_ref):\n        # Function to optimize\n        r = mixing_ratio_from_sphum(sphum_start)\n        theta = potential_temp(temp_start, p_start, p_ref)\n        # theta = temp_start * (p_ref / p_start) ** kappa\n        value = theta ** (-1 / kappa) * p_ref * r / (epsilon + r)\n\n        return value - saturation_vapor_pressure(temp_lcl) / temp_lcl ** (1 / kappa)\n    return optimize.newton(lcl_opt_func, 270, args=(p_start, temp_start, sphum_start, p_ref))\n</code></pre>"},{"location":"code/convection/simple_betts_miller/#isca_tools.convection.simple_betts_miller.ref_temp_above_lcl","title":"<code>ref_temp_above_lcl(temp_lcl, p_lcl, p_full)</code>","text":"<p>Function to replicate the way the reference temperature profile above the LCL is computed in Isca with the Simple Betts-Miller convection scheme.</p> <p>Parameters:</p> Name Type Description Default <code>temp_lcl</code> <code>float</code> <p>Temperature at the Lifting Condensation Level (LCL) in K.</p> required <code>p_lcl</code> <code>float</code> <p>Pressure of the LCL in Pa. This will not be one of the pressure levels in the model, and will be larger than any value in <code>p_full</code>.</p> required <code>p_full</code> <code>ndarray</code> <p><code>float [n_p_levels]</code>. Full model pressure levels in ascending order. <code>p_full[0]</code> represents space and <code>p_full[-1]</code> is the smallest pressure level in the model that is above the LCL.Units: Pa.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_p_levels]</code>. Reference temperature in K at each pressure level indicated in <code>p_full</code>.</p> Source code in <code>isca_tools/convection/simple_betts_miller.py</code> <pre><code>def ref_temp_above_lcl(temp_lcl: float, p_lcl: float, p_full: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Function to replicate the way the\n    [reference temperature profile above the LCL](https://github.com/ExeClim/Isca/blob/\n    7acc5d2c10bfa8f116d9e0f90d535a3067f898cd/src/atmos_param/qe_moist_convection/qe_moist_convection.F90#L621C7-L651)\n    is computed in *Isca* with the\n    [Simple Betts-Miller](https://jduffield65.github.io/Isca/namelists/convection/qe_moist_convection/)\n    convection scheme.\n\n    Args:\n        temp_lcl: Temperature at the Lifting Condensation Level (LCL) in *K*.\n        p_lcl: Pressure of the LCL in *Pa*. This will not be one of the pressure levels in the model, and will\n            be larger than any value in `p_full`.\n        p_full: `float [n_p_levels]`.&lt;/br&gt;\n            Full model pressure levels in ascending order. `p_full[0]` represents space and `p_full[-1]` is the smallest\n            pressure level in the model that is above the LCL.&lt;/br&gt;Units: *Pa*.\n\n    Returns:\n        `float [n_p_levels]`.&lt;/br&gt;\n            Reference temperature in *K* at each pressure level indicated in `p_full`.\n    \"\"\"\n    if np.ediff1d(p_full).min() &lt; 0:\n        raise ValueError('p_full needs to be in ascending order.')\n    if p_lcl &lt; p_full[-1]:\n        raise ValueError(f'p_lcl ({p_lcl}Pa) should be larger than the last value in p_full ({p_full[-1]}Pa).')\n\n    # Add LCL pressure to end of pressure array as it is the largest\n    p_full = np.concatenate((p_full, [p_lcl]))\n    temp_ref = np.zeros_like(p_full)\n    temp_ref[-1] = temp_lcl\n    mix_ratio_ref = np.zeros_like(p_full)\n    mix_ratio_lcl = mixing_ratio_from_partial_pressure(saturation_vapor_pressure(temp_lcl), p_lcl)\n    mix_ratio_ref[-1] = mix_ratio_lcl\n    # pressure is in ascending order so iterate from largest pressure (last index) which is LCL, to lowest\n    # (first index) which is space\n    for k in range(len(p_full) - 2, -1, -1):\n        # First get estimate for temperature and mixing ratio half-way between the pressure levels\n        # using larger pressure level\n        a = kappa * temp_ref[k+1] + L_v/c_p * mix_ratio_ref[k+1]\n        b = L_v**2 * mix_ratio_ref[k+1] / (c_p * R_v * temp_ref[k+1]**2)\n        dtlnp = a/(1+b)\n        temp_half = temp_ref[k+1] + dtlnp * np.log(p_full[k] / p_full[k + 1]) / 2\n        mix_ratio_half = mixing_ratio_from_partial_pressure(saturation_vapor_pressure(temp_half),\n                                                            (p_full[k] + p_full[k + 1]) / 2)\n\n        # Use halfway values to compute temperature at smaller pressure level\n        a = kappa * temp_half + L_v/c_p * mix_ratio_half\n        b = L_v**2 * mix_ratio_half / (c_p * R_v * temp_half**2)\n        dtlnp = a/(1+b)\n        temp_ref[k] = temp_ref[k+1] + dtlnp * np.log(p_full[k] / p_full[k+1])\n\n        # Use temperature at smaller pressure to compute mixing ratio at smaller pressure\n        mix_ratio_ref[k] = mixing_ratio_from_partial_pressure(temp_ref[k], p_full[k])\n    return temp_ref[:-1]        # don't return LCL value\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/core/","title":"Core","text":""},{"location":"code/era5/get_jasmin_era5/core/#isca_tools.era5.get_jasmin_era5.core.Find_era5","title":"<code>Find_era5</code>","text":"Source code in <code>isca_tools/era5/get_jasmin_era5/core.py</code> <pre><code>class Find_era5:\n    \"\"\"\n\n    \"\"\"\n    def __init__(self, archive: Literal[None, 1, 't'] = None):\n        \"\"\"\n        Initialise object to load ERA5 data from JASMIN\n        Args:\n            archive: There are three types of ERA5 archives:\n\n                * `None` to use default ERA5 archive at `/badc/ecmwf-era5`\n\n                * `1` to use ERA5.1 at `/badc/ecmwf-era51`,\n                    which is suggested for model level data in years 2000-2006 inclusive.\n\n                * `t` to use Preliminary at `/badc/ecmwf-era5t`, near real-time data\n        \"\"\"\n        self._init_vars(archive)\n        self.pl = Pressure_levels_era5(archive)\n        self.gz = Geopotential_levels_era5(archive)\n        self.enda = Ensemble_era5(archive)\n\n    def _init_vars(self, archive: Literal[None, 1, 't'] = None):\n        self.archive = '' if archive is None else str(archive)\n        self.path = pathlib.Path(f\"/badc/ecmwf-era5{self.archive}/data/\")\n        self._INVARIANTS = [\n            \"anor\",\n            \"cl\",\n            \"cvh\",\n            \"cvl\",\n            \"dl\",\n            \"isor\",\n            \"lsm\",\n            \"sdfor\",\n            \"sdor\",\n            \"slor\",\n            \"slt\",\n            \"tvh\",\n            \"tvl\",\n            \"z\",\n        ]\n        self._INVARIANT_DATE = datetime(2000, 1, 1)\n\n        self._ML_VARS = ['sp', 'lnsp', 'o3', 'q', 't', 'u', 'v', 'vo', 'z']       # variables on model levels\n        self._SURF_VARS = ['10u', '10v', '2d', '2t', 'asn', 'cape', 'ci',   # variables on surface level\n                           'msl', 'sd', 'skt', 'sst', 'tcc', 'tcwv']\n        self._ML_WARNING_YEARS = np.arange(2000, 2007).tolist()  # in these years model level data suffer from statospheric cold biases - should use ERA5.1\n\n    def __getitem__(self, args):\n        var = args[0]\n        date = args[1]\n        sel = {}\n        if len(args) &gt; 2 and args[2] is not None:\n            sel[\"level\"] = args[2]\n        if len(args) &gt; 3 and args[3] is not None:\n            sel[\"longitude\"] = args[3]\n        if len(args) &gt; 4 and args[4] is not None:\n            sel[\"latitude\"] = args[4]\n\n        if len(args) &gt; 5:\n            model = args[5]\n        else:\n            model = \"oper\"\n\n        if isinstance(date, slice):\n            if date.step is None:\n                freq = \"1h\"\n            else:\n                freq = date.step\n            dates = (\n                pd.date_range(\n                    pd.to_datetime(date.start),\n                    pd.to_datetime(date.stop),\n                    inclusive=\"left\",\n                    freq=freq,\n                )\n                .to_pydatetime()\n                .tolist()\n            )\n        else:\n            dates = [pd.to_datetime(date).to_pydatetime()]\n\n        if isinstance(var, str):\n            var = [var]\n\n        # If requested surface pressure, must get log of surface pressure first and convert later\n        # Record info here\n        sp_info = {'in_var': 'sp' in var}\n        if sp_info['in_var']:\n            var.remove('sp')\n            if 'lnsp' not in var:\n                # Requested just sp\n                var.append('lnsp')\n                sp_info['delete_lnsp'] = True\n            else:\n                # Requested lnsp and sp\n                sp_info['delete_lnsp'] = False\n\n        self.warn_missing_years(var, dates)\n\n        files = sum(\n            [\n                self.find_files(v, dates, model=model)\n                for v in var\n                if v not in self._INVARIANTS\n            ],\n            [],\n        )\n        ds = None\n        if len(files) &gt; 0:\n            ds = xr.open_mfdataset(files, combine=\"by_coords\")\n            ds = sel_era5(ds, sel)\n        invar_files = sum(\n            [\n                self.find_files(v, dates, model=model)\n                for v in var\n                if v in self._INVARIANTS\n            ],\n            [],\n        )\n        if len(invar_files) &gt; 0:\n            invar_ds = xr.open_mfdataset(invar_files, combine=\"by_coords\").squeeze(\n                drop=True\n            )\n            invar_ds = sel_era5(invar_ds, sel)\n            if len(files) &gt; 0:\n                for invar in invar_ds.data_vars:\n                    ds[invar] = invar_ds[invar]\n            else:\n                ds = invar_ds\n        if ds is None:\n            # If no data found\n            raise ValueError(f'No data found for ecmwf-era5{self.archive}, var={var} and date={date}.\\n'\n                             f'Model level variables = {self._ML_VARS}\\n'\n                             f'Surface variables = {self._SURF_VARS}\\n'\n                             f'Invariant variables = {self._INVARIANTS}')\n\n        if sp_info['in_var']:\n            ds = convert_lnsp_to_sp(ds, delete_lnsp=sp_info['delete_lnsp'])\n\n        return ds\n\n    def find_files(\n        self, var: str, dates: list[datetime], model: str = \"oper\"\n    ) -&gt; list[str]:\n        if var in self._INVARIANTS:\n            return self.find_invariant(var)\n        else:\n            return sum(\n                [self.find_single_file(var, date, model=model) for date in dates], []\n            )\n\n    def find_invariant(self, var: str) -&gt; list[str]:\n        if self.archive != '':\n            warnings.warn(f'Using base archive (ecmwf-era5), for invariant var={var} '\n                          f'despite requested archive of ecmwf-era5{self.archive}.')\n        date = self._INVARIANT_DATE\n        files = sorted(\n            list(\n                self.path.glob(\n                    f\"invariants/ecmwf-era5_oper_an_sfc_{date.year:04d}{date.month:02d}{date.day:02d}0000.{var}.inv.nc\"\n                )\n            )\n        )\n\n        return files\n\n    def find_single_file(\n        self, var: str, date: datetime, model: str = \"oper\"\n    ) -&gt; list[str]:\n        if model == \"enda\":\n            level_type = \"em_sfc\"\n        else:\n            level_type = \"*\"\n        files = sorted(\n            list(\n                self.path.glob(\n                    f\"{model}/{level_type}/{date.year:04d}/{date.month:02d}/{date.day:02d}/\"\n                    f\"ecmwf-era5{self.archive}_{model}_*_{date.year:04d}{date.month:02d}{date.day:02d}{date.hour:02d}*.{var}.nc\"\n                )\n            )\n        )\n        return files\n\n    def warn_missing_years(self, var: list[str], dates: list[datetime], model: str = \"oper\") -&gt; None:\n        \"\"\"\n        Warn about some years that might be missing model level data for ERA5 - print README in relevant directory\n        Args:\n            var: List of variables requested\n            dates: List of dates requested\n            model: Model requested\n\n        Returns:\n\n        \"\"\"\n        var_to_warn = [v for v in var if v in self._ML_VARS]\n        if self.archive=='' and (model == \"oper\") and (len(var_to_warn) &gt; 0):\n            years_to_warn = list({d.year for d in dates if d.year in self._ML_WARNING_YEARS})\n            if len(years_to_warn) &gt; 0:\n                dir_use = list(self.path.glob(f'oper/an_ml/{years_to_warn[0]}'))[0]\n                with open(f\"{dir_use}/00README\", \"r\") as f:\n                    contents = f.read()\n                warnings.warn(f\"README for year {years_to_warn[0]} (Can use ERA5.1 by setting `archive=1`):\\n{contents}\")\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/core/#isca_tools.era5.get_jasmin_era5.core.Find_era5.__init__","title":"<code>__init__(archive=None)</code>","text":"<p>Initialise object to load ERA5 data from JASMIN Args:     archive: There are three types of ERA5 archives:</p> <pre><code>    * `None` to use default ERA5 archive at `/badc/ecmwf-era5`\n\n    * `1` to use ERA5.1 at `/badc/ecmwf-era51`,\n        which is suggested for model level data in years 2000-2006 inclusive.\n\n    * `t` to use Preliminary at `/badc/ecmwf-era5t`, near real-time data\n</code></pre> Source code in <code>isca_tools/era5/get_jasmin_era5/core.py</code> <pre><code>def __init__(self, archive: Literal[None, 1, 't'] = None):\n    \"\"\"\n    Initialise object to load ERA5 data from JASMIN\n    Args:\n        archive: There are three types of ERA5 archives:\n\n            * `None` to use default ERA5 archive at `/badc/ecmwf-era5`\n\n            * `1` to use ERA5.1 at `/badc/ecmwf-era51`,\n                which is suggested for model level data in years 2000-2006 inclusive.\n\n            * `t` to use Preliminary at `/badc/ecmwf-era5t`, near real-time data\n    \"\"\"\n    self._init_vars(archive)\n    self.pl = Pressure_levels_era5(archive)\n    self.gz = Geopotential_levels_era5(archive)\n    self.enda = Ensemble_era5(archive)\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/core/#isca_tools.era5.get_jasmin_era5.core.Find_era5.warn_missing_years","title":"<code>warn_missing_years(var, dates, model='oper')</code>","text":"<p>Warn about some years that might be missing model level data for ERA5 - print README in relevant directory Args:     var: List of variables requested     dates: List of dates requested     model: Model requested</p> <p>Returns:</p> Source code in <code>isca_tools/era5/get_jasmin_era5/core.py</code> <pre><code>def warn_missing_years(self, var: list[str], dates: list[datetime], model: str = \"oper\") -&gt; None:\n    \"\"\"\n    Warn about some years that might be missing model level data for ERA5 - print README in relevant directory\n    Args:\n        var: List of variables requested\n        dates: List of dates requested\n        model: Model requested\n\n    Returns:\n\n    \"\"\"\n    var_to_warn = [v for v in var if v in self._ML_VARS]\n    if self.archive=='' and (model == \"oper\") and (len(var_to_warn) &gt; 0):\n        years_to_warn = list({d.year for d in dates if d.year in self._ML_WARNING_YEARS})\n        if len(years_to_warn) &gt; 0:\n            dir_use = list(self.path.glob(f'oper/an_ml/{years_to_warn[0]}'))[0]\n            with open(f\"{dir_use}/00README\", \"r\") as f:\n                contents = f.read()\n            warnings.warn(f\"README for year {years_to_warn[0]} (Can use ERA5.1 by setting `archive=1`):\\n{contents}\")\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/","title":"Utils","text":""},{"location":"code/era5/get_jasmin_era5/utils/#isca_tools.era5.get_jasmin_era5.utils.convert_lnsp_to_sp","title":"<code>convert_lnsp_to_sp(ds, delete_lnsp=True)</code>","text":"<p>Function to convert logarithm of surface pressure, into surface pressure with units of Pa</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset containing variable called <code>lnsp</code> to be converted.</p> required <code>delete_lnsp</code> <code>bool</code> <p>If True, delete the original <code>lnsp</code> variable.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing variable called <code>sp</code></p> Source code in <code>isca_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def convert_lnsp_to_sp(ds: xr.Dataset, delete_lnsp: bool = True) -&gt; xr.Dataset:\n    \"\"\"\n    Function to convert logarithm of surface pressure, into surface pressure with units of Pa\n\n    Args:\n        ds: Dataset containing variable called `lnsp` to be converted.\n        delete_lnsp: If True, delete the original `lnsp` variable.\n\n    Returns:\n        Dataset containing variable called `sp`\n    \"\"\"\n    if 'lnsp' not in ds:\n        print(\"Dataset does not contain variable called 'lnsp', returning original dataset.\")\n        return ds\n\n    # Compute surface pressure\n    sp = np.exp(ds['lnsp'])\n\n    # Rename and set attributes\n    sp.name = 'sp'\n    sp.attrs['long_name'] = 'surface pressure'\n    sp.attrs['units'] = 'Pa'\n\n    # Drop lnsp and add sp\n    if delete_lnsp:\n        ds = ds.drop_vars('lnsp')\n    ds['sp'] = sp\n\n    return ds\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/#isca_tools.era5.get_jasmin_era5.utils.get_ab","title":"<code>get_ab(n_levels)</code>","text":"<p>Returns ECMWF hybrid pressure level a and b coefficients for the specified level definition.</p> <p>Parameters:</p> Name Type Description Default <code>int </code> <p>: n_levels Number of levels to provide coefficients for. Must be one of the following ECMWF level definitions:     137, 91, 62, 60, 50, 40, 31, 19, 16</p> required <p>Returns:</p> Type Description <p>numpy array :: a (length = n_levels + 1) Floating point a coefficients in Pa</p> <p>numpy array :: b (length = n_levels + 1) Floating point b coefficients</p> Source code in <code>isca_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def get_ab(n_levels):\n    \"\"\"\n    --------------------------------------------------------------------------------\n    Returns ECMWF hybrid pressure level a and b coefficients for the specified\n    level definition.\n\n    Arguments:\n        int :: n_levels\n            Number of levels to provide coefficients for. Must be one of the\n            following ECMWF level definitions:\n                137, 91, 62, 60, 50, 40, 31, 19, 16\n\n    Returns:\n        numpy array :: a (length = n_levels + 1)\n            Floating point a coefficients in Pa\n        numpy array :: b (length = n_levels + 1)\n            Floating point b coefficients\n\n    --------------------------------------------------------------------------------\n    \"\"\"\n    level_list = [137, 91, 62, 60, 50, 40, 31, 19, 16]\n    if n_levels not in level_list:\n        raise Exception(\n            \"\"\"Get_ab: Number of levels input not recognised as a\n                        valid ECMWF level definition. n_levels must be one of\n                        the following: \"\"\"\n            + str(level_list)\n        )\n    if n_levels == 137:\n        a = [\n            0.0,\n            2.000365,\n            3.102241,\n            4.666084,\n            6.827977,\n            9.746966,\n            13.605424,\n            18.608931,\n            24.985718,\n            32.98571,\n            42.879242,\n            54.955463,\n            69.520576,\n            86.895882,\n            107.415741,\n            131.425507,\n            159.279404,\n            191.338562,\n            227.968948,\n            269.539581,\n            316.420746,\n            368.982361,\n            427.592499,\n            492.616028,\n            564.413452,\n            643.339905,\n            729.744141,\n            823.967834,\n            926.34491,\n            1037.201172,\n            1156.853638,\n            1285.610352,\n            1423.770142,\n            1571.622925,\n            1729.448975,\n            1897.519287,\n            2076.095947,\n            2265.431641,\n            2465.770508,\n            2677.348145,\n            2900.391357,\n            3135.119385,\n            3381.743652,\n            3640.468262,\n            3911.490479,\n            4194.930664,\n            4490.817383,\n            4799.149414,\n            5119.89502,\n            5452.990723,\n            5798.344727,\n            6156.074219,\n            6526.946777,\n            6911.870605,\n            7311.869141,\n            7727.412109,\n            8159.354004,\n            8608.525391,\n            9076.400391,\n            9562.682617,\n            10065.97852,\n            10584.63184,\n            11116.66211,\n            11660.06738,\n            12211.54785,\n            12766.87305,\n            13324.66895,\n            13881.33106,\n            14432.13965,\n            14975.61523,\n            15508.25684,\n            16026.11523,\n            16527.32227,\n            17008.78906,\n            17467.61328,\n            17901.62109,\n            18308.43359,\n            18685.71875,\n            19031.28906,\n            19343.51172,\n            19620.04297,\n            19859.39063,\n            20059.93164,\n            20219.66406,\n            20337.86328,\n            20412.30859,\n            20442.07813,\n            20425.71875,\n            20361.81641,\n            20249.51172,\n            20087.08594,\n            19874.02539,\n            19608.57227,\n            19290.22656,\n            18917.46094,\n            18489.70703,\n            18006.92578,\n            17471.83984,\n            16888.6875,\n            16262.04688,\n            15596.69531,\n            14898.45313,\n            14173.32422,\n            13427.76953,\n            12668.25781,\n            11901.33984,\n            11133.30469,\n            10370.17578,\n            9617.515625,\n            8880.453125,\n            8163.375,\n            7470.34375,\n            6804.421875,\n            6168.53125,\n            5564.382813,\n            4993.796875,\n            4457.375,\n            3955.960938,\n            3489.234375,\n            3057.265625,\n            2659.140625,\n            2294.242188,\n            1961.5,\n            1659.476563,\n            1387.546875,\n            1143.25,\n            926.507813,\n            734.992188,\n            568.0625,\n            424.414063,\n            302.476563,\n            202.484375,\n            122.101563,\n            62.78125,\n            22.835938,\n            3.757813,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.000007,\n            0.000024,\n            0.000059,\n            0.000112,\n            0.000199,\n            0.00034,\n            0.000562,\n            0.00089,\n            0.001353,\n            0.001992,\n            0.002857,\n            0.003971,\n            0.005378,\n            0.007133,\n            0.009261,\n            0.011806,\n            0.014816,\n            0.018318,\n            0.022355,\n            0.026964,\n            0.032176,\n            0.038026,\n            0.044548,\n            0.051773,\n            0.059728,\n            0.068448,\n            0.077958,\n            0.088286,\n            0.099462,\n            0.111505,\n            0.124448,\n            0.138313,\n            0.153125,\n            0.16891,\n            0.185689,\n            0.203491,\n            0.222333,\n            0.242244,\n            0.263242,\n            0.285354,\n            0.308598,\n            0.332939,\n            0.358254,\n            0.384363,\n            0.411125,\n            0.438391,\n            0.466003,\n            0.4938,\n            0.521619,\n            0.549301,\n            0.576692,\n            0.603648,\n            0.630036,\n            0.655736,\n            0.680643,\n            0.704669,\n            0.727739,\n            0.749797,\n            0.770798,\n            0.790717,\n            0.809536,\n            0.827256,\n            0.843881,\n            0.859432,\n            0.873929,\n            0.887408,\n            0.8999,\n            0.911448,\n            0.922096,\n            0.931881,\n            0.94086,\n            0.949064,\n            0.95655,\n            0.963352,\n            0.969513,\n            0.975078,\n            0.980072,\n            0.984542,\n            0.9885,\n            0.991984,\n            0.995003,\n            0.99763,\n            1,\n        ]\n    elif n_levels == 60:\n        a = [\n            0.0,\n            20.0,\n            38.425343,\n            63.647804,\n            95.636963,\n            134.483307,\n            180.584351,\n            234.779053,\n            298.495789,\n            373.971924,\n            464.618134,\n            575.651001,\n            713.218079,\n            883.660522,\n            1094.834717,\n            1356.474609,\n            1680.640259,\n            2082.273926,\n            2579.888672,\n            3196.421631,\n            3960.291504,\n            4906.708496,\n            6018.019531,\n            7306.631348,\n            8765.053711,\n            10376.12695,\n            12077.44629,\n            13775.3252,\n            15379.80566,\n            16819.47461,\n            18045.18359,\n            19027.69531,\n            19755.10938,\n            20222.20508,\n            20429.86328,\n            20384.48047,\n            20097.40234,\n            19584.33008,\n            18864.75,\n            17961.35742,\n            16899.46875,\n            15706.44727,\n            14411.12402,\n            13043.21875,\n            11632.75879,\n            10209.50098,\n            8802.356445,\n            7438.803223,\n            6144.314941,\n            4941.77832,\n            3850.91333,\n            2887.696533,\n            2063.779785,\n            1385.912598,\n            855.361755,\n            467.333588,\n            210.39389,\n            65.889244,\n            7.367743,\n            0.0,\n            0.0,\n        ]\n        b = [\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.000076,\n            0.000461,\n            0.001815,\n            0.005081,\n            0.011143,\n            0.020678,\n            0.034121,\n            0.05169,\n            0.073534,\n            0.099675,\n            0.130023,\n            0.164384,\n            0.202476,\n            0.243933,\n            0.288323,\n            0.335155,\n            0.383892,\n            0.433963,\n            0.484772,\n            0.53571,\n            0.586168,\n            0.635547,\n            0.683269,\n            0.728786,\n            0.771597,\n            0.811253,\n            0.847375,\n            0.879657,\n            0.907884,\n            0.93194,\n            0.951822,\n            0.967645,\n            0.979663,\n            0.98827,\n            0.994019,\n            0.99763,\n            1.0,\n        ]\n    elif n_levels == 91:\n        a = [\n            2.00004,\n            3.980832,\n            7.387186,\n            12.908319,\n            21.413612,\n            33.952858,\n            51.746601,\n            76.167656,\n            108.715561,\n            150.986023,\n            204.637451,\n            271.356506,\n            352.824493,\n            450.685791,\n            566.519226,\n            701.813354,\n            857.945801,\n            1036.166504,\n            1237.585449,\n            1463.16394,\n            1713.709595,\n            1989.87439,\n            2292.155518,\n            2620.898438,\n            2976.302246,\n            3358.425781,\n            3767.196045,\n            4202.416504,\n            4663.776367,\n            5150.859863,\n            5663.15625,\n            6199.839355,\n            6759.727051,\n            7341.469727,\n            7942.92627,\n            8564.624023,\n            9208.305664,\n            9873.560547,\n            10558.88184,\n            11262.48438,\n            11982.66211,\n            12713.89746,\n            13453.22559,\n            14192.00977,\n            14922.68555,\n            15638.05371,\n            16329.56055,\n            16990.62305,\n            17613.28125,\n            18191.0293,\n            18716.96875,\n            19184.54492,\n            19587.51367,\n            19919.79688,\n            20175.39453,\n            20348.91602,\n            20434.1582,\n            20426.21875,\n            20319.01172,\n            20107.03125,\n            19785.35742,\n            19348.77539,\n            18798.82227,\n            18141.29688,\n            17385.5957,\n            16544.58594,\n            15633.56641,\n            14665.64551,\n            13653.21973,\n            12608.38379,\n            11543.16699,\n            10471.31055,\n            9405.222656,\n            8356.25293,\n            7335.164551,\n            6353.920898,\n            5422.802734,\n            4550.21582,\n            3743.464355,\n            3010.146973,\n            2356.202637,\n            1784.854614,\n            1297.656128,\n            895.193542,\n            576.314148,\n            336.772369,\n            162.043427,\n            54.208336,\n            6.575628,\n            0.00316,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.000014,\n            0.000055,\n            0.000131,\n            0.000279,\n            0.000548,\n            0.001,\n            0.001701,\n            0.002765,\n            0.004267,\n            0.006322,\n            0.009035,\n            0.012508,\n            0.01686,\n            0.022189,\n            0.02861,\n            0.036227,\n            0.045146,\n            0.055474,\n            0.067316,\n            0.080777,\n            0.095964,\n            0.112979,\n            0.131935,\n            0.152934,\n            0.176091,\n            0.20152,\n            0.229315,\n            0.259554,\n            0.291993,\n            0.326329,\n            0.362203,\n            0.399205,\n            0.436906,\n            0.475016,\n            0.51328,\n            0.551458,\n            0.589317,\n            0.626559,\n            0.662934,\n            0.698224,\n            0.732224,\n            0.764679,\n            0.795385,\n            0.824185,\n            0.85095,\n            0.875518,\n            0.897767,\n            0.917651,\n            0.935157,\n            0.950274,\n            0.963007,\n            0.973466,\n            0.982238,\n            0.989153,\n            0.994204,\n            0.99763,\n            1,\n        ]\n    elif n_levels == 62:\n        a = [\n            0,\n            988.835876,\n            1977.67627,\n            2966.516602,\n            3955.356934,\n            4944.197266,\n            5933.037598,\n            6921.870117,\n            7909.441406,\n            8890.707031,\n            9860.52832,\n            10807.7832,\n            11722.74902,\n            12595.00684,\n            13419.46387,\n            14192.00977,\n            14922.68555,\n            15638.05371,\n            16329.56055,\n            16990.62305,\n            17613.28125,\n            18191.0293,\n            18716.96875,\n            19184.54492,\n            19587.51367,\n            19919.79688,\n            20175.39453,\n            20348.91602,\n            20434.1582,\n            20426.21875,\n            20319.01172,\n            20107.03125,\n            19785.35742,\n            19348.77539,\n            18798.82227,\n            18141.29688,\n            17385.5957,\n            16544.58594,\n            15633.56641,\n            14665.64551,\n            13653.21973,\n            12608.38379,\n            11543.16699,\n            10471.31055,\n            9405.222656,\n            8356.25293,\n            7335.164551,\n            6353.920898,\n            5422.802734,\n            4550.21582,\n            3743.464355,\n            3010.146973,\n            2356.202637,\n            1784.854614,\n            1297.656128,\n            895.193542,\n            576.314148,\n            336.772369,\n            162.043427,\n            54.208336,\n            6.575628,\n            0.00316,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.000013,\n            0.000087,\n            0.000275,\n            0.000685,\n            0.001415,\n            0.002565,\n            0.004187,\n            0.006322,\n            0.009035,\n            0.012508,\n            0.01686,\n            0.022189,\n            0.02861,\n            0.036227,\n            0.045146,\n            0.055474,\n            0.067316,\n            0.080777,\n            0.095964,\n            0.112979,\n            0.131935,\n            0.152934,\n            0.176091,\n            0.20152,\n            0.229315,\n            0.259554,\n            0.291993,\n            0.326329,\n            0.362203,\n            0.399205,\n            0.436906,\n            0.475016,\n            0.51328,\n            0.551458,\n            0.589317,\n            0.626559,\n            0.662934,\n            0.698224,\n            0.732224,\n            0.764679,\n            0.795385,\n            0.824185,\n            0.85095,\n            0.875518,\n            0.897767,\n            0.917651,\n            0.935157,\n            0.950274,\n            0.963007,\n            0.973466,\n            0.982238,\n            0.989153,\n            0.994204,\n            0.99763,\n            1,\n        ]\n    elif n_levels == 50:\n        a = [\n            0,\n            20.006149,\n            43.29781,\n            75.34623,\n            115.082146,\n            161.897491,\n            215.896912,\n            278.005798,\n            350.138184,\n            435.562286,\n            539.651489,\n            668.61554,\n            828.398987,\n            1026.366943,\n            1271.644531,\n            1575.537842,\n            1952.054443,\n            2418.549805,\n            2996.526611,\n            3712.626221,\n            4599.856934,\n            5699.114746,\n            6998.388184,\n            8507.411133,\n            10181.70703,\n            11883.08984,\n            13442.91504,\n            14736.35449,\n            15689.20606,\n            16266.60938,\n            16465.00391,\n            16297.62012,\n            15791.59766,\n            14985.26953,\n            13925.51953,\n            12665.29492,\n            11261.23047,\n            9771.40625,\n            8253.210938,\n            6761.339844,\n            5345.917969,\n            4050.71875,\n            2911.570312,\n            1954.804688,\n            1195.890625,\n            638.148438,\n            271.625,\n            72.0625,\n            0,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.0001,\n            0.000673,\n            0.003163,\n            0.009292,\n            0.020319,\n            0.036975,\n            0.059488,\n            0.087895,\n            0.122004,\n            0.161442,\n            0.205703,\n            0.254189,\n            0.306235,\n            0.361145,\n            0.418202,\n            0.476688,\n            0.535887,\n            0.595084,\n            0.653565,\n            0.710594,\n            0.765405,\n            0.817167,\n            0.864956,\n            0.907716,\n            0.944213,\n            0.972985,\n            0.992281,\n            1,\n        ]\n    elif n_levels == 40:\n        a = [\n            0,\n            2000,\n            4000,\n            6000,\n            8000,\n            9988.882838,\n            11914.52447,\n            13722.94294,\n            15369.73086,\n            16819.47627,\n            18045.18359,\n            19027.69448,\n            19755.10876,\n            20222.20531,\n            20429.86297,\n            20384.48143,\n            20097.40215,\n            19584.32924,\n            18864.75039,\n            17961.35774,\n            16899.46879,\n            15706.44732,\n            14411.12426,\n            13043.21862,\n            11632.75836,\n            10209.50134,\n            8802.356155,\n            7438.803092,\n            6144.315003,\n            4941.778213,\n            3850.913422,\n            2887.696603,\n            2063.779905,\n            1385.912553,\n            855.36175,\n            467.333577,\n            210.393894,\n            65.889243,\n            7.367743,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.000197,\n            0.001511,\n            0.004884,\n            0.011076,\n            0.020678,\n            0.034121,\n            0.05169,\n            0.073534,\n            0.099675,\n            0.130023,\n            0.164384,\n            0.202476,\n            0.243933,\n            0.288323,\n            0.335155,\n            0.383892,\n            0.433963,\n            0.484772,\n            0.53571,\n            0.586168,\n            0.635547,\n            0.683269,\n            0.728786,\n            0.771597,\n            0.811253,\n            0.847375,\n            0.879657,\n            0.907884,\n            0.93194,\n            0.951822,\n            0.967645,\n            0.979663,\n            0.98827,\n            0.994019,\n            0.99763,\n            1,\n        ]\n    elif n_levels == 31:\n        a = [\n            0,\n            2000,\n            4000,\n            6000,\n            8000,\n            9976.135361,\n            11820.53962,\n            13431.39393,\n            14736.35691,\n            15689.20746,\n            16266.6105,\n            16465.00573,\n            16297.61933,\n            15791.5986,\n            14985.26963,\n            13925.51786,\n            12665.29166,\n            11261.22888,\n            9771.40629,\n            8253.212096,\n            6761.341326,\n            5345.91424,\n            4050.717678,\n            2911.569385,\n            1954.805296,\n            1195.889791,\n            638.148911,\n            271.626545,\n            72.063577,\n            0,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.000391,\n            0.00292,\n            0.009194,\n            0.020319,\n            0.036975,\n            0.059488,\n            0.087895,\n            0.122004,\n            0.161442,\n            0.205703,\n            0.254189,\n            0.306235,\n            0.361145,\n            0.418202,\n            0.476688,\n            0.535887,\n            0.595084,\n            0.653565,\n            0.710594,\n            0.765405,\n            0.817167,\n            0.864956,\n            0.907716,\n            0.944213,\n            0.972985,\n            0.992281,\n            1,\n        ]\n    elif n_levels == 19:\n        a = [\n            2000,\n            4000,\n            6046.110595,\n            8267.92756,\n            10609.51323,\n            12851.10017,\n            14698.49809,\n            15861.12518,\n            16116.23661,\n            15356.92412,\n            13621.4604,\n            11101.56199,\n            8127.144155,\n            5125.141747,\n            2549.969411,\n            783.195032,\n            0,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0.000339,\n            0.003357,\n            0.01307,\n            0.034077,\n            0.07065,\n            0.125917,\n            0.201195,\n            0.29552,\n            0.405409,\n            0.524932,\n            0.646108,\n            0.759698,\n            0.856438,\n            0.928747,\n            0.972985,\n            0.992281,\n            1,\n        ]\n    elif n_levels == 16:\n        a = [\n            0,\n            5000,\n            9890.52,\n            14166.3,\n            17346.07,\n            19121.15,\n            19371.25,\n            18164.47,\n            15742.18,\n            12488.05,\n            8881.824,\n            5437.539,\n            2626.258,\n            783.2966,\n            0,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0.001721,\n            0.013198,\n            0.042217,\n            0.093762,\n            0.169571,\n            0.268016,\n            0.384274,\n            0.510831,\n            0.638268,\n            0.756385,\n            0.855613,\n            0.928746,\n            0.972985,\n            0.992282,\n            1,\n        ]\n    a = np.array(a).astype(\"float\")\n    b = np.array(b).astype(\"float\")\n    return a, b\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/#isca_tools.era5.get_jasmin_era5.utils.get_dp","title":"<code>get_dp(ps, n_levels)</code>","text":"<p>Returns difference in pressure between levels for ECMWF hybrid pressure levels.</p> <p>Parameters:</p> Name Type Description Default <code>numpy array or scalar </code> <p>: ps Surface pressure in Pa</p> required <code>int </code> <p>: n_levels Number of levels to provide coefficients for. Must be one of the following ECMWF level definitions:     137, 91, 62, 60, 50, 40, 31, 19, 16</p> required <p>Returns:</p> Type Description <p>numpy array :: pl (shape = (n_levels, ps.shape) Floating point array of pressure differential between full levels in units Pa</p> Source code in <code>isca_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def get_dp(ps, n_levels):\n    \"\"\"\n    --------------------------------------------------------------------------------\n    Returns difference in pressure between levels for ECMWF hybrid pressure\n    levels.\n\n    Arguments:\n        numpy array or scalar :: ps\n            Surface pressure in Pa\n        int :: n_levels\n            Number of levels to provide coefficients for. Must be one of the\n            following ECMWF level definitions:\n                137, 91, 62, 60, 50, 40, 31, 19, 16\n\n    Returns:\n        numpy array :: pl (shape = (n_levels, ps.shape)\n            Floating point array of pressure differential between full levels in\n            units Pa\n\n    --------------------------------------------------------------------------------\n    \"\"\"\n    ph = get_ph(ps, n_levels)\n    dp = ph[1:] - ph[:-1]\n    return dp\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/#isca_tools.era5.get_jasmin_era5.utils.get_gz","title":"<code>get_gz(ps, gzs, T, q, n_levels)</code>","text":"<p>Calculates the geopotential on model levels for ECMWF hybrid pressure levels.</p> <p>Parameters:</p> Name Type Description Default <code>numpy array or scalar </code> <p>: ps Surface pressure in Pa</p> required <code>numpy array or scalar </code> <p>: gzs Surface geopotential in m2 s-2</p> required <code>numpy array </code> <p>: T Atmospheric temperature on levels in K. Must have shape (n_levels, ps.shape)</p> required <code>numpy array </code> <p>: q Atmospheric specific humidity on levels in kg kg**-1. Must have shape (n_levels, ps.shape)</p> required <code>int </code> <p>: n_levels Number of levels to provide coefficients for. Must be one of the following ECMWF level definitions:     137, 91, 62, 60, 50, 40, 31, 19, 16</p> required <p>Returns:</p> Type Description <p>numpy array :: gzf (shape = (n_levels, ps.shape) Floating point array of geopotential values on full levels in units m2 s-2</p> Source code in <code>isca_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def get_gz(ps, gzs, T, q, n_levels):\n    \"\"\"\n    --------------------------------------------------------------------------------\n    Calculates the geopotential on model levels for ECMWF hybrid pressure\n    levels.\n\n    Arguments:\n        numpy array or scalar :: ps\n            Surface pressure in Pa\n        numpy array or scalar :: gzs\n            Surface geopotential in m**2 s**-2\n        numpy array :: T\n            Atmospheric temperature on levels in K. Must have shape (n_levels,\n            ps.shape)\n        numpy array :: q\n            Atmospheric specific humidity on levels in kg kg**-1. Must have\n            shape (n_levels, ps.shape)\n        int :: n_levels\n            Number of levels to provide coefficients for. Must be one of the\n            following ECMWF level definitions:\n                137, 91, 62, 60, 50, 40, 31, 19, 16\n\n    Returns:\n        numpy array :: gzf (shape = (n_levels, ps.shape)\n            Floating point array of geopotential values on full levels in units\n            m**2 s**-2\n\n    --------------------------------------------------------------------------------\n    \"\"\"\n    # Check that the dimensions of all inputs are consistent\n    try:\n        ps_shape = ps.shape\n    except:\n        if hasattr(ps, \"__iter__\"):\n            raise Exception(\n                \"\"\"get_gz: ps argument must be a numpy array or\n                            scalar value\"\"\"\n            )\n        ps_shape = ()\n    try:\n        gzs_shape = gzs.shape\n    except:\n        if hasattr(gzs, \"__iter__\"):\n            raise Exception(\n                \"\"\"get_gz: gzs argument must be a numpy array or\n                            scalar value\"\"\"\n            )\n        gzs_shape = ()\n    if len(gzs_shape) == len(ps_shape):\n        if gzs_shape != ps_shape:\n            if len(gzs_shape) != 0 and len(ps_shape) != 0:\n                raise Exception(\n                    \"\"\"get_gz: ps and gzs inputs must have the same\n                                shape or have scalar values\"\"\"\n                )\n    elif len(gzs_shape) == (len(ps_shape) - 1):\n        if gzs_shape != ps_shape[1:]:\n            if len(gzs_shape) != 0 and len(ps_shape) != 0:\n                raise Exception(\n                    \"\"\"get_gz: ps and gzs inputs must have the same\n                                shape or have scalar values\"\"\"\n                )\n    else:\n        raise Exception(\n            \"\"\"get_gz: ps and gzs inputs must have the same\n                                shape or have scalar values\"\"\"\n        )\n    if len(ps_shape) != 0:\n        out_shape = (n_levels,) + ps_shape\n    elif len(gzs_shape) != 0:\n        out_shape = (n_levels,) + gzs_shape\n    else:\n        out_shape = (n_levels,)\n    try:\n        if T.shape != q.shape:\n            raise Exception(\n                \"\"\"get_gz: T and q arguments must have the same\n                            shape\"\"\"\n            )\n    except:\n        raise Exception(\n            \"\"\"get_gz: Cannot get shape attribute of T and/or q\n                        arguments\"\"\"\n        )\n    else:\n        for dim in T.shape:\n            if dim not in out_shape:\n                raise Exception(\n                    \"\"\"get_gz: Shape of T not compatible with ps and\n                                and gzs arguments\"\"\"\n                )\n        for dim in q.shape:\n            if dim not in out_shape:\n                raise Exception(\n                    \"\"\"get_gz: Shape of T not compatible with ps and\n                                and gzs arguments\"\"\"\n                )\n\n    # Find axis index of height dimension\n    h_axis = [i for i, ax in enumerate(out_shape) if ax not in ps_shape][0]\n    ps_reshape = list(out_shape)\n    ps_reshape[h_axis] = 1\n    ps_reshape = tuple(ps_reshape)\n\n    # Get half level pressure, full level pressure and delta p\n    ph = get_ph(ps, n_levels)\n    pl = get_pl(ps, n_levels)\n    dp = get_dp(ps, n_levels)\n    # dlogP -- change in logP\n    dlogP = np.zeros(out_shape)\n    dlogP[1:] = np.log(ph[2:] / ph[1:-1])\n    dlogP[0] = np.log(ph[1] / pl[0])\n    # alpha factor for calculating geopotential\n    alpha = 1.0 - (ph[1:] / dp) * dlogP\n    alpha[0] = np.log(2)\n    # Get moist temperature at each level\n    Tm = T * (1.0 + 0.609133 * q)\n    TRd = Tm * 287.06\n    # Get geopotential half levels\n    gzh = np.cumsum((TRd * dlogP)[::-1], axis=h_axis)[::-1] + gzs\n    gzf = gzh + TRd * alpha\n    return gzf\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/#isca_tools.era5.get_jasmin_era5.utils.get_ph","title":"<code>get_ph(ps, n_levels)</code>","text":"<p>Returns pressure on half levels for ECMWF hybrid pressure levels.</p> <p>Parameters:</p> Name Type Description Default <code>numpy array or scalar </code> <p>: ps Surface pressure in Pa</p> required <code>int </code> <p>: n_levels Number of levels to provide coefficients for. Must be one of the following ECMWF level definitions:     137, 91, 62, 60, 50, 40, 31, 19, 16</p> required <p>Returns:</p> Type Description <p>numpy array :: ph (shape = (n_levels + 1, ps.shape) Floating point array of pressure values on half levels in units Pa</p> Source code in <code>isca_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def get_ph(ps, n_levels):\n    \"\"\"\n    --------------------------------------------------------------------------------\n    Returns pressure on half levels for ECMWF hybrid pressure levels.\n\n    Arguments:\n        numpy array or scalar :: ps\n            Surface pressure in Pa\n        int :: n_levels\n            Number of levels to provide coefficients for. Must be one of the\n            following ECMWF level definitions:\n                137, 91, 62, 60, 50, 40, 31, 19, 16\n\n    Returns:\n        numpy array :: ph (shape = (n_levels + 1, ps.shape)\n            Floating point array of pressure values on half levels in units Pa\n\n    --------------------------------------------------------------------------------\n    \"\"\"\n    a, b = get_ab(n_levels)\n    try:\n        ps_shape = ps.shape\n    except:\n        if hasattr(ps, \"__iter__\"):\n            raise Exception(\n                \"\"\"get_ph: ps argument must be a numpy array or\n                            scalar value\"\"\"\n            )\n        ps_shape = ()\n    level_shape = np.ones(len(ps_shape) + 1).astype(\"int\")\n    level_shape[0] = n_levels + 1\n    level_shape = tuple(level_shape)\n    ph = a.reshape(level_shape) + b.reshape(level_shape) * ps\n    return ph\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/#isca_tools.era5.get_jasmin_era5.utils.get_pl","title":"<code>get_pl(ps, n_levels)</code>","text":"<p>Returns pressure on full levels for ECMWF hybrid pressure levels.</p> <p>Parameters:</p> Name Type Description Default <code>numpy array or scalar </code> <p>: ps Surface pressure in Pa</p> required <code>int </code> <p>: n_levels Number of levels to provide coefficients for. Must be one of the following ECMWF level definitions:     137, 91, 62, 60, 50, 40, 31, 19, 16</p> required <p>Returns:</p> Type Description <p>numpy array :: pl (shape = (n_levels, ps.shape) Floating point array of pressure on full levels in units Pa</p> Source code in <code>isca_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def get_pl(ps, n_levels):\n    \"\"\"\n    --------------------------------------------------------------------------------\n    Returns pressure on full levels for ECMWF hybrid pressure levels.\n\n    Arguments:\n        numpy array or scalar :: ps\n            Surface pressure in Pa\n        int :: n_levels\n            Number of levels to provide coefficients for. Must be one of the\n            following ECMWF level definitions:\n                137, 91, 62, 60, 50, 40, 31, 19, 16\n\n    Returns:\n        numpy array :: pl (shape = (n_levels, ps.shape)\n            Floating point array of pressure on full levels in units Pa\n\n    --------------------------------------------------------------------------------\n    \"\"\"\n    ph = get_ph(ps, n_levels)\n    pl = (ph[1:] + ph[:-1]) * 0.5\n    return pl\n</code></pre>"},{"location":"code/jasmin/run/base/","title":"Run","text":""},{"location":"code/jasmin/run/base/#isca_tools.jasmin.run.base.add_list_to_str","title":"<code>add_list_to_str(var_str, var_list)</code>","text":"<p>Adds all entries in <code>var_list</code> to end of <code>var_str</code>. If <code>var_str = 'hello'</code> and <code>var_list = ['Bob', 'Alice']</code> then will return <code>'hello Bob Alice'</code>.</p> <p>Parameters:</p> Name Type Description Default <code>var_str</code> <code>str</code> <p>Starting string, to which <code>var_list</code> will be added.</p> required <code>var_list</code> <code>Optional[Union[List, float, int, str]]</code> <p>List of entries to add to <code>var_str</code>. If one of entries is itself a list, will convert that entry into comma separated string.</p> required <p>Returns:</p> Name Type Description <code>var_str</code> <code>str</code> <p>Initial <code>var_str</code> with <code>var_list</code> entries added.</p> Source code in <code>isca_tools/jasmin/run/base.py</code> <pre><code>def add_list_to_str(var_str: str, var_list: Optional[Union[List, float, int, str]]) -&gt; str:\n    \"\"\"\n    Adds all entries in `var_list` to end of `var_str`.&lt;/br&gt;\n    If `var_str = 'hello'` and `var_list = ['Bob', 'Alice']` then will return `'hello Bob Alice'`.\n\n    Args:\n        var_str: Starting string, to which `var_list` will be added.\n        var_list: List of entries to add to `var_str`.&lt;/br&gt;\n            If one of entries is itself a list, will convert that entry into comma separated string.\n\n    Returns:\n        var_str: Initial `var_str` with `var_list` entries added.\n    \"\"\"\n    if isinstance(var_list, list):\n        for arg in var_list:\n            if isinstance(arg, list):\n                # If one of the arguments is a list, convert into comma separated string so all info stil passed\n                arg_comma_sep = \",\".join(str(x) for x in arg)\n                var_str += f\" {arg_comma_sep}\"\n            else:\n                var_str += f\" {arg}\"\n    else:\n        var_str += f\" {var_list}\"\n    return var_str\n</code></pre>"},{"location":"code/jasmin/run/base/#isca_tools.jasmin.run.base.get_slurm_info_from_file","title":"<code>get_slurm_info_from_file(input_file_path)</code>","text":"<p>Reads in a <code>.nml</code> file with a <code>slurm_info</code> section, containing information about python script to run on slurm.</p> <p>Parameters:</p> Name Type Description Default <code>input_file_path</code> <code>str</code> <p>The <code>.nml</code> file path, with <code>slurm_info</code> section containing.</p> required <p>Returns:</p> Name Type Description <code>info</code> <code>dict</code> <p>Dictionary containing all entries in <code>slurm_info</code> section of <code>input_file_path</code>. If <code>job_name</code> not provided, will set to directory containing <code>input_file_path</code>. If <code>script_args</code> not provided, will set to <code>input_file_path</code>.</p> Source code in <code>isca_tools/jasmin/run/base.py</code> <pre><code>def get_slurm_info_from_file(input_file_path: str) -&gt; dict:\n    \"\"\"\n    Reads in a `.nml` file with a `slurm_info` section, containing information about python script to run on slurm.\n\n    Args:\n        input_file_path: The `.nml` file path, with `slurm_info` section containing.\n\n    Returns:\n        info: Dictionary containing all entries in `slurm_info` section of `input_file_path`.&lt;/br&gt;\n            If `job_name` not provided, will set to directory containing `input_file_path`.&lt;/br&gt;\n            If `script_args` not provided, will set to `input_file_path`.\n    \"\"\"\n    info = f90nml.read(input_file_path)['slurm_info']\n    if info['script_args'] is None:\n        # Default arg for script is address to this input file\n        info['script_args'] = input_file_path\n    if info['job_name'] is None:\n        # Default job name is directory containing input file, without the jobs_dir\n        info['job_name'] = os.path.dirname(input_file_path).replace(info['jobs_dir'], '')\n    if info['job_name'][0] == '/':\n        info['job_name'] = info['job_name'][1:]  # make sure does not start with '/'\n    return info\n</code></pre>"},{"location":"code/jasmin/run/base/#isca_tools.jasmin.run.base.get_unique_dir_name","title":"<code>get_unique_dir_name(base_dir)</code>","text":"<p>Return a unique directory name by appending a number if needed. E.g., 'results', 'results_1', 'results_2', ...</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str</code> <p>Path to directory</p> required <p>Returns:</p> Name Type Description <code>base_dir</code> <code>str</code> <p>Unique directory name</p> Source code in <code>isca_tools/jasmin/run/base.py</code> <pre><code>def get_unique_dir_name(base_dir: str) -&gt; str:\n    \"\"\"\n    Return a unique directory name by appending a number if needed.\n    E.g., 'results', 'results_1', 'results_2', ...\n\n    Args:\n        base_dir: Path to directory\n\n    Returns:\n        base_dir: Unique directory name\n    \"\"\"\n    if not os.path.exists(base_dir):\n        return base_dir\n\n    i = 1\n    while True:\n        new_dir = f\"{base_dir}_{i}\"\n        if not os.path.exists(new_dir):\n            return new_dir\n        i += 1\n</code></pre>"},{"location":"code/jasmin/run/base/#isca_tools.jasmin.run.base.import_func_from_path","title":"<code>import_func_from_path(script_path, func_name='main', module_name='my_dynamic_module')</code>","text":"<p>Loads in the function called <code>func_name</code> from <code>script_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>script_path</code> <code>str</code> <p>Path to python <code>.py</code> file which contains <code>func_name</code>.</p> required <code>func_name</code> <code>str</code> <p>Function name to import.</p> <code>'main'</code> <code>module_name</code> <code>str</code> <p>Temporary name for module being loaded dynamically. Could be anything.</p> <code>'my_dynamic_module'</code> <p>Returns:</p> Name Type Description <code>func</code> <code>Optional[Callable]</code> <p>Desired function, or <code>None</code> if no function called <code>func_name</code> is found in <code>script_path</code>.</p> Source code in <code>isca_tools/jasmin/run/base.py</code> <pre><code>def import_func_from_path(script_path: str, func_name: str=\"main\",\n                          module_name: str=\"my_dynamic_module\") -&gt; Optional[Callable]:\n    \"\"\"\n    Loads in the function called `func_name` from `script_path`.\n\n    Args:\n        script_path: Path to python `.py` file which contains `func_name`.\n        func_name: Function name to import.\n        module_name: Temporary name for module being loaded dynamically. Could be anything.\n\n    Returns:\n        func: Desired function, or `None` if no function called `func_name` is found in `script_path`.\n    \"\"\"\n    spec = importlib.util.spec_from_file_location(module_name, script_path)\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = module\n    spec.loader.exec_module(module)\n    return getattr(module, func_name, None)  # Returns the main() function if it exists\n</code></pre>"},{"location":"code/jasmin/run/base/#isca_tools.jasmin.run.base.run_script","title":"<code>run_script(script_path=None, script_args=None, job_name=None, time='02:00:00', n_tasks=1, cpus_per_task=1, mem=16, partition='standard', qos=None, account='global_ex', conda_env='myenv', exist_output_ok=None, input_file_path=None, slurm=False, dependent_job_id=None)</code>","text":"<p>Function to submit a python script located at <code>script_path</code> to JASMIN using Slurm.</p> <p>Parameters:</p> Name Type Description Default <code>script_path</code> <code>Optional[str]</code> <p>Path of python script to run.</p> <code>None</code> <code>script_args</code> <code>Optional[Union[List, float, int, str]]</code> <p>Arguments to be passed to <code>script_path</code>. Can pass as many as you want, using a list.</p> <code>None</code> <code>job_name</code> <code>Optional[str]</code> <p>Name of job submitted to slurm. If not provided, will set to name of python script without <code>.py</code> extension, and without <code>$HOME/Isca/jobs</code> at the start.</p> <code>None</code> <code>time</code> <code>str</code> <p>Maximum wall time for your job in format <code>hh:mm:ss</code></p> <code>'02:00:00'</code> <code>n_tasks</code> <code>int</code> <p>Number of tasks to run (usually 1 for a single script).</p> <code>1</code> <code>cpus_per_task</code> <code>int</code> <p>How many CPUs to allocate per task.</p> <code>1</code> <code>mem</code> <code>Union[float, int]</code> <p>Memory to allocate for the job in GB.</p> <code>16</code> <code>partition</code> <code>str</code> <p>Specifies the partition for the job. Options are <code>standard</code>, <code>highres</code> and <code>debug</code>.</p> <code>'standard'</code> <code>qos</code> <code>Optional[str]</code> <p>Quality of service examples include <code>debug</code>, <code>short</code> and <code>standard</code>. Each have a different max wall time and priority. If <code>None</code>, will set to the same as <code>partition</code>.</p> <code>None</code> <code>conda_env</code> <code>str</code> <p>Name of the conda environment on JASMIN to use.</p> <code>'myenv'</code> <code>account</code> <code>str</code> <p>Account to use for submitting jobs. Should be able to find as a group workspace in the myservices section of your JASMIN account.</p> <code>'global_ex'</code> <code>exist_output_ok</code> <code>Optional[bool]</code> <p>Whether to run script if console_output for <code>job_name</code> already exists. If None, will save output to directory with a number added e.g. if <code>output</code> exists, will save as <code>output_1</code>.</p> <code>None</code> <code>input_file_path</code> <code>Optional[str]</code> <p>Give option to provide nml file containing all Slurm info within <code>slurm_info</code> section.</p> <code>None</code> <code>slurm</code> <code>bool</code> <p>If <code>True</code>, will submit job to LOTUS cluster using Slurm queue. Otherwise, it will just run the script interactively, with no submission to Slurm.</p> <code>False</code> <code>dependent_job_id</code> <code>Optional[str]</code> <p>Job should only run after job with this dependency has finished. Only makes a difference if <code>slurm=True</code>.</p> <code>None</code> Source code in <code>isca_tools/jasmin/run/base.py</code> <pre><code>def run_script(script_path: Optional[str] = None, script_args: Optional[Union[List, float, int, str]] = None,\n               job_name: Optional[str] = None, time: str = '02:00:00', n_tasks: int = 1,\n               cpus_per_task: int = 1, mem: Union[float, int] = 16, partition: str = 'standard',\n               qos: Optional[str] = None, account: str = 'global_ex', conda_env: str = 'myenv',\n               exist_output_ok: Optional[bool] = None, input_file_path: Optional[str] = None,\n               slurm: bool = False, dependent_job_id: Optional[str] = None) -&gt; Optional[str]:\n    \"\"\"\n    Function to submit a python script located at `script_path` to JASMIN using *Slurm*.\n\n    Args:\n        script_path: Path of python script to run.\n        script_args: Arguments to be passed to `script_path`. Can pass as many as you want, using a list.\n        job_name: Name of job submitted to slurm. If not provided, will set to name of python script without `.py`\n            extension, and without `$HOME/Isca/jobs` at the start.\n        time: Maximum wall time for your job in format `hh:mm:ss`\n        n_tasks: Number of tasks to run (usually 1 for a single script).\n        cpus_per_task: How many CPUs to allocate per task.\n        mem: Memory to allocate for the job in GB.\n        partition: Specifies the partition for the job.\n            [Options](https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-a-job/#partitions-and-qos)\n            are `standard`, `highres` and `debug`.\n        qos: Quality of service\n            [examples](https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024/#partitions-and-qos)\n            include `debug`, `short` and `standard`. Each have a different max wall time and priority.&lt;/br&gt;\n            If `None`, will set to the same as `partition`.\n        conda_env: Name of the conda environment on JASMIN to use.\n        account: Account to use for submitting jobs. Should be able to find as a group workspace in the\n            [myservices](https://accounts.jasmin.ac.uk/services/my_services/) section of your JASMIN account.\n        exist_output_ok: Whether to run script if console_output for `job_name` already exists.&lt;/br&gt;\n            If None, will save output to directory with a number added e.g. if `output` exists, will save\n            as `output_1`.\n        input_file_path: Give option to provide nml file containing all *Slurm* info within `slurm_info` section.\n        slurm: If `True`, will submit job to LOTUS cluster using *Slurm* queue.\n            Otherwise, it will just run the script interactively, with no submission to *Slurm*.\n        dependent_job_id: Job should only run after job with this dependency has finished. Only makes a difference\n            if `slurm=True`.\n\n    \"\"\"\n    if input_file_path is not None:\n        # If provide input nml file, get all slurm info from this file - need no other info\n        if not os.path.exists(input_file_path):\n            raise ValueError(f\"Input file {input_file_path} does not exist\")\n        slurm_info = get_slurm_info_from_file(input_file_path)\n        script_args, job_name, time, n_tasks, cpus_per_task, mem, partition, qos, conda_env, account, exist_output_ok = \\\n            itemgetter('script_args', 'job_name', 'time', 'n_tasks', 'cpus_per_task',\n                       'mem', 'partition', 'qos', 'conda_env', 'account', 'exist_output_ok')(slurm_info)\n        # make script path the full path - combine jobs_dir and script_path\n        script_path = os.path.join(slurm_info['jobs_dir'], slurm_info['script_path'])\n    if slurm:\n        if job_name is None:\n            job_name = script_path.replace(os.path.join(os.environ['HOME'], 'Isca', 'jobs'), '')\n            if job_name[0] == '/':\n                job_name = job_name[1:]     # make sure does not start with '/'\n            job_name = job_name.replace('.py', '')\n        # Make directory where output saved\n        job_output_dir = os.path.join(os.environ['HOME'], 'Isca/jobs/jasmin/console_output')\n        dir_output = os.path.join(job_output_dir, job_name)\n        if exist_output_ok is None:\n            dir_output = get_unique_dir_name(dir_output)\n            job_name = dir_output.replace(job_output_dir+'/', '')     # update job name so matches dir_output\n            exist_output_ok = False\n        os.makedirs(dir_output, exist_ok=exist_output_ok)\n        slurm_script = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'run_slurm.sh')\n\n        if qos is None:\n            qos = partition\n\n        if dependent_job_id is None:\n            dependent_job_id = ''\n\n        # Note that sys.argv[0] is the path to the run_script.py script that was used to call this function.\n        # We now call it again but with input arguments so that it runs the job on slurm.\n        submit_string = f\"bash {slurm_script if dependent_job_id == '' else slurm_script.replace('.sh','_depend.sh')} \"\\\n                        f\"{job_name} {time} {n_tasks} {cpus_per_task} {mem} {partition} {qos} {account} \"\\\n                        f\"{conda_env} {script_path} {dependent_job_id}\"\n        submit_string = add_list_to_str(submit_string.strip(), script_args)   # use strip to get rid of any empty spaces at start or end\n        output = subprocess.check_output(submit_string, shell=True).decode(\"utf-8\").strip()  # get job just submitted info\n        print(f\"{output}{dependent_job_id if dependent_job_id == '' else ' (dependency: job ' + dependent_job_id + ')'}\")\n        return output.split()[-1]  # Save this job id (last word) for the next submission\n    else:\n        # Import main function from script - do this rather than submitting to console, as can then use debugging stuff\n        main_func = import_func_from_path(script_path)\n        if isinstance(script_args, list):\n            main_func(*script_args)         # call function with all arguments\n        elif script_args is None:\n            main_func()                     # if no arguments, call function without providing any arguments\n        else:\n            main_func(script_args)          # if single argument, call with just that argument\n        return None\n</code></pre>"},{"location":"code/land/base/","title":"Base","text":""},{"location":"code/land/base/#isca_tools.land.base.continent_land_location","title":"<code>continent_land_location(lon_array, lat_array, continent='NA')</code>","text":"<p>Returns a boolean array indicating the latitude, longitude coordinates containing land for a particular <code>continent</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lon_array</code> <code>ndarray</code> <p><code>float [n_lat x n_lon]</code>. Array indicating the longitude at each (latitude, longitude) coordinate in the grid used for the experiment.</p> required <code>lat_array</code> <code>ndarray</code> <p><code>float [n_lat x n_lon]</code>. Array indicating the latitude at each (latitude, longitude) coordinate in the grid used for the experiment.</p> required <code>continent</code> <code>str</code> <p>There are 7 options indicating different continents:</p> <ul> <li><code>NA</code>: North America</li> <li><code>SA</code>: South America</li> <li><code>EA</code>: Eurasia</li> <li><code>AF</code>: Africa</li> <li><code>OZ</code>: Australia</li> <li><code>IN</code>: India</li> <li><code>SEA</code>: South East Asia</li> </ul> <code>'NA'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>land_loc</code>: <code>bool [n_lat x n_lon]</code> <code>land_loc[lat, lon]</code> will  be <code>True</code> if the <code>continent</code> contains land at the coordinate (<code>lat</code>, <code>lon</code>).</p> Source code in <code>isca_tools/land/base.py</code> <pre><code>def continent_land_location(lon_array: np.ndarray, lat_array: np.ndarray, continent: str = 'NA') -&gt; np.ndarray:\n    \"\"\"\n    Returns a boolean array indicating the latitude, longitude coordinates containing land for a particular `continent`.\n\n    Args:\n        lon_array: `float [n_lat x n_lon]`.&lt;/br&gt;\n            Array indicating the longitude at each (latitude, longitude) coordinate in the grid used for the experiment.\n        lat_array: `float [n_lat x n_lon]`.&lt;/br&gt;\n            Array indicating the latitude at each (latitude, longitude) coordinate in the grid used for the experiment.\n        continent: There are 7 options indicating different continents:\n\n            * `NA`: North America\n            * `SA`: South America\n            * `EA`: Eurasia\n            * `AF`: Africa\n            * `OZ`: Australia\n            * `IN`: India\n            * `SEA`: South East Asia\n\n    Returns:\n        `land_loc`: `bool [n_lat x n_lon]`&lt;/br&gt;\n            `land_loc[lat, lon]` will  be `True` if the `continent` contains land at the coordinate (`lat`, `lon`).\n    \"\"\"\n    if continent.upper() == 'NA':\n        land_loc = (103. - 43. / 40. * (lon_array - 180) &lt; lat_array) &amp; (\n                (lon_array - 180) * 43. / 50. - 51.8 &lt; lat_array) &amp; (lat_array &lt; 60.)\n    elif continent.upper() == 'SA':\n        land_loc = (737. - 7.2 * (lon_array - 180) &lt; lat_array) &amp; (\n                (lon_array - 180) * 10. / 7. + -212.1 &lt; lat_array) &amp; (lat_array &lt; -22. / 45 * (lon_array - 180) + 65.9)\n    elif continent.upper() == 'EA':\n        eurasia_pos = (17. &lt;= lat_array) &amp; (lat_array &lt; 60.) &amp; (-5. &lt; lon_array) &amp; (\n                43. / 40. * lon_array - 101.25 &lt; lat_array)\n        eurasia_neg = (17. &lt;= lat_array) &amp; (lat_array &lt; 60.) &amp; (355. &lt; lon_array)\n        land_loc = eurasia_pos + eurasia_neg\n    elif continent.upper() == 'AF':\n        africa_pos = (lat_array &lt; 17.) &amp; (-52. / 27. * lon_array + 7.37 &lt; lat_array) &amp; (\n                    52. / 38. * lon_array - 65.1 &lt; lat_array)\n        africa_neg = (lat_array &lt; 17.) &amp; (-52. / 27. * (lon_array - 360) + 7.37 &lt; lat_array)\n        land_loc = africa_pos + africa_neg\n    elif continent.upper() == 'OZ':\n        land_loc = (lat_array &gt; - 35.) &amp; (lat_array &lt; -17.) &amp; (lon_array &gt; 115.) &amp; (lon_array &lt; 150.)\n    elif continent.upper() == 'IN':\n        land_loc = (lat_array &lt; 23.) &amp; (-15. / 8. * lon_array + 152 &lt; lat_array) &amp; (\n                    15. / 13. * lon_array - 81 &lt; lat_array)\n    elif continent.upper() == 'SEA':\n        land_loc = (lat_array &lt; 23.) &amp; (43. / 40. * lon_array - 101.25 &lt; lat_array) &amp; (\n                    -14. / 13. * lon_array + 120 &lt; lat_array)\n    else:\n        raise ValueError(f\"Continent given was {continent} but it must be either 'NA', 'SA', 'EA', 'AF', 'OZ', 'IN' or \"\n                         f\"'SEA'\")\n    return land_loc\n</code></pre>"},{"location":"code/land/base/#isca_tools.land.base.write_land","title":"<code>write_land(file_name, namelist_file, land_mode=None, boundaries=None, continents='all', topography=None, topography_gauss=None, waterworld=False)</code>","text":"<p>This function generates a .nc file containing the variable <code>land_mask</code>, indicating the coordinates where land is, and <code>zsurf</code>, indicating the topography at each coordinate.</p> <p>Extended from an Isca script.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>.nc file containing the land coordinates and corresponding topography will be saved with this name in the folder given by <code>input_dir</code> in the <code>experiment_details</code> namelist of the <code>namelist_file</code>.</p> required <code>namelist_file</code> <code>str</code> <p>File path to namelist <code>nml</code> file for the experiment. This specifies the physical parameters e.g. resolution used for the simulation.</p> required <code>land_mode</code> <code>Optional[str]</code> <p>Type of land to use for the experiment. There are three options:</p> <ul> <li><code>None</code>: No land.</li> <li><code>square</code>: Square block of land with boundaries specified by the <code>boundaries</code> variable.</li> <li><code>continents</code>: Use all or a subset of Earth's continents as set by the <code>continents</code> variable.</li> </ul> <code>None</code> <code>boundaries</code> <code>Optional[List[float]]</code> <p><code>float [4]</code>. The <code>[South, North, West, East]</code> boundaries of the land in degrees. Only required if <code>land_mode = square</code>. \\(-90 \\leq \\phi \\leq 90\\) \\(0 \\leq \\lambda \\leq 360\\)</p> <code>None</code> <code>continents</code> <code>Union[List[str], str]</code> <p>There are 7 possible continents:</p> <ul> <li><code>NA</code>: North America</li> <li><code>SA</code>: South America</li> <li><code>EA</code>: Eurasia</li> <li><code>AF</code>: Africa</li> <li><code>OZ</code>: Australia</li> <li><code>IN</code>: India</li> <li><code>SEA</code>: South East Asia</li> </ul> <p>If <code>continents = all</code>, all of the above will be used. If <code>continents = old</code>, <code>[NA, SA, EA, AF]</code> will be used. Otherwise, you can give a list indicating a subset of continents e.g. <code>[NA, SA]</code>.</p> <code>'all'</code> <code>topography</code> <code>Optional[str]</code> <p>Type of topography to use for the experiment. There are five options:</p> <ul> <li><code>None</code>: No topography.</li> <li><code>gaussian</code>: A single mountain with location and height specified through the <code>topography_gauss</code> variable.</li> <li><code>rockys</code>: Just the Rocky mountain range.</li> <li><code>tibet</code>: Just Tibet.</li> <li><code>all</code>: Includes the Rockys and Tibet.</li> </ul> <code>None</code> <code>topography_gauss</code> <code>Optional[List[float]]</code> <p><code>float [5]</code>. List containing:</p> <ul> <li>Central latitude of mountain (degrees).</li> <li>Central longitude of mountain (degrees).</li> <li>Radius of mountain in degrees. Typical would be 20.</li> <li>Standard deviation indicating how steep the mountain is. The smaller the value, the steeper the mountain. Units are degrees and typical value would be 10.</li> <li>Height of mountain peak in meters.</li> </ul> <code>None</code> <code>waterworld</code> <code>bool</code> <p>If <code>False</code>, topography is not allowed where there is no land. Otherwise, aquamountains are possible.</p> <code>False</code> Source code in <code>isca_tools/land/base.py</code> <pre><code>def write_land(file_name: str, namelist_file: str, land_mode: Optional[str] = None,\n               boundaries: Optional[List[float]] = None, continents: Union[List[str], str] = 'all',\n               topography: Optional[str] = None,\n               topography_gauss: Optional[List[float]] = None, waterworld: bool = False):\n    \"\"\"\n    This function generates a *.nc* file containing the variable `land_mask`, indicating the coordinates where land is,\n    and `zsurf`, indicating the topography at each coordinate.\n\n    Extended from an\n    [Isca script](https://github.com/ExeClim/Isca/blob/master/src/extra/python/isca/land_generator_fn.py).\n\n    Args:\n        file_name: *.nc* file containing the land coordinates and corresponding topography will be saved with this name\n            in the folder given by `input_dir` in the `experiment_details` namelist of the `namelist_file`.\n        namelist_file: File path to namelist `nml` file for the experiment.\n            This specifies the physical parameters e.g. resolution used for the simulation.\n        land_mode: Type of land to use for the experiment. There are three options:\n\n            * `None`: No land.\n            * `square`: Square block of land with boundaries specified by the `boundaries` variable.\n            * `continents`: Use all or a subset of Earth's continents as set by the `continents` variable.\n        boundaries: `float [4]`.\n            The `[South, North, West, East]` boundaries of the land in degrees.\n            Only required if `land_mode = square`.&lt;/br&gt;\n            $-90 \\leq \\phi \\leq 90$&lt;/br&gt;\n            $0 \\leq \\lambda \\leq 360$\n        continents: There are 7 possible continents:\n\n            * `NA`: North America\n            * `SA`: South America\n            * `EA`: Eurasia\n            * `AF`: Africa\n            * `OZ`: Australia\n            * `IN`: India\n            * `SEA`: South East Asia\n\n            If `continents = all`, all of the above will be used.&lt;/br&gt;\n            If `continents = old`, `[NA, SA, EA, AF]` will be used.&lt;/br&gt;\n            Otherwise, you can give a list indicating a subset of continents e.g. `[NA, SA]`.\n        topography: Type of topography to use for the experiment. There are five options:\n\n            * `None`: No topography.\n            * `gaussian`: A single mountain with location and height specified through the `topography_gauss` variable.\n            * `rockys`: Just the Rocky mountain range.\n            * `tibet`: Just Tibet.\n            * `all`: Includes the Rockys and Tibet.\n        topography_gauss: `float [5]`.&lt;/br&gt;\n            List containing:\n\n            * Central latitude of mountain (degrees).\n            * Central longitude of mountain (degrees).\n            * Radius of mountain in degrees. Typical would be 20.\n            * Standard deviation indicating how steep the mountain is. The smaller the value, the steeper the\n            mountain. Units are degrees and typical value would be 10.\n            * Height of mountain peak in meters.\n        waterworld: If `False`, topography is not allowed where there is no land.\n            Otherwise, *aquamountains* are possible.\n\n    \"\"\"\n    # TODO: can probably extend the 'square' option to give a sequence of squares.\n    namelist = load_namelist(namelist_file=namelist_file)\n    # Load in grid file containing longitude/latitude info for the resolution used for this experiment\n    grid_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'run', 'grid_files')\n    nlat_res_dict = {32: 'T21', 64: 'T42', 128: 'T85'}\n\n    if 'column_nml' in namelist:\n        # If use single column, then specify number of lon/lat coordinates so cannot read from resolution file\n        nlon = int(namelist['column_nml']['lon_max'])\n        nlat = int(namelist['column_nml']['lat_max'])\n        lons = np.arange(nlon) * 360 / nlon  # from column_grid.F90 file\n        if nlat in nlat_res_dict:\n            grid_file = os.path.join(grid_dir, f\"{nlat_res_dict[nlat].lower()}_grid.nc\")\n            resolution_file = Dataset(grid_file, 'r', format='NETCDF3_CLASSIC')\n            lats = resolution_file.variables['lat'][:]\n        else:\n            raise ValueError(f\"Don't know latitude grid for nlat={nlat}\")\n            # Complicated computation of lat coordinates for non-regular grid, something like below\n            # from column_grid.F90 file\n            # np.rad2deg(np.arcsin(np.cos(np.pi * (np.arange(1, nlat + 1) - 0.25) / (nlat + 0.5))))\n    else:\n        res = int(namelist['experiment_details']['resolution'][1:])  # resolution for experiment read in\n        grid_file = os.path.join(grid_dir, f\"t{res}_grid.nc\")\n        resolution_file = Dataset(grid_file, 'r', format='NETCDF3_CLASSIC')\n        lons = resolution_file.variables['lon'][:]\n        lats = resolution_file.variables['lat'][:]\n        nlon = lons.shape[0]\n        nlat = lats.shape[0]\n        # lonb = resolution_file.variables['lonb'][:]\n        # latb = resolution_file.variables['latb'][:]\n\n\n    # make 2d arrays of latitude and longitude\n    lon_array, lat_array = np.meshgrid(lons, lats)\n    # lonb_array, latb_array = np.meshgrid(lonb, latb)\n\n    # Configure where land is\n    land_array = np.zeros((nlat, nlon))\n    if land_mode is None:\n        pass\n    elif land_mode.lower() == 'square':\n        if boundaries is None:\n            raise ValueError(f\"boundaries is {None} but should be a list of 4 values indicating the \"\n                             f\"[south, north, west, east] boundaries of the land.\")\n        idx = (boundaries[0] &lt;= lat_array) &amp; (lat_array &lt; boundaries[1]) &amp; (boundaries[2] &lt; lon_array) &amp; (\n                boundaries[3] &gt; lon_array)\n        land_array[idx] = 1\n    elif land_mode.lower() == 'continents':\n        if isinstance(continents, str):\n            if continents.lower() == 'all':\n                # All continents\n                continents = ['NA', 'SA', 'EA', 'AF', 'OZ', 'IN', 'SEA']\n            elif continents.lower() == 'old':\n                # Continents from the original continent set-up adapted from the Sauliere 2012 paper (Jan 16)\n                continents = ['NA', 'SA', 'EA', 'AF']\n            else:\n                raise ValueError(f\"continents given as {continents} but must be 'all', 'old' or a list of continents.\")\n        idx = continent_land_location(lon_array, lat_array, continents[0])\n        for i in range(1, len(continents)):\n            idx += continent_land_location(lon_array, lat_array, continents[i])\n        land_array[idx] = 1\n    else:\n        raise ValueError(f\"land_mode was given as {land_mode} but must be None, 'square' or 'continents'.\")\n\n    # Configure topography\n    topo_array = np.zeros((nlat, nlon))   # height field\n    if topography is None:\n        pass\n    elif topography.lower() == 'gaussian':\n        topo_array += gaussian_mountain(lon_array, lat_array, topography_gauss[0], topography_gauss[1],\n                                        topography_gauss[2], topography_gauss[3], topography_gauss[4])\n    elif topography.lower() == 'all':\n        # Add both rockys and tibet from Sauliere 2012\n        topo_array += mountain_range_height(lon_array, lat_array, 'rockys')\n        topo_array += mountain_range_height(lon_array, lat_array, 'tibet')\n    elif topography.lower() == 'tibet':\n        topo_array += mountain_range_height(lon_array, lat_array, 'tibet')\n    elif topography.lower() == 'rockys':\n        topo_array += mountain_range_height(lon_array, lat_array, 'rockys')\n    else:\n        raise ValueError(f\"topography given as {topography} but must be None, 'all', 'rockys', 'tibet' or 'gaussian'.\")\n    if not waterworld:\n        # Don't allow topography where there is no land\n        topo_array[(land_array == 0) &amp; (topo_array != 0)] = 0\n\n    # Write land and topography arrays to file\n    file_name = file_name.replace('.nc', '')\n    file_name = file_name + '.nc'\n    file_name = os.path.join(namelist['experiment_details']['input_dir'], file_name)\n    if os.path.exists(file_name):\n        raise ValueError(f\"The file {file_name} already exists. Delete or re-name this to continue.\")\n    topo_file = Dataset(file_name, 'w', format='NETCDF3_CLASSIC')\n    topo_file.createDimension('lat', nlat)\n    topo_file.createDimension('lon', nlon)\n    latitudes = topo_file.createVariable('lat','f4',('lat',))\n    longitudes = topo_file.createVariable('lon','f4',('lon',))\n    topo_array_netcdf = topo_file.createVariable('zsurf','f4',('lat','lon',))\n    land_array_netcdf = topo_file.createVariable('land_mask','f4',('lat','lon',))\n    latitudes[:] = lats\n    longitudes[:] = lons\n    topo_array_netcdf[:] = topo_array\n    land_array_netcdf[:] = land_array\n    topo_file.close()\n    print('Output written to: ' + file_name)\n</code></pre>"},{"location":"code/land/mountains/","title":"Mountains","text":""},{"location":"code/land/mountains/#isca_tools.land.mountains.gaussian_mountain","title":"<code>gaussian_mountain(lon_array, lat_array, central_lat, central_lon, radius_degrees, std_dev, height)</code>","text":"<p>Returns the height of the land at each latitude, longitude coordinate for a single gaussian mountain.</p> <p>Parameters:</p> Name Type Description Default <code>lon_array</code> <code>ndarray</code> <p><code>float [n_lat x n_lon]</code>. Array indicating the longitude at each (latitude, longitude) coordinate in the grid used for the experiment.</p> required <code>lat_array</code> <code>ndarray</code> <p><code>float [n_lat x n_lon]</code>. Array indicating the latitude at each (latitude, longitude) coordinate in the grid used for the experiment.</p> required <code>central_lat</code> <code>float</code> <p>Latitude coordinate of mountain in degrees.</p> required <code>central_lon</code> <code>float</code> <p>Longitude coordinate of mountain in degrees.</p> required <code>radius_degrees</code> <code>float</code> <p>Radius of mountain in degrees. Altitude at a distance from the center greater than this will be set to 0, so set to very high number to ignore this functionality. Typical: 20.</p> required <code>std_dev</code> <code>float</code> <p>Standard deviation indicating how steep the mountain is. The smaller the value, the steeper the mountain. Units are degrees and typical value would be 10.</p> required <code>height</code> <code>float</code> <p>Height of mountain peak in meters.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p><code>h_arr</code>: <code>float [n_lat x n_lon]</code> <code>h_arr[lat, lon]</code> is the height of the land at the coordinate (<code>lat</code>, <code>lon</code>) in meters. Most coordinates will be 0.</p> Source code in <code>isca_tools/land/mountains.py</code> <pre><code>def gaussian_mountain(lon_array: np.ndarray, lat_array: np.ndarray, central_lat: float, central_lon: float,\n                      radius_degrees: float, std_dev: float, height: float) -&gt; np.ndarray:\n    \"\"\"\n    Returns the height of the land at each latitude, longitude coordinate for a single gaussian mountain.\n\n    Args:\n        lon_array: `float [n_lat x n_lon]`.&lt;/br&gt;\n            Array indicating the longitude at each (latitude, longitude) coordinate in the grid used for the experiment.\n        lat_array: `float [n_lat x n_lon]`.&lt;/br&gt;\n            Array indicating the latitude at each (latitude, longitude) coordinate in the grid used for the experiment.\n        central_lat: Latitude coordinate of mountain in degrees.\n        central_lon: Longitude coordinate of mountain in degrees.\n        radius_degrees: Radius of mountain in degrees. Altitude at a distance from the center greater than this will\n            be set to 0, so set to very high number to ignore this functionality. Typical: 20.\n        std_dev: Standard deviation indicating how steep the mountain is. The smaller the value, the steeper the\n            mountain. Units are degrees and typical value would be 10.\n        height: Height of mountain peak in meters.\n\n    Returns:\n        `h_arr`: `float [n_lat x n_lon]`&lt;/br&gt;\n            `h_arr[lat, lon]` is the height of the land at the coordinate (`lat`, `lon`) in meters.\n            Most coordinates will be 0.\n\n    \"\"\"\n    rsqd_array = np.sqrt((lon_array - central_lon) ** 2. + (lat_array - central_lat) ** 2.)\n    h_arr = height * np.exp(-(rsqd_array**2.)/(2.*std_dev**2.))\n    h_arr[rsqd_array &gt; radius_degrees] = 0  # Make sure height goes to 0 at some point\n    return h_arr\n</code></pre>"},{"location":"code/land/mountains/#isca_tools.land.mountains.mountain_range_height","title":"<code>mountain_range_height(lon_array, lat_array, mountain='rockys')</code>","text":"<p>Returns the height of the land at each latitude, longitude coordinate for a given mountain range.</p> <p>Parameters:</p> Name Type Description Default <code>lon_array</code> <code>ndarray</code> <p><code>float [n_lat x n_lon]</code>. Array indicating the longitude at each (latitude, longitude) coordinate in the grid used for the experiment.</p> required <code>lat_array</code> <code>ndarray</code> <p><code>float [n_lat x n_lon]</code>. Array indicating the latitude at each (latitude, longitude) coordinate in the grid used for the experiment.</p> required <code>mountain</code> <code>str</code> <p>There are 2 options indicating different mountain ranges:</p> <ul> <li><code>rockys</code></li> <li><code>tibet</code></li> </ul> <code>'rockys'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>h_arr</code>: <code>float [n_lat x n_lon]</code> <code>h_arr[lat, lon]</code> is the height of the land at the coordinate (<code>lat</code>, <code>lon</code>) in meters. Most coordinates will be 0.</p> Source code in <code>isca_tools/land/mountains.py</code> <pre><code>def mountain_range_height(lon_array: np.ndarray, lat_array: np.ndarray, mountain: str = 'rockys') -&gt; np.ndarray:\n    \"\"\"\n    Returns the height of the land at each latitude, longitude coordinate for a given mountain range.\n\n    Args:\n        lon_array: `float [n_lat x n_lon]`.&lt;/br&gt;\n            Array indicating the longitude at each (latitude, longitude) coordinate in the grid used for the experiment.\n        lat_array: `float [n_lat x n_lon]`.&lt;/br&gt;\n            Array indicating the latitude at each (latitude, longitude) coordinate in the grid used for the experiment.\n        mountain: There are 2 options indicating different mountain ranges:\n\n            * `rockys`\n            * `tibet`\n\n    Returns:\n        `h_arr`: `float [n_lat x n_lon]`&lt;/br&gt;\n            `h_arr[lat, lon]` is the height of the land at the coordinate (`lat`, `lon`) in meters.\n            Most coordinates will be 0.\n\n    \"\"\"\n    if mountain.lower() == 'rockys':\n        # Rockys from Sauliere 2012\n        h_0 = 2670\n        central_lon = 247.5\n        central_lat = 40\n        L_1 = 7.5\n        L_2 = 20\n        gamma_1 = 42\n        gamma_2 = 42\n        delta_1 = ((lon_array - central_lon) * np.cos(np.radians(gamma_1)) + (lat_array - central_lat) *\n                   np.sin(np.radians(gamma_1))) / L_1\n        delta_2 = (-(lon_array - central_lon) * np.sin(np.radians(gamma_2)) + (lat_array - central_lat) *\n                   np.cos(np.radians(gamma_2))) / L_2\n        h_arr = h_0 * np.exp(-(delta_1 ** 2. + delta_2 ** 2.))\n    elif mountain.lower() == 'tibet':\n        # Tibet from Sauliere 2012\n        h_0 = 5700.\n        central_lon = 82.5\n        central_lat = 28\n        L_1 = 12.5\n        L_2 = 12.5\n        gamma_1 = -49.5\n        gamma_2 = -18\n        delta_1 = ((lon_array - central_lon) * np.cos(np.radians(gamma_1)) + (lat_array - central_lat) *\n                   np.sin(np.radians(gamma_1))) / L_1\n        delta_2 = (-(lon_array - central_lon) * np.sin(np.radians(gamma_2)) + (lat_array - central_lat) *\n                   np.cos(np.radians(gamma_2))) / L_2\n        h_arr_tibet_no_amp = np.exp(-(delta_1 ** 2)) * (1 / delta_2) * np.exp(-0.5 * (np.log(delta_2)) ** 2)\n        # For some reason my maximum value of h_arr_tibet_no_amp &gt; 1. Renormalise so h_0 sets amplitude.\n        maxval = np.nanmax(h_arr_tibet_no_amp)\n        h_arr = (h_arr_tibet_no_amp / maxval) * h_0\n    else:\n        raise ValueError(f\"mountain was given as {mountain} but it must be 'rockys' or 'tibet'\")\n    # make sure exponentials are cut at some point - use the value from p70 of Brayshaw's thesis.\n    set_to_zero_height_ind = (h_arr / h_0 &lt;= 0.05)\n    h_arr[set_to_zero_height_ind] = 0\n    return h_arr\n</code></pre>"},{"location":"code/papers/byrne_2021/","title":"Byrne 2021","text":""},{"location":"code/papers/byrne_2021/#isca_tools.papers.byrne_2021.get_delta_temp_quant_theory","title":"<code>get_delta_temp_quant_theory(temp_mean_land, sphum_mean_land, temp_quant_land_x, temp_quant_ocean_p, sphum_quant_land_x, sphum_quant_ocean_p, quant_use, px, pressure_surface, const_rh=False)</code>","text":"<p>Computes the theoretical temperature difference between simulations of neighbouring optical depth values for each percentile, \\(\\delta T_L^x\\), according to equation 5 in Byrne 2021:</p> \\[(1 + \\epsilon \\delta r_L^x)\\delta T_L^x = \\gamma^{T_O}\\delta T_O + \\gamma^{r_O}\\delta r_O - \\eta \\delta \\overline{r_L}\\] <p>If data from <code>n_exp</code> optical depth values provided, <code>n_exp-1</code> theoretical temperature differences will be returned for each percentile.</p> <p>Parameters:</p> Name Type Description Default <code>temp_mean_land</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average near surface land temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K.</p> required <code>sphum_mean_land</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average near surface specific humidity of each simulation. Units: kg/kg.</p> required <code>temp_quant_land_x</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_quant_land_x[i, j]</code> is the near surface land temperature of experiment <code>i</code>, averaged over all days exceeding the percentile <code>quant_use[j]</code> of temperature (\\(x\\) means averaged over the temperature percentile \\(x\\)). Units: K.</p> required <code>temp_quant_ocean_p</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_quant_ocean_p[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface ocean temperature of experiment <code>i</code> (\\(p\\) means at percentile \\(p\\) of given quantity). Units: K.</p> required <code>sphum_quant_land_x</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant_land_x[i, j]</code> is the near surface land specific humidity of experiment <code>i</code>, averaged over all days exceeding the percentile <code>quant_use[j]</code> of temperature. Units: kg/kg.</p> required <code>sphum_quant_ocean_p</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant_ocean_p[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface ocean specific humidity of experiment <code>i</code>. Units: kg/kg.</p> required <code>quant_use</code> <code>ndarray</code> <p><code>int [n_quant]</code>. This contains the percentiles, that the above variables correspond to. It must contain all values of <code>p_x</code> in it.</p> required <code>px</code> <code>ndarray</code> <p><code>int [n_exp, n_quant]</code> <code>p_x[i, j]</code> is the percentile of MSE corresponding to the MSE averaged over all days exceeding the percentile quant_use[i] of temperature in experiment <code>i</code>. Note that <code>px</code> for the warmest simulation is not used but makes sense to have same shape as other variables.</p> required <code>pressure_surface</code> <code>float</code> <p>Near surface pressure level. Units: Pa.</p> required <code>const_rh</code> <code>bool</code> <p>If <code>True</code>, will return the constant relative humidity version of the theory, i.e. \\(\\gamma^{T_O} \\delta T_O\\). Otherwise, will return the full theory.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_exp-1, n_quant]</code>. <code>delta_temp_quant_theory[i, j]</code> refers to the theoretical temperature difference between experiment <code>i</code> and <code>i+1</code> for percentile <code>quant_use[j]</code>.</p> Source code in <code>isca_tools/papers/byrne_2021.py</code> <pre><code>def get_delta_temp_quant_theory(temp_mean_land: np.ndarray, sphum_mean_land: np.ndarray, temp_quant_land_x: np.ndarray,\n                                temp_quant_ocean_p: np.ndarray, sphum_quant_land_x: np.ndarray,\n                                sphum_quant_ocean_p: np.ndarray, quant_use: np.ndarray, px: np.ndarray,\n                                pressure_surface: float, const_rh: bool = False) -&gt; np.ndarray:\n    \"\"\"\n    Computes the theoretical temperature difference between simulations of neighbouring optical depth values for each\n    percentile, $\\delta T_L^x$, according to equation 5 in *Byrne 2021*:\n\n    $$(1 + \\epsilon \\delta r_L^x)\\delta T_L^x = \\gamma^{T_O}\\delta T_O + \\gamma^{r_O}\\delta r_O -\n    \\eta \\delta \\overline{r_L}$$\n\n    If data from `n_exp` optical depth values provided, `n_exp-1` theoretical temperature differences will be returned\n    for each percentile.\n\n    Args:\n        temp_mean_land: `float [n_exp]`&lt;/br&gt;\n            Average near surface land temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*.\n        sphum_mean_land: `float [n_exp]`&lt;/br&gt;\n            Average near surface specific humidity of each simulation. Units: *kg/kg*.\n        temp_quant_land_x: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_quant_land_x[i, j]` is the near surface land temperature of experiment `i`, averaged over all days\n            exceeding the percentile `quant_use[j]` of temperature\n            ($x$ means averaged over the temperature percentile $x$). Units: *K*.\n        temp_quant_ocean_p: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_quant_ocean_p[i, j]` is the percentile `quant_use[j]` of near surface ocean temperature of\n            experiment `i` ($p$ means at percentile $p$ of given quantity). Units: *K*.\n        sphum_quant_land_x: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant_land_x[i, j]` is the near surface land specific humidity of experiment `i`, averaged over\n            all days exceeding the percentile `quant_use[j]` of temperature. Units: *kg/kg*.\n        sphum_quant_ocean_p: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant_ocean_p[i, j]` is the percentile `quant_use[j]` of near surface ocean specific humidity of\n            experiment `i`. Units: *kg/kg*.\n        quant_use: `int [n_quant]`. This contains the percentiles, that the above variables correspond to. It must\n            contain all values of `p_x` in it.\n        px: `int [n_exp, n_quant]`&lt;/br&gt;\n            `p_x[i, j]` is the percentile of MSE corresponding to the MSE averaged over all days exceeding\n            the percentile quant_use[i] of temperature in experiment `i`.\n            Note that `px` for the warmest simulation is not used but makes sense to have same shape as other variables.\n        pressure_surface: Near surface pressure level. Units: *Pa*.\n        const_rh: If `True`, will return the constant relative humidity version of the theory, i.e.\n            $\\gamma^{T_O} \\delta T_O$. Otherwise, will return the full theory.\n\n    Returns:\n        `float [n_exp-1, n_quant]`.&lt;/br&gt;\n            `delta_temp_quant_theory[i, j]` refers to the theoretical temperature difference between experiment `i` and\n            `i+1` for percentile `quant_use[j]`.\n    \"\"\"\n    n_exp = temp_mean_land.shape[0]\n    sphum_quant_sat_l = sphum_sat(temp_quant_land_x, pressure_surface)\n    sphum_mean_sat_l = np.expand_dims(sphum_sat(temp_mean_land, pressure_surface), axis=-1)\n    r_quant_l = sphum_quant_land_x / sphum_quant_sat_l\n    r_mean_l = np.expand_dims(sphum_mean_land, axis=-1) / sphum_mean_sat_l\n    delta_r_mean_l = np.diff(r_mean_l, axis=0)\n    delta_r_quant_l = np.diff(r_quant_l, axis=0)\n\n    # Ocean constants required - these are for the percentile px which corresponds\n    # to the average above the x percentile in temperature\n    p_x_ind = np.asarray([numpy_indexed.indices(quant_use, px[i]) for i in range(n_exp)])\n\n    # For change in ocean variables, use quantile p_x from colder simulation for each set of subsequent simulations\n    # Idea is to only use information from colder simulation to predict warmer one\n    delta_temp_o = np.zeros_like(delta_r_quant_l)\n    delta_r_quant_o = np.zeros_like(delta_r_quant_l)\n    for i in range(n_exp-1):\n        delta_temp_o[i] = temp_quant_ocean_p[i+1, p_x_ind[i]] - temp_quant_ocean_p[i, p_x_ind[i]]\n        delta_r_quant_o[i] = \\\n            sphum_quant_ocean_p[i+1, p_x_ind[i]]/sphum_sat(temp_quant_ocean_p[i+1, p_x_ind[i]], pressure_surface) - \\\n            sphum_quant_ocean_p[i, p_x_ind[i]]/sphum_sat(temp_quant_ocean_p[i, p_x_ind[i]], pressure_surface)\n\n    gamma_t, gamma_r_o, e_param, eta_param = get_gamma(temp_mean_land, temp_quant_land_x, temp_quant_ocean_p,\n                                                   sphum_quant_land_x, sphum_quant_ocean_p, quant_use, px,\n                                                   pressure_surface)\n\n    if const_rh:\n        delta_temp_quant_theory = gamma_t[:-1] * delta_temp_o\n    else:\n        delta_temp_quant_theory = (gamma_t[:-1] * delta_temp_o + gamma_r_o[:-1] * delta_r_quant_o -\n                                   eta_param[:-1] * delta_r_mean_l) / (1 + e_param[:-1] * delta_r_quant_l)\n    return delta_temp_quant_theory\n</code></pre>"},{"location":"code/papers/byrne_2021/#isca_tools.papers.byrne_2021.get_gamma","title":"<code>get_gamma(temp_mean_land, temp_quant_land_x, temp_quant_ocean_p, sphum_quant_land_x, sphum_quant_ocean_p, quant_use, px, pressure_surface)</code>","text":"<p>This function returns the sensitivity parameters in the theory. One for changes in ocean temperature, \\(\\delta T_O\\), and one for ocean relative humidity, \\(\\delta r_O\\):</p> \\[\\gamma^{T_O} = \\frac{c_p + L_v \\alpha_O q_O}{c_p + L_v \\alpha_L q_L^x};\\quad \\gamma^{r_O} = \\frac{L_v q_{O, sat}}{c_p + L_v \\alpha_L q^x_L}\\] <p>Two more parameters are also returned:</p> \\[\\epsilon = \\frac{L_v \\alpha_L q^x_{L,sat}}{c_p + L_v \\alpha_L q^x_L};\\quad \\eta = \\frac{\\epsilon}{\\alpha_L}\\frac{\\overline{q_{L,sat}}}{q^x_{L,sat}}\\] <p>These can then be combined to give the sensitivity parameter to a 1% change in mean land relative humidity, \\(\\delta \\overline{r_L}\\):</p> \\[\\gamma^{r_L} = -\\frac{\\eta}{100 + \\epsilon}\\] <p>Parameters:</p> Name Type Description Default <code>temp_mean_land</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average near surface land temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K.</p> required <code>temp_quant_land_x</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_quant_land_x[i, j]</code> is the near surface land temperature of experiment <code>i</code>, averaged over all days exceeding the percentile <code>quant_use[j]</code> of temperature (\\(x\\) means averaged over the temperature percentile \\(x\\)). Units: K.</p> required <code>temp_quant_ocean_p</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_quant_ocean_p[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface ocean temperature of experiment <code>i</code> (\\(p\\) means at percentile \\(p\\) of given quantity). Units: K.</p> required <code>sphum_quant_land_x</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant_land_x[i, j]</code> is the near surface land specific humidity of experiment <code>i</code>, averaged over all days exceeding the percentile <code>quant_use[j]</code> of temperature. Units: kg/kg.</p> required <code>sphum_quant_ocean_p</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant_ocean_p[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface ocean specific humidity of experiment <code>i</code>. Units: kg/kg.</p> required <code>quant_use</code> <code>ndarray</code> <p><code>int [n_quant]</code>. This contains the percentiles, that the above variables correspond to. It must contain all values of <code>p_x</code> in it.</p> required <code>px</code> <code>ndarray</code> <p><code>int [n_exp, n_quant]</code> <code>p_x[i, j]</code> is the percentile of MSE corresponding to the MSE averaged over all days exceeding the percentile quant_use[i] of temperature in experiment <code>i</code>. Note that <code>px</code> for the warmest simulation is not used but makes sense to have same shape as other variables.</p> required <code>pressure_surface</code> <code>float</code> <p>Near surface pressure level. Units: Pa.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p><code>gamma_t</code>: <code>float [n_exp, n_quant]</code> The sensitivity to change in ocean temperature for each experiment and quantile.</p> <code>ndarray</code> <p><code>gamma_r_o</code>: <code>float [n_exp, n_quant]</code> The sensitivity to change in ocean relative humidity difference from the mean for each experiment and quantile.</p> <code>ndarray</code> <p><code>e_param</code>: <code>float [n_exp, n_quant]</code> The \\(\\epsilon = \\frac{L_v \\alpha_L q^x_{L,sat}}{c_p + L_v \\alpha_L q^x_L}\\) parameter.</p> <code>ndarray</code> <p><code>eta_param</code>: <code>float [n_exp, n_quant]</code> The \\(\\eta = \\frac{\\epsilon}{\\alpha_L}\\frac{\\overline{q_{L,sat}}}{q^x_{L,sat}}\\) parameter.</p> Source code in <code>isca_tools/papers/byrne_2021.py</code> <pre><code>def get_gamma(temp_mean_land: np.ndarray, temp_quant_land_x: np.ndarray, temp_quant_ocean_p: np.ndarray,\n              sphum_quant_land_x: np.ndarray, sphum_quant_ocean_p: np.ndarray, quant_use: np.ndarray, px: np.ndarray,\n              pressure_surface: float) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    This function returns the sensitivity parameters in the theory.\n    One for changes in ocean temperature, $\\\\delta T_O$, and one for ocean relative humidity,\n    $\\delta r_O$:\n\n    $$\\gamma^{T_O} = \\\\frac{c_p + L_v \\\\alpha_O q_O}{c_p + L_v \\\\alpha_L q_L^x};\\quad\n    \\gamma^{r_O} = \\\\frac{L_v q_{O, sat}}{c_p + L_v \\\\alpha_L q^x_L}$$\n\n    Two more parameters are also returned:\n\n    $$\\\\epsilon = \\\\frac{L_v \\\\alpha_L q^x_{L,sat}}{c_p + L_v \\\\alpha_L q^x_L};\\quad\n    \\\\eta = \\\\frac{\\\\epsilon}{\\\\alpha_L}\\\\frac{\\overline{q_{L,sat}}}{q^x_{L,sat}}$$\n\n    These can then be combined to give the sensitivity parameter to a 1% change in mean land relative humidity,\n    $\\delta \\overline{r_L}$:\n\n    $$\\gamma^{r_L} = -\\\\frac{\\eta}{100 + \\\\epsilon}$$\n\n    Args:\n        temp_mean_land: `float [n_exp]`&lt;/br&gt;\n            Average near surface land temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*.\n        temp_quant_land_x: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_quant_land_x[i, j]` is the near surface land temperature of experiment `i`, averaged over all days\n            exceeding the percentile `quant_use[j]` of temperature\n            ($x$ means averaged over the temperature percentile $x$). Units: *K*.\n        temp_quant_ocean_p: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_quant_ocean_p[i, j]` is the percentile `quant_use[j]` of near surface ocean temperature of\n            experiment `i` ($p$ means at percentile $p$ of given quantity). Units: *K*.\n        sphum_quant_land_x: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant_land_x[i, j]` is the near surface land specific humidity of experiment `i`, averaged over\n            all days exceeding the percentile `quant_use[j]` of temperature. Units: *kg/kg*.\n        sphum_quant_ocean_p: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant_ocean_p[i, j]` is the percentile `quant_use[j]` of near surface ocean specific humidity of\n            experiment `i`. Units: *kg/kg*.\n        quant_use: `int [n_quant]`. This contains the percentiles, that the above variables correspond to. It must\n            contain all values of `p_x` in it.\n        px: `int [n_exp, n_quant]`&lt;/br&gt;\n            `p_x[i, j]` is the percentile of MSE corresponding to the MSE averaged over all days exceeding\n            the percentile quant_use[i] of temperature in experiment `i`.\n            Note that `px` for the warmest simulation is not used but makes sense to have same shape as other variables.\n        pressure_surface: Near surface pressure level. Units: *Pa*.\n\n    Returns:\n        `gamma_t`: `float [n_exp, n_quant]`&lt;/br&gt;\n            The sensitivity to change in ocean temperature for each experiment and quantile.\n        `gamma_r_o`: `float [n_exp, n_quant]`&lt;/br&gt;\n            The sensitivity to change in ocean relative humidity difference from the mean for each experiment and\n            quantile.\n        `e_param`: `float [n_exp, n_quant]`&lt;/br&gt;\n            The $\\\\epsilon = \\\\frac{L_v \\\\alpha_L q^x_{L,sat}}{c_p + L_v \\\\alpha_L q^x_L}$ parameter.\n        `eta_param`: `float [n_exp, n_quant]`&lt;/br&gt;\n            The $\\\\eta = \\\\frac{\\\\epsilon}{\\\\alpha_L}\\\\frac{\\overline{q_{L,sat}}}{q^x_{L,sat}}$ parameter.\n\n    \"\"\"\n    n_exp = temp_mean_land.shape[0]\n    alpha_l = clausius_clapeyron_factor(temp_quant_land_x, pressure_surface)\n    sphum_quant_sat_l = sphum_sat(temp_quant_land_x, pressure_surface)\n    sphum_mean_sat_l = np.expand_dims(sphum_sat(temp_mean_land, pressure_surface), axis=-1)\n\n    # Ocean constants required - these are for the percentile px which corresponds\n    # to the average above the x percentile in temperature\n    p_x_ind = np.asarray([numpy_indexed.indices(quant_use, px[i]) for i in range(n_exp)])\n    temp_quant_o = np.asarray([temp_quant_ocean_p[i, p_x_ind[i]] for i in range(n_exp)])\n    sphum_quant_o = np.asarray([sphum_quant_ocean_p[i, p_x_ind[i]] for i in range(n_exp)])\n    sphum_quant_sat_o = sphum_sat(temp_quant_o, pressure_surface)\n\n    alpha_o = clausius_clapeyron_factor(temp_quant_o, pressure_surface)\n    e_param = L_v * alpha_l * sphum_quant_sat_l / (c_p + L_v * alpha_l * sphum_quant_land_x)\n    eta_param = sphum_mean_sat_l / sphum_quant_sat_l * e_param / alpha_l\n    gamma_t = (c_p + L_v * alpha_o * sphum_quant_o) / (c_p + L_v * alpha_l * sphum_quant_land_x)\n    gamma_r_o = L_v * sphum_quant_sat_o / (c_p + L_v * alpha_l * sphum_quant_land_x)\n    return gamma_t, gamma_r_o, e_param, eta_param\n</code></pre>"},{"location":"code/papers/byrne_2021/#isca_tools.papers.byrne_2021.get_px","title":"<code>get_px(ds, mse_quant_x, quant_use, as_int=False)</code>","text":"<p>Returns the percentile of Moist Static Energy corresponding to the MSE averaged over all days exceeding a given percentile of temperature.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>List[Dataset]</code> <p><code>[n_exp]</code>. ds[i] is the dataset for experiment <code>i</code> containing only variables at the lowest pressure level. Also, the <code>lon</code>, <code>lat</code>, <code>time</code> coordinates must be collapsed into a single coordinate. It must include <code>temp</code>, <code>sphum</code> and <code>height</code>.</p> required <code>mse_quant_x</code> <code>ndarray</code> <p><code>[n_exp, n_quant]</code>. <code>mse_quant_x[i, j]</code> is the near MSE of experiment <code>i</code>, averaged over all days exceeding the percentile <code>quant_use[j]</code> of temperature. Units: kJ/kg.</p> required <code>quant_use</code> <code>ndarray</code> <p><code>int [n_quant]</code>. This contains the percentiles, that <code>mse_quant_x</code> corresponds to.</p> required <code>as_int</code> <code>bool</code> <p>If <code>True</code>, will round the percentile to the nearest integer.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> or <code>int [n_exp, n_quant]</code>. <code>p_x[i, j]</code> is the percentile of MSE corresponding to the MSE averaged over all days exceeding the percentile quant_use[i] of temperature in experiment <code>i</code>.</p> Source code in <code>isca_tools/papers/byrne_2021.py</code> <pre><code>def get_px(ds: List[xr.Dataset], mse_quant_x: np.ndarray, quant_use: np.ndarray, as_int: bool = False) -&gt; np.ndarray:\n    \"\"\"\n    Returns the percentile of Moist Static Energy corresponding to the MSE averaged over all days exceeding\n    a given percentile of temperature.\n\n    Args:\n        ds: `[n_exp]`.&lt;/br&gt;\n            ds[i] is the dataset for experiment `i` containing only variables at the lowest pressure level.\n            Also, the `lon`, `lat`, `time` coordinates must be collapsed into a single coordinate.\n            It must include `temp`, `sphum` and `height`.\n        mse_quant_x: `[n_exp, n_quant]`.&lt;/br&gt;\n            `mse_quant_x[i, j]` is the near MSE of experiment `i`, averaged over all days\n            exceeding the percentile `quant_use[j]` of temperature. Units: *kJ/kg*.\n        quant_use: `int [n_quant]`. This contains the percentiles, that `mse_quant_x` corresponds to.\n        as_int: If `True`, will round the percentile to the nearest integer.\n\n    Returns:\n        `float [n_exp, n_quant]` or `int [n_exp, n_quant]`.&lt;/br&gt;\n            `p_x[i, j]` is the percentile of MSE corresponding to the MSE averaged over all days exceeding\n            the percentile quant_use[i] of temperature in experiment `i`.\n    \"\"\"\n    n_exp = len(ds)\n    px = np.zeros((n_exp, len(quant_use)))\n    for i in range(n_exp):\n        if 'pfull' in ds[i].dims:\n            # get rid of pressure coordinate if exists\n            ds_use = ds[i].sel(pfull=np.inf, method='nearest', drop=True)\n            mse_all = moist_static_energy(ds_use.temp, ds_use.sphum, ds_use.height)\n        elif len(ds[i].temp.shape) != 1:\n            raise ValueError(f'Dataset has coordinates:\\n{ds[i].coords}\\nbut should only have 1')\n        else:\n            mse_all = moist_static_energy(ds[i].temp, ds[i].sphum, ds[i].height)\n        for j, quant in enumerate(quant_use):\n            px[i, j] = percentileofscore(mse_all, mse_quant_x[i, j])\n    if as_int:\n        px = np.round(px).astype(int)\n    return px\n</code></pre>"},{"location":"code/papers/byrne_2021/#isca_tools.papers.byrne_2021.get_quant_ind","title":"<code>get_quant_ind(var, percentile, range_below=0, range_above=np.inf, av_dim=None, return_mask=False)</code>","text":"<p>This functions returns the indices of all occurrences whereby the value of <code>var</code> is between the <code>percentile-range_below</code> and <code>percentile+range_above</code> percentile.</p> <p>By default, <code>range_below=0</code> and <code>range_above=inf</code> so it just returns all indices above the given <code>percentile</code>.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Union[DataArray, ndarray]</code> <p><code>float [n_lon_lat_time]</code> Variable to find quantiles for, usually surface temperature. The first coordinate of this variable must be called <code>lon_lat_time</code>, which is all longitudes, latitudes and times collapsed into a single coordinate.</p> required <code>percentile</code> <code>int</code> <p>The percentile around which, you want to find the indices.</p> required <code>range_below</code> <code>float</code> <p>All indices will <code>var</code> in the percentile range between <code>percentile-range_below</code> and <code>percentile+range_above</code> will be returned.</p> <code>0</code> <code>range_above</code> <code>float</code> <p>All indices will <code>var</code> in the percentile range between <code>percentile-range_below</code> and <code>percentile+range_above</code> will be returned.</p> <code>inf</code> <code>av_dim</code> <code>Optional[Union[List, str, int]]</code> <p>Dimension to find quantile over, should be string if <code>var</code> is xarray or integer if <code>var</code> is numpy array. If not given, will find quantile over 'lon_lat_time' or 'lon_time' dimension.</p> <code>None</code> <code>return_mask</code> <code>bool</code> <p>If <code>True</code>, will return boolean mask indicating which coordinates are in the quant range. Otherwise will return the indices</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>int [n_ind]</code> All indices where <code>var</code> is in the correct percentile range. Or boolean of length <code>n_lon_lat_time</code> if <code>return_mask</code> is <code>True</code>.</p> Source code in <code>isca_tools/papers/byrne_2021.py</code> <pre><code>def get_quant_ind(var: Union[xr.DataArray, np.ndarray], percentile: int, range_below: float = 0,\n                  range_above: float = np.inf, av_dim: Optional[Union[List, str, int]]=None,\n                  return_mask: bool = False) -&gt; np.ndarray:\n    \"\"\"\n    This functions returns the indices of all occurrences whereby the value of `var` is between the\n    `percentile-range_below` and `percentile+range_above` percentile.\n\n    By default, `range_below=0` and `range_above=inf` so it just returns all indices above the given `percentile`.\n\n    Args:\n        var: `float [n_lon_lat_time]`&lt;/br&gt;\n            Variable to find quantiles for, usually surface temperature. The first coordinate of this variable\n            must be called `lon_lat_time`, which is all longitudes, latitudes and times collapsed into a single\n            coordinate.\n        percentile: The percentile around which, you want to find the indices.\n        range_below: All indices will `var` in the percentile range between `percentile-range_below` and\n            `percentile+range_above` will be returned.\n        range_above: All indices will `var` in the percentile range between `percentile-range_below` and\n            `percentile+range_above` will be returned.\n        av_dim: Dimension to find quantile over, should be string if `var` is xarray or integer if `var` is numpy\n            array. If not given, will find quantile over 'lon_lat_time' or 'lon_time' dimension.\n        return_mask: If `True`, will return boolean mask indicating which coordinates are in the quant range.&lt;/br&gt;\n            Otherwise will return the indices\n\n    Returns:\n        `int [n_ind]`&lt;/br&gt;\n            All indices where `var` is in the correct percentile range.&lt;/br&gt;\n            Or boolean of length `n_lon_lat_time` if `return_mask` is `True`.&lt;/br&gt;\n    \"\"\"\n    quant_min = np.clip(percentile-range_below, 0, 100)\n    quant_max = np.clip(percentile+range_above, 0, 100)\n    if isinstance(var, np.ndarray):\n        quantile_thresh_min = np.quantile(var, quant_min/100, axis=av_dim)\n        quantile_thresh_max = np.quantile(var, quant_max/100, axis=av_dim)\n    else:\n        if av_dim is None:\n            if 'lon_lat_time' in var.dims:\n                av_dim = 'lon_lat_time'\n            elif 'lon_time' in var.dims:\n                av_dim = 'lon_time'\n            else:\n                raise ValueError('No suitable dimension to average over - neither lon_lat_time nor lon_time in var')\n        # else:\n        #     if av_dim not in var.dims:\n        #         raise ValueError(f'No suitable dimension to average over - {av_dim} is not in var')\n        quantile_thresh_min = var.quantile(quant_min / 100, dim=av_dim, keep_attrs=True)\n        quantile_thresh_max = var.quantile(quant_max / 100, dim=av_dim, keep_attrs=True)\n    mask = np.logical_and(var &gt; quantile_thresh_min, var &lt;= quantile_thresh_max)\n    if return_mask:\n        return mask\n    else:\n        return np.where(mask)[0]\n</code></pre>"},{"location":"code/papers/miyawaki_2022/","title":"Miyawaki 2022","text":""},{"location":"code/papers/miyawaki_2022/#isca_tools.papers.miyawaki_2022.get_dmse_dt","title":"<code>get_dmse_dt(temp, sphum, height, p_levels, time, zonal_mean=True, spline_smoothing_factor=0)</code>","text":"<p>For a given latitude, this computes the time derivative of the mass weighted vertical integral of the zonal mean moist static energy, \\(&lt;[\\partial_t m]&gt;\\), used in the paper to compute the parameter \\(R_1\\).</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>DataArray</code> <p><code>float [n_time, n_p_levels, n_lon]</code>. Temperature at each coordinate considered. Units: Kelvin.</p> required <code>sphum</code> <code>DataArray</code> <p><code>float [n_time, n_p_levels, n_lon]</code>. Specific humidity at each coordinate considered. Units: kg/kg.</p> required <code>height</code> <code>DataArray</code> <p><code>float [n_time, n_p_levels, n_lon]</code>. Geopotential height of each level considered.</p> required <code>p_levels</code> <code>DataArray</code> <p><code>float [n_p_levels]</code>. Pressure levels of atmosphere, <code>p_levels[0]</code> is top of atmosphere and <code>p_levels[-1]</code> is the surface. Units: Pa.</p> required <code>time</code> <code>DataArray</code> <p><code>float [n_time]</code>. Time at which data recorded. Units: second.</p> required <code>zonal_mean</code> <code>bool</code> <p><code>bool</code>. Whether to take zonal average or not. If <code>False</code>, returned arrays will have size <code>[n_time x n_lon]</code></p> <code>True</code> <code>spline_smoothing_factor</code> <code>float</code> <p><code>float</code>. If positive, a spline will be fit to smooth <code>mse_integ</code> and compute <code>dmse_dt</code>. Typical: 0.001. If 0, the deriviative will be computed using <code>np.gradient</code>, but in general I recommend using the spline.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataArray</code> <p><code>mse_integ</code>: <code>float [n_time]</code> or <code>float [n_time x n_lon]</code> The mass weighted vertical integral of the (zonal mean) moist static energy at each time i.e. \\(&lt;[m]&gt;\\). Units: \\(J/m^2\\).</p> <code>DataArray</code> <p><code>dmse_dt</code>: <code>float [n_time]</code> or <code>float [n_time x n_lon]</code> The time derivative of the mass weighted vertical integral of the (zonal mean) moist static energy at each time i.e. \\(&lt;[\\partial_t m]&gt;\\). Units: \\(W/m^2\\).</p> Source code in <code>isca_tools/papers/miyawaki_2022.py</code> <pre><code>def get_dmse_dt(temp: xr.DataArray, sphum: xr.DataArray, height: xr.DataArray, p_levels: xr.DataArray,\n                time: xr.DataArray, zonal_mean: bool = True,\n                spline_smoothing_factor: float = 0) -&gt; Tuple[xr.DataArray, xr.DataArray]:\n    \"\"\"\n    For a given latitude, this computes the time derivative of the mass weighted vertical integral of the zonal mean\n    moist static energy, $&lt;[\\\\partial_t m]&gt;$, used in the paper to compute the parameter $R_1$.\n\n    Args:\n        temp: `float [n_time, n_p_levels, n_lon]`.&lt;/br&gt;\n            Temperature at each coordinate considered. Units: *Kelvin*.\n        sphum: `float [n_time, n_p_levels, n_lon]`.&lt;/br&gt;\n            Specific humidity at each coordinate considered. Units: *kg/kg*.\n        height: `float [n_time, n_p_levels, n_lon]`.&lt;/br&gt;\n            Geopotential height of each level considered.\n        p_levels: `float [n_p_levels]`.&lt;/br&gt;\n            Pressure levels of atmosphere, `p_levels[0]` is top of atmosphere and `p_levels[-1]` is the surface.\n            Units: *Pa*.\n        time: `float [n_time]`.&lt;/br&gt;\n            Time at which data recorded. Units: *second*.\n        zonal_mean: `bool`.&lt;/br&gt;\n            Whether to take zonal average or not. If `False`, returned arrays will have size `[n_time x n_lon]`\n        spline_smoothing_factor: `float`.&lt;/br&gt;\n            If positive, a spline will be fit to smooth `mse_integ` and compute `dmse_dt`. *Typical: 0.001*.&lt;/br&gt;\n            If 0, the deriviative will be computed using `np.gradient`, but in general I recommend using the spline.\n\n    Returns:\n        `mse_integ`: `float [n_time]` or `float [n_time x n_lon]`&lt;/br&gt;\n            The mass weighted vertical integral of the (zonal mean) moist static energy at each time i.e. $&lt;[m]&gt;$.\n            Units: $J/m^2$.\n        `dmse_dt`: `float [n_time]` or `float [n_time x n_lon]`&lt;/br&gt;\n            The time derivative of the mass weighted vertical integral of the (zonal mean) moist static energy at\n            each time i.e. $&lt;[\\\\partial_t m]&gt;$. Units: $W/m^2$.\n    \"\"\"\n    # compute zonal mean mse in units of J/kg\n    mse = moist_static_energy(temp, sphum, height) * 1000\n    if 'lon' in list(temp.coords.keys()) and zonal_mean:\n        # Take zonal mean if longitude is a coordinate\n        mse = mse.mean(dim='lon')\n    mse_integ = integrate.simpson(mse / g, p_levels, axis=1)    # do mass weighted integral over atmosphere\n    if spline_smoothing_factor &gt; 0:\n        if not zonal_mean:\n            raise ValueError('Can only use spline if taking the zonal mean')\n        # Divide by mean to fit spline as otherwise numbers too large to fit properly\n        spl = UnivariateSpline(time, mse_integ / np.mean(mse_integ), s=spline_smoothing_factor)\n        dmse_dt = spl.derivative()(time) * np.mean(mse_integ)      # compute derivative direct from spline fit\n        mse_integ = spl(time) * np.mean(mse_integ)    # Set mse_integ to spline version so this is what is returned\n    elif spline_smoothing_factor == 0:\n        # compute derivative directly from the data\n        dmse_dt = np.gradient(mse_integ, time, axis=0)\n    else:\n        raise ValueError(f'spline_smoothing_factor = {spline_smoothing_factor}, which is negative. It must be &gt;=0.')\n    return mse_integ, dmse_dt\n</code></pre>"},{"location":"code/papers/miyawaki_2022/#isca_tools.papers.miyawaki_2022.get_dvmse_dy","title":"<code>get_dvmse_dy(R_a, lh, sh, dmse_dt)</code>","text":"<p>This infers the divergence of moist static energy flux, \\(&lt;[\\partial_y vm]&gt;\\), from equation \\((2)\\) in the paper:</p> <p>\\(&lt;[\\partial_t m]&gt; + &lt;[\\partial_y vm]&gt; = [R_a] + [LH] + [SH]\\)</p> <p>It is not computed directly for reasons outlined in section 2b of the paper.</p> <p>Parameters:</p> Name Type Description Default <code>R_a</code> <code>DataArray</code> <p><code>float [n_time]</code>. Atmospheric radiative heating rate i.e. difference between top of atmosphere and surface radiative fluxes. Negative means atmosphere is cooling. Units: \\(W/m^2\\). This is what is returned by <code>frierson_atmospheric_heating</code> if using the grey radiation with the Frierson scheme.</p> required <code>lh</code> <code>DataArray</code> <p><code>float [n_time]</code>. Surface latent heat flux (up is positive). This is saved by Isca if the variable <code>flux_lhe</code> in the <code>mixed_layer</code> module is specified in the diagnostic table. Units: \\(W/m^2\\).</p> required <code>sh</code> <code>DataArray</code> <p><code>float [n_time]</code>. Surface sensible heat flux (up is positive). This is saved by Isca if the variable <code>flux_t</code> in the <code>mixed_layer</code> module is specified in the diagnostic table. Units: \\(W/m^2\\).</p> required <code>dmse_dt</code> <code>DataArray</code> <p><code>float [n_time]</code>. The time derivative of the mass weighted vertical integral of the zonal mean moist static energy at each time, \\(&lt;[\\partial_t m]&gt;\\). Units: \\(W/m^2\\).</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p><code>float [n_time]</code>. \\(&lt;[\\partial_y vm]&gt;\\) - Mass weighted vertical integral of the zonal mean meridional divergence of moist static energy flux, \\(vm\\).</p> Source code in <code>isca_tools/papers/miyawaki_2022.py</code> <pre><code>def get_dvmse_dy(R_a: xr.DataArray, lh: xr.DataArray, sh: xr.DataArray, dmse_dt: xr.DataArray) -&gt; xr.DataArray:\n    \"\"\"\n    This infers the divergence of moist static energy flux, $&lt;[\\\\partial_y vm]&gt;$, from equation $(2)$ in the paper:\n\n    $&lt;[\\\\partial_t m]&gt; + &lt;[\\\\partial_y vm]&gt; = [R_a] + [LH] + [SH]$\n\n    It is not computed directly for reasons outlined in section 2b of the paper.\n\n    Args:\n        R_a: `float [n_time]`.&lt;/br&gt;\n            Atmospheric radiative heating rate i.e. difference between top of atmosphere and surface radiative fluxes.\n            Negative means atmosphere is cooling. Units: $W/m^2$.&lt;/br&gt;\n            This is what is returned by `frierson_atmospheric_heating` if using the grey radiation with the\n            *Frierson* scheme.\n        lh: `float [n_time]`.&lt;/br&gt;\n            Surface latent heat flux (up is positive). This is saved by *Isca* if the variable `flux_lhe` in the\n            `mixed_layer` module is specified in the diagnostic table. Units: $W/m^2$.\n        sh: `float [n_time]`.&lt;/br&gt;\n            Surface sensible heat flux (up is positive). This is saved by *Isca* if the variable `flux_t` in the\n            `mixed_layer` module is specified in the diagnostic table. Units: $W/m^2$.\n        dmse_dt: `float [n_time]`.&lt;/br&gt;\n            The time derivative of the mass weighted vertical integral of the zonal mean moist static energy at\n            each time, $&lt;[\\\\partial_t m]&gt;$. Units: $W/m^2$.\n\n    Returns:\n        `float [n_time]`.&lt;/br&gt;\n            $&lt;[\\\\partial_y vm]&gt;$ - Mass weighted vertical integral of the zonal mean meridional divergence of\n            moist static energy flux, $vm$.\n    \"\"\"\n    return R_a + lh + sh - dmse_dt\n</code></pre>"},{"location":"code/papers/miyawaki_2022/#isca_tools.papers.miyawaki_2022.get_r1","title":"<code>get_r1(R_a, dmse_dt, dvmse_dy, time=None, spline_smoothing_factor=0)</code>","text":"<p>Returns the non-dimensional number, \\(R_1 = \\frac{\\partial_t m + \\partial_y vm}{R_a}\\).</p> <p>Parameters:</p> Name Type Description Default <code>R_a</code> <code>DataArray</code> <p><code>float [n_time]</code>. Atmospheric radiative heating rate i.e. difference between top of atmosphere and surface radiative fluxes. Negative means atmosphere is cooling. Units: \\(W/m^2\\). This is what is returned by <code>frierson_atmospheric_heating</code> if using the grey radiation with the Frierson scheme.</p> required <code>dmse_dt</code> <code>DataArray</code> <p><code>float [n_time]</code>. The time derivative of the mass weighted vertical integral of the zonal mean moist static energy at each time, \\(&lt;[\\partial_t m]&gt;\\). Units: \\(W/m^2\\).</p> required <code>dvmse_dy</code> <code>DataArray</code> <p><code>float [n_time]</code>. \\(&lt;[\\partial_y vm]&gt;\\) - Mass weighted vertical integral of the zonal mean meridional divergence of moist static energy flux, \\(vm\\).</p> required <code>time</code> <code>Optional[DataArray]</code> <p><code>None</code> or <code>float [n_time]</code>. Time at which data recorded. Only required if using the <code>spline_smoothing_factor&gt;0</code>. Units: second.</p> <code>None</code> <code>spline_smoothing_factor</code> <code>float</code> <p><code>float</code>. If positive, a spline will be fit to smooth \\(R_1\\). Typical: 0.2.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataArray</code> <p><code>float [n_time]</code>.</p> <code>DataArray</code> <p>The non-dimensional number, \\(R_1 = \\frac{\\partial_t m + \\partial_y vm}{R_a}\\).</p> Source code in <code>isca_tools/papers/miyawaki_2022.py</code> <pre><code>def get_r1(R_a: xr.DataArray, dmse_dt: xr.DataArray, dvmse_dy: xr.DataArray,\n           time: Optional[xr.DataArray] = None, spline_smoothing_factor: float = 0) -&gt; xr.DataArray:\n    \"\"\"\n    Returns the non-dimensional number, $R_1 = \\\\frac{\\\\partial_t m + \\\\partial_y vm}{R_a}$.\n\n    Args:\n        R_a: `float [n_time]`.&lt;/br&gt;\n            Atmospheric radiative heating rate i.e. difference between top of atmosphere and surface radiative fluxes.\n            Negative means atmosphere is cooling. Units: $W/m^2$.&lt;/br&gt;\n            This is what is returned by `frierson_atmospheric_heating` if using the grey radiation with the\n            *Frierson* scheme.\n        dmse_dt: `float [n_time]`.&lt;/br&gt;\n            The time derivative of the mass weighted vertical integral of the zonal mean moist static energy at\n            each time, $&lt;[\\\\partial_t m]&gt;$. Units: $W/m^2$.\n        dvmse_dy: `float [n_time]`.&lt;/br&gt;\n            $&lt;[\\\\partial_y vm]&gt;$ - Mass weighted vertical integral of the zonal mean meridional divergence of\n            moist static energy flux, $vm$.\n        time: `None` or `float [n_time]`.&lt;/br&gt;\n            Time at which data recorded. Only required if using the `spline_smoothing_factor&gt;0`. Units: *second*.\n        spline_smoothing_factor: `float`.&lt;/br&gt;\n            If positive, a spline will be fit to smooth $R_1$. *Typical: 0.2*.&lt;/br&gt;\n\n    Returns:\n        `float [n_time]`.&lt;/br&gt;\n        The non-dimensional number, $R_1 = \\\\frac{\\\\partial_t m + \\\\partial_y vm}{R_a}$.\n\n    \"\"\"\n    r1 = (dmse_dt + dvmse_dy)/R_a\n    if spline_smoothing_factor &gt; 0:\n        if time is None:\n            raise ValueError('Specified spline but no time array given.')\n        spl = UnivariateSpline(time, r1, s=spline_smoothing_factor)\n        r1 = spl(time)\n    elif spline_smoothing_factor &lt; 0:\n        raise ValueError(f'spline_smoothing_factor = {spline_smoothing_factor}, which is negative. It must be &gt;=0.')\n    return r1\n</code></pre>"},{"location":"code/plot/base/","title":"Base","text":""},{"location":"code/plot/base/#isca_tools.plot.base.colored_line","title":"<code>colored_line(x, y, c, ax, **lc_kwargs)</code>","text":"<p>Plot a line with a color specified along the line by a third value. Code copied from matplotlib website.</p> <p>It does this by creating a collection of line segments. Each line segment is made up of two straight lines each connecting the current (x, y) point to the midpoints of the lines connecting the current point with its two neighbors. This creates a smooth line with no gaps between the line segments.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Horizontal coordinates of data points</p> required <code>y</code> <code>ndarray</code> <p>Vertical coordinates of data points</p> required <code>c</code> <code>ndarray</code> <p>The color values, which should be the same size as <code>x</code> and <code>y</code>.</p> required <code>ax</code> <code>Axes</code> <p>Axis object on which to plot the colored line.</p> required <code>**lc_kwargs</code> <p>Any additional arguments to pass to matplotlib.collections.LineCollection constructor. This should not include the array keyword argument because that is set to the color argument. If provided, it will be overridden.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LineCollection</code> <p>The generated line collection representing the colored line.</p> Source code in <code>isca_tools/plot/base.py</code> <pre><code>def colored_line(x: np.ndarray, y: np.ndarray, c: np.ndarray, ax: plt.Axes, **lc_kwargs) -&gt; LineCollection:\n    \"\"\"\n    Plot a line with a color specified along the line by a third value.\n    Code copied from\n    [matplotlib website](https://matplotlib.org/stable/gallery/lines_bars_and_markers/multicolored_line.html).\n\n    It does this by creating a collection of line segments. Each line segment is\n    made up of two straight lines each connecting the current (x, y) point to the\n    midpoints of the lines connecting the current point with its two neighbors.\n    This creates a smooth line with no gaps between the line segments.\n\n    Args:\n        x: Horizontal coordinates of data points\n        y: Vertical coordinates of data points\n        c: The color values, which should be the same size as `x` and `y`.\n        ax: Axis object on which to plot the colored line.\n        **lc_kwargs:\n            Any additional arguments to pass to matplotlib.collections.LineCollection\n            constructor. This should not include the array keyword argument because\n            that is set to the color argument. If provided, it will be overridden.\n\n    Returns:\n        The generated line collection representing the colored line.\n    \"\"\"\n    if \"array\" in lc_kwargs:\n        warnings.warn('The provided \"array\" keyword argument will be overridden')\n\n    # Default the capstyle to butt so that the line segments smoothly line up\n    default_kwargs = {\"capstyle\": \"butt\"}\n    default_kwargs.update(lc_kwargs)\n\n    # Compute the midpoints of the line segments. Include the first and last points\n    # twice so we don't need any special syntax later to handle them.\n    x = np.asarray(x)\n    y = np.asarray(y)\n    x_midpts = np.hstack((x[0], 0.5 * (x[1:] + x[:-1]), x[-1]))\n    y_midpts = np.hstack((y[0], 0.5 * (y[1:] + y[:-1]), y[-1]))\n\n    # Determine the start, middle, and end coordinate pair of each line segment.\n    # Use the reshape to add an extra dimension so each pair of points is in its\n    # own list. Then concatenate them to create:\n    # [\n    #   [(x1_start, y1_start), (x1_mid, y1_mid), (x1_end, y1_end)],\n    #   [(x2_start, y2_start), (x2_mid, y2_mid), (x2_end, y2_end)],\n    #   ...\n    # ]\n    coord_start = np.column_stack((x_midpts[:-1], y_midpts[:-1]))[:, np.newaxis, :]\n    coord_mid = np.column_stack((x, y))[:, np.newaxis, :]\n    coord_end = np.column_stack((x_midpts[1:], y_midpts[1:]))[:, np.newaxis, :]\n    segments = np.concatenate((coord_start, coord_mid, coord_end), axis=1)\n\n    lc = LineCollection(segments, **default_kwargs)\n    lc.set_array(c)  # set the colors of each segment\n\n    return ax.add_collection(lc)\n</code></pre>"},{"location":"code/plot/base/#isca_tools.plot.base.fig_resize","title":"<code>fig_resize(fig, width_fig_desired=None, ar=4 / 3)</code>","text":"<p>Change height of figure such that aspect ratio of each subplot is set to <code>ar</code>, while the width is maintained at the same value.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Figure to resize.</p> required <code>width_fig_desired</code> <code>Optional[float]</code> <p>Width of figure after resize in inches. If not provided, will keep current width.</p> <code>None</code> <code>ar</code> <code>float</code> <p>Desired aspect ratio (width/height) of each subplot within <code>fig</code>.</p> <code>4 / 3</code> Source code in <code>isca_tools/plot/base.py</code> <pre><code>def fig_resize(fig: plt.Figure, width_fig_desired: Optional[float]=None, ar: float=4/3) -&gt; None:\n    \"\"\"\n    Change height of figure such that aspect ratio of each subplot is set to `ar`, while the width is maintained\n    at the same value.\n\n    Args:\n        fig: Figure to resize.\n        width_fig_desired: Width of figure after resize in inches. If not provided, will keep current width.\n        ar: Desired aspect ratio (width/height) of each subplot within `fig`.\n\n    \"\"\"\n    if width_fig_desired is None:\n        width_fig_desired = fig.get_size_inches()[0]\n    n_row, n_col = get_fig_n_rows_cols(fig)\n    width_subplot_desired = (width_fig_desired / n_col)\n    height_subplot_desired = width_subplot_desired / ar  # height of each subplot\n    height_fig_desired = height_subplot_desired * n_row\n    fig.set_size_inches(width_fig_desired, height_fig_desired)\n    return None\n</code></pre>"},{"location":"code/plot/base/#isca_tools.plot.base.get_fig_n_rows_cols","title":"<code>get_fig_n_rows_cols(fig)</code>","text":"<p>Returns the number of rows and columns of subplots in a figure.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Figure to find subplot arrangement for.</p> required <p>Returns:</p> Name Type Description <code>n_row</code> <code>int</code> <p>Number of rows of subplots.</p> <code>n_col</code> <code>int</code> <p>Number of columns of subplots.</p> Source code in <code>isca_tools/plot/base.py</code> <pre><code>def get_fig_n_rows_cols(fig: plt.Figure) -&gt; Tuple[int, int]:\n    \"\"\"\n    Returns the number of rows and columns of subplots in a figure.\n\n    Args:\n        fig: Figure to find subplot arrangement for.\n\n    Returns:\n        n_row: Number of rows of subplots.\n        n_col: Number of columns of subplots.\n    \"\"\"\n    # Get the axes objects from the figure\n    axes = fig.axes\n\n    # To determine the number of rows and columns, we need to find the grid shape\n    # by considering the number of axes and their positions\n\n    # Sort axes by their x and y positions to infer the grid structure\n    sorted_axes = sorted(axes, key=lambda ax: (ax.get_position().y0, ax.get_position().x0))\n\n    # Assuming the grid is arranged row-wise (top-to-bottom, left-to-right)\n    # Calculate number of rows and columns based on sorted axes positions\n    n_col = len(set(ax.get_position().x0 for ax in sorted_axes))  # Number of unique x positions\n    n_row = len(set(ax.get_position().y0 for ax in sorted_axes))  # Number of unique y positions\n    return n_row, n_col\n</code></pre>"},{"location":"code/plot/base/#isca_tools.plot.base.label_subplots","title":"<code>label_subplots(fig, ax_list, labels=None, fontsize=9, fontcolor='k', box_alpha=1, pos_x=5, pos_y=-5)</code>","text":"<p>This adds a label to each subplot in the top right corner.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Figure containing subplots to add labels to.</p> required <code>ax_list</code> <code>Union[Axes, List[Axes]]</code> <p>[n_ax] List of all axes in the figure. If only one figure, can just provide the axes and not a list.</p> required <code>labels</code> <code>Optional[List[str]]</code> <p>[n_ax] Label for each subplot. If not given, the label will just be the letters of the alphabet: a, b, c, ...</p> <code>None</code> <code>fontsize</code> <code>float</code> <p>Font size to use</p> <code>9</code> <code>fontcolor</code> <code>str</code> <p>What color font to use</p> <code>'k'</code> <code>box_alpha</code> <code>float</code> <p>Opacity of box bounding text (1 is opaque and 0 is transparent)</p> <code>1</code> <code>pos_x</code> <code>float</code> <p>Can specify distance from top right corner.</p> <code>5</code> <code>pos_y</code> <code>float</code> <p>Can specify distance from top right corner (should be negative).</p> <code>-5</code> Source code in <code>isca_tools/plot/base.py</code> <pre><code>def label_subplots(fig: plt.Figure, ax_list: Union[plt.Axes, List[plt.Axes]], labels: Optional[List[str]] = None,\n                   fontsize: float = 9, fontcolor: str = 'k', box_alpha: float = 1,\n                   pos_x: float=5, pos_y: float=-5) -&gt; None:\n    \"\"\"\n    This adds a label to each subplot in the top right corner.\n\n    Args:\n        fig: Figure containing subplots to add labels to.\n        ax_list: [n_ax]\n            List of all axes in the figure. If only one figure, can just provide the axes and not a list.\n        labels: [n_ax]\n            Label for each subplot. If not given, the label will just be the letters of the alphabet: a, b, c, ...\n        fontsize: Font size to use\n        fontcolor: What color font to use\n        box_alpha: Opacity of box bounding text (1 is opaque and 0 is transparent)\n        pos_x: Can specify distance from top right corner.\n        pos_y: Can specify distance from top right corner (should be negative).\n    \"\"\"\n    if isinstance(ax_list, plt.Axes):\n        # If only provided one axes, make it into a list\n        ax_list = [ax_list]\n    trans = mtransforms.ScaledTranslation(pos_x / 72, pos_y / 72, fig.dpi_scale_trans)\n    if labels is None:\n        labels = [f\"{chr(ord('a') + i)})\" for i in range(len(ax_list))]\n    if len(labels) != len(ax_list):\n        raise ValueError(f'{len(labels)} labels provided but there are {len(ax_list)} axes')\n    for i, ax in enumerate(ax_list):\n        ax.text(0.0, 1.0, labels[i], transform=ax.transAxes + trans,\n                fontsize=fontsize, verticalalignment='top', color=fontcolor,\n                bbox=dict(facecolor='1', edgecolor='none', pad=3.0, alpha=box_alpha))\n    return None\n</code></pre>"},{"location":"code/plot/base/#isca_tools.plot.base.savefig","title":"<code>savefig(fig, file_name='output', output_dir='/Users/joshduffield/Desktop/', format='pdf', dpi=800, bbox_inches='tight', pad_inches=0.05, overwrite_file=False, save_if_exists=True)</code>","text":"<p>Function to save figure, basically just calls <code>plt.savefig</code> but has more useful default values, and option to not overwrite figure if already exists.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Matplolib figure that you would like to save</p> required <code>file_name</code> <code>str</code> <p>Name of saved figure file in <code>output_dir</code>. Should not include <code>format</code>.</p> <code>'output'</code> <code>output_dir</code> <code>str</code> <p>Directory into which figure file will be saved.</p> <code>'/Users/joshduffield/Desktop/'</code> <code>format</code> <code>str</code> <p>The file format, e.g. <code>'png'</code>, <code>'pdf'</code>, <code>'svg'</code>, <code>'jpeg'</code>. If a different format is included in <code>file_name</code>, then the format in <code>file_name</code> will be used.</p> <code>'pdf'</code> <code>dpi</code> <code>Union[float, str]</code> <p>The resolution in dots per inch. If <code>'figure'</code>, use the figure's dpi value.</p> <code>800</code> <code>bbox_inches</code> <code>Optional[str]</code> <p>Bounding box in inches: only the given portion of the figure is saved. If <code>'tight'</code>, try to figure out the tight bbox of the figure.</p> <code>'tight'</code> <code>pad_inches</code> <code>Union[float, str]</code> <p>Amount of padding in inches around the figure when bbox_inches is 'tight'. If <code>'layout'</code> use the padding from the constrained or compressed layout engine; ignored if one of those engines is not in use.</p> <code>0.05</code> <code>overwrite_file</code> <code>bool</code> <p>If <code>False</code> and file already exists, will add an integer (starting with <code>2</code>) to end of <code>file_name</code> until find a name that does not exist. Only relevant if <code>save_if_exists=True</code>.</p> <code>False</code> <code>save_if_exists</code> <code>bool</code> <p>If <code>False</code> and file already exists, will not save any file, otherwise will save according to <code>overwrite_file</code>.</p> <code>True</code> Source code in <code>isca_tools/plot/base.py</code> <pre><code>def savefig(fig: plt.Figure, file_name: str = 'output', output_dir: str = '/Users/joshduffield/Desktop/',\n            format: str = 'pdf', dpi: Union[float, str] = 800, bbox_inches: Optional[str] = 'tight',\n            pad_inches: Union[float, str] = 0.05, overwrite_file: bool = False, save_if_exists: bool = True) -&gt; None:\n    \"\"\"\n    Function to save figure, basically just calls\n    [`plt.savefig`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html) but has more useful\n    default values, and option to not overwrite figure if already exists.\n\n    Args:\n        fig: Matplolib figure that you would like to save\n        file_name: Name of saved figure file in `output_dir`. Should not include `format`.\n        output_dir: Directory into which figure file will be saved.\n        format: The file format, e.g. `'png'`, `'pdf'`, `'svg'`, `'jpeg'`.&lt;/br&gt;\n            If a different format is included in `file_name`, then the format in `file_name` will be used.\n        dpi: The resolution in dots per inch. If `'figure'`, use the figure's dpi value.\n        bbox_inches: Bounding box in inches: only the given portion of the figure is saved.\n            If `'tight'`, try to figure out the tight bbox of the figure.\n        pad_inches: Amount of padding in inches around the figure when bbox_inches is 'tight'.\n            If `'layout'` use the padding from the constrained or compressed layout engine;\n            ignored if one of those engines is not in use.\n        overwrite_file: If `False` and file already exists, will add an integer (starting with `2`) to end of\n            `file_name` until find a name that does not exist. Only relevant if `save_if_exists=True`.\n        save_if_exists: If `False` and file already exists, will not save any file, otherwise will save according\n            to `overwrite_file`.\n\n    \"\"\"\n    format_array = ['png', 'pdf', 'svg', 'jpeg']\n    if format not in format_array:\n        raise ValueError(f'Format {format} is not supported, must be in {format_array}')\n\n    # If file_name contains a different format to `format`, use format in file_name\n    format_with_dot = '.' + format.replace(\".\", \"\")\n    for key in format_array:\n        if f\".{key}\" in file_name:\n            format_with_dot = f\".{key}\"\n\n    # Check to ensure don't have the format included twice\n    if format_with_dot in file_name:\n        file_name_use = file_name.replace(format_with_dot, '')\n    else:\n        file_name_use = file_name\n    file_path = os.path.join(output_dir, file_name_use + format_with_dot)\n    if os.path.isfile(file_path) and not save_if_exists:\n        pass\n    else:\n        if not overwrite_file:\n            # Add number to file name so does not overwrite existing file\n            i = 1\n            while os.path.isfile(file_path):\n                i += 1\n                file_path = os.path.join(output_dir, file_name_use + f'{i}{format_with_dot}')\n        fig.savefig(file_path, dpi=dpi, bbox_inches=bbox_inches, pad_inches=pad_inches)\n    return None\n</code></pre>"},{"location":"code/plot/base/#isca_tools.plot.base.update_fontsize","title":"<code>update_fontsize(fig, base_fontsize=8, base_ax_width=2.464)</code>","text":"<p>Resize fontsize based on subplot width, given that <code>base_fontsize</code> is a good fontsize for a subplot of width <code>base_ax_width</code> inches.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Figure to change fontsize for.</p> required <code>base_fontsize</code> <code>float</code> <p>A good fontsize for a subplot of width <code>base_ax_width</code> inches.</p> <code>8</code> <code>base_ax_width</code> <code>float</code> <p>A subplot of width <code>base_ax_width</code> inches, looks good with fontsize set to <code>base_fontsize</code>.</p> <code>2.464</code> Source code in <code>isca_tools/plot/base.py</code> <pre><code>def update_fontsize(fig: plt.Figure, base_fontsize: float=8, base_ax_width: float=2.464) -&gt; None:\n    \"\"\"\n    Resize fontsize based on subplot width, given that `base_fontsize` is a good fontsize for a subplot\n    of width `base_ax_width` inches.\n\n    Args:\n        fig: Figure to change fontsize for.\n        base_fontsize: A good fontsize for a subplot of width `base_ax_width` inches.\n        base_ax_width: A subplot of width `base_ax_width` inches, looks good with fontsize set to `base_fontsize`.\n\n    \"\"\"\n    ax_width = fig.axes[0].get_position().width * fig.get_size_inches()[0]      # use first axes to get subplot width\n    scale_factor = ax_width/base_ax_width\n    new_fontsize = scale_factor * base_fontsize\n    for text in fig.findobj(plt.Text):  # Find all text objects\n        text.set_fontsize(new_fontsize)\n    return None\n</code></pre>"},{"location":"code/plot/base/#isca_tools.plot.base.update_linewidth","title":"<code>update_linewidth(fig, base_linewidth=1, base_ax_width=2.464)</code>","text":"<p>Resize linewidths based on subplot width, given that <code>base_linewidth</code> is a good linewidth for a subplot of width <code>base_ax_width</code> inches.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Figure to change linewidth for.</p> required <code>base_linewidth</code> <code>float</code> <p>A good linewidth for a subplot of width <code>base_ax_width</code> inches.</p> <code>1</code> <code>base_ax_width</code> <code>float</code> <p>A subplot of width <code>base_ax_width</code> inches, looks good with linewidth set to <code>base_linewidth</code>.</p> <code>2.464</code> Source code in <code>isca_tools/plot/base.py</code> <pre><code>def update_linewidth(fig: plt.Figure, base_linewidth: float=1, base_ax_width: float=2.464) -&gt; None:\n    \"\"\"\n    Resize linewidths based on subplot width, given that `base_linewidth` is a good linewidth for a subplot\n    of width `base_ax_width` inches.\n\n    Args:\n        fig: Figure to change linewidth for.\n        base_linewidth: A good linewidth for a subplot of width `base_ax_width` inches.\n        base_ax_width: A subplot of width `base_ax_width` inches, looks good with linewidth set to `base_linewidth`.\n\n    \"\"\"\n    ax_width = fig.axes[0].get_position().width * fig.get_size_inches()[0]  # use first axes to get subplot width\n    scale_factor = ax_width / base_ax_width\n    new_linewidth = base_linewidth * scale_factor\n\n    # Set new line width for the plot\n    for ax in fig.axes:\n        for line in ax.get_lines():\n            line.set_linewidth(new_linewidth)\n    return None\n</code></pre>"},{"location":"code/plot/land/","title":"Land","text":""},{"location":"code/plot/land/#isca_tools.plot.land.show_land","title":"<code>show_land(land_file, fig=None, ax=None)</code>","text":"<p>Quick function to show where land is and any topography</p> <p>Parameters:</p> Name Type Description Default <code>land_file</code> <code>str</code> <p>Location of .nc file created using the <code>write_land</code> function.</p> required <code>fig</code> <code>Optional[Figure]</code> <p>Figure to show where land is.</p> <code>None</code> <code>ax</code> <code>Optional[Axes]</code> <p>Axes in Figure to show where land is.</p> <code>None</code> Source code in <code>isca_tools/plot/land.py</code> <pre><code>def show_land(land_file: str, fig: Optional[plt.Figure] = None, ax: Optional[plt.Axes] = None):\n    \"\"\"\n    Quick function to show where land is and any topography\n\n    Args:\n        land_file: Location of *.nc* file created using the `write_land` function.\n        fig: Figure to show where land is.\n        ax: Axes in Figure to show where land is.\n    \"\"\"\n    land_data = Dataset(land_file, 'r', format='NETCDF3_CLASSIC')\n    lons = land_data.variables['lon'][:]\n    lats = land_data.variables['lat'][:]\n    lon_array, lat_array = np.meshgrid(lons, lats)\n    if fig is None:\n        fig = plt.figure()\n    if ax is None:\n        ax = fig.gca()\n    land_array = np.asarray(land_data.variables['land_mask'])\n    if land_array.max() &gt; 0:\n        ax.contour(lon_array, lat_array, land_array, levels=[0.9, 1.1], linewidths=3, colors='k')\n    topo_array = np.asarray(land_data.variables['zsurf'])\n    if topo_array.max() &gt; 0:\n        cs = ax.contourf(lon_array, lat_array, topo_array, cmap=plt.get_cmap('RdBu_r'))\n        cb = plt.colorbar(cs, shrink=0.5, extend='both')\n    else:\n        # If no topography, show ocean as blue and land as sand coloured\n        topo_array[land_array &gt; 0] = 1\n        cs = ax.pcolormesh(lon_array, lat_array, topo_array, cmap=plt.get_cmap('Paired'), vmin=-0.2, vmax=1.2)\n    ax.set_xticks(np.linspace(0, 360, 13))\n    ax.set_yticks(np.linspace(-90, 90, 7))\n    ax.set_xlabel('Longitude [deg]')\n    ax.set_ylabel('Latitude [deg]')\n</code></pre>"},{"location":"code/plot/spin_up/","title":"Spin Up","text":""},{"location":"code/plot/spin_up/#isca_tools.plot.spin_up.plot_spin_up","title":"<code>plot_spin_up(olr, sw_net_down, t_surf, ax)</code>","text":"<p>Function to plot how net TOA flux and mean global surface temperature evolve with time. Net flux should converge to zero, once spin up has finished.</p> <p><code>olr</code>, <code>sw_net_down</code> and <code>t_surf</code> are just variables saved in the dataset .nc file produced by Isca e.g. <code>t_surf = ds.t_surf</code> if <code>ds</code> is the dataset for the experiment.</p> <p>Parameters:</p> Name Type Description Default <code>olr</code> <code>DataArray</code> <p>Outgoing Longwave Radiation with time, longitude and latitude dimensions.</p> required <code>sw_net_down</code> <code>DataArray</code> <p>Net downward shortwave radiation at top of atmosphere, accounting for any reflected due to the <code>albedo</code> or absorbed in the atmosphere due to shortwave optical depth. This can be obtained with <code>isca_tools.utils.radiation.frierson_net_toa_sw_dwn</code> for the grey gas Isca simulation. Dimensions are time, longitude, latitude.</p> required <code>t_surf</code> <code>DataArray</code> <p>Surface temperature with time, longitude and latitude dimensions.</p> required <code>ax</code> <code>Axes</code> <p>Axes to plot results on.</p> required Source code in <code>isca_tools/plot/spin_up.py</code> <pre><code>def plot_spin_up(olr: xr.DataArray, sw_net_down: xr.DataArray, t_surf: xr.DataArray, ax: plt.Axes):\n    \"\"\"\n    Function to plot how net TOA flux and mean global surface temperature evolve with time.\n    Net flux should converge to zero, once spin up has finished.\n\n    `olr`, `sw_net_down` and `t_surf` are just variables saved in the dataset *.nc* file produced by *Isca*\n    e.g. `t_surf = ds.t_surf` if `ds` is the dataset for the experiment.\n\n    Args:\n        olr: Outgoing Longwave Radiation with time, longitude and latitude dimensions.\n        sw_net_down: Net downward shortwave radiation at top of atmosphere, accounting for any reflected due to\n            the `albedo` or absorbed in the atmosphere due to shortwave optical depth.&lt;/br&gt;\n            This can be obtained with `isca_tools.utils.radiation.frierson_net_toa_sw_dwn` for the grey gas Isca\n            simulation.\n            Dimensions are time, longitude, latitude.\n        t_surf: Surface temperature with time, longitude and latitude dimensions.\n        ax: Axes to plot results on.\n    \"\"\"\n    # When area weighted summed over whole globe, TOA OLR-SW should converge to zero\n    olr_sum = area_weighting(olr).sum(dim=['lon', 'lat'])\n    short_wave_sum = area_weighting(sw_net_down).sum(dim=['lon', 'lat'])\n    net_flux = olr_sum - short_wave_sum\n    net_flux.plot.line(ax=ax, color='b')\n    ax.set_ylabel('TOA Net Outgoing FLux / $Wm^{-2}$\\n$OLR - SW_{net}$', color='b')\n    # Add second axes to show how the global average surface temperature evolves\n    ax2 = ax.twinx()\n    t_surf_mean = area_weighting(t_surf).mean(dim=['lon', 'lat']) - 273.15  # In Celsius\n    t_surf_mean.plot.line(ax=ax2, color='r')\n    ax2.set_ylabel('Surface Temperature / $\u00b0C$', color='r')\n    try:\n        ax.set_xlabel(t_surf.time.units)\n    except AttributeError:\n        pass\n</code></pre>"},{"location":"code/run/base/","title":"Base","text":""},{"location":"code/run/base/#isca_tools.run.base.get_file_suffix","title":"<code>get_file_suffix(dir, suffix)</code>","text":"<p>Returns a list of all files in <code>dir</code> which end in <code>suffix</code>.</p> <p>This is the same function that is in <code>utils.load</code> but cannot do a relative import due to slurm job submission stuff.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Directory of interest.</p> required <code>suffix</code> <code>str</code> <p>Usually the file type of interest e.g. <code>.nml</code> or <code>.txt</code>.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of all files with the correct <code>suffix</code>.</p> Source code in <code>isca_tools/run/base.py</code> <pre><code>def get_file_suffix(dir: str, suffix: str) -&gt; List[str]:\n    \"\"\"\n    Returns a list of all files in `dir` which end in `suffix`.\n\n    This is the same function that is in `utils.load` but cannot do a relative import due to slurm job submission\n    stuff.\n\n    Args:\n        dir: Directory of interest.\n        suffix: Usually the file type of interest e.g. `.nml` or `.txt`.\n\n    Returns:\n        List of all files with the correct `suffix`.\n    \"\"\"\n    file_name = []\n    for file in os.listdir(dir):\n        if file.endswith(suffix):\n            file_name += [file]\n    return file_name\n</code></pre>"},{"location":"code/run/base/#isca_tools.run.base.get_unique_dir_name","title":"<code>get_unique_dir_name(base_dir)</code>","text":"<p>Return a unique directory name by appending a number if needed. E.g., 'results', 'results_1', 'results_2', ...</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str</code> <p>Path to directory</p> required <p>Returns:</p> Name Type Description <code>base_dir</code> <code>str</code> <p>Unique directory name</p> Source code in <code>isca_tools/run/base.py</code> <pre><code>def get_unique_dir_name(base_dir: str) -&gt; str:\n    \"\"\"\n    Return a unique directory name by appending a number if needed.\n    E.g., 'results', 'results_1', 'results_2', ...\n\n    Args:\n        base_dir: Path to directory\n\n    Returns:\n        base_dir: Unique directory name\n    \"\"\"\n    if not os.path.exists(base_dir):\n        return base_dir\n\n    i = 1\n    while True:\n        new_dir = f\"{base_dir}_{i}\"\n        if not os.path.exists(new_dir):\n            return new_dir\n        i += 1\n</code></pre>"},{"location":"code/run/base/#isca_tools.run.base.run_experiment","title":"<code>run_experiment(namelist_file, diag_table_file, slurm=False)</code>","text":"<p>This splits the total simulation up into jobs based on info specified in <code>namelist_file</code> and then runs <code>run_job</code> for each job. If <code>slurm==True</code>, then each job will be submitted to a Slurm queue through the <code>run_slurm.sh</code> script.</p> <p>Parameters:</p> Name Type Description Default <code>namelist_file</code> <code>str</code> <p>File path to namelist <code>nml</code> file for the experiment. This specifies the physical parameters used for the simulation. Also contains <code>experiment_details</code> section which contains the following:</p> <ul> <li><code>name</code>: string. Name of experiment e.g. data saved in folder <code>$GFDL_DATA/{name}</code></li> <li><code>input_dir</code>: string. Directory containing any input files e.g. <code>namelist.nml</code> or <code>co2.nc</code>.</li> <li><code>n_months_total</code>: int. Total duration of simulation in months.</li> <li><code>n_months_job</code>: int. Approximate duration of each job of the simulation in months. E.g. if <code>n_months_total=12</code> and <code>n_months_job=6</code>, the experiment would be split up into 2 jobs each of length 6 months.</li> <li><code>n_nodes</code>: int. Number of nodes to run job on (Slurm info).</li> <li><code>n_cores</code>: int. Number of cores for each node to run job on (Slurm info).</li> <li><code>resolution</code>: string. Horizontal resolution of experiment (options are <code>T21</code>, <code>T42</code> or <code>T85</code>).</li> <li><code>partition</code>: string. Slurm queue to submit job to.</li> <li><code>overwrite_data</code>: bool. If this is <code>True</code> and data already exists in <code>$GFDL_DATA/{name}</code>,     then it will be overwritten. If it is <code>False</code> and the data exists, an error will occur.</li> <li><code>compile</code>: bool. If <code>True</code>, it will recompile the codebase before running the experiment.</li> <li><code>max_walltime</code>: string. Maximum time that job can run on Slurm. E.g. 1 hour would be '01:00:00'.</li> <li><code>delete_restart_files</code>: bool. If <code>True</code>, only the restart file for the final month will be kept.     Otherwise, a restart file will be generated for every month.</li> </ul> required <code>diag_table_file</code> <code>str</code> <p>File path to the diagnostic table file for the experiment. This specifies the outputs of the experiment.</p> required <code>slurm</code> <code>bool</code> <p>If <code>True</code>, will split each job to a Slurm queue. Otherwise, it will just loop over each job locally.</p> <code>False</code> Source code in <code>isca_tools/run/base.py</code> <pre><code>def run_experiment(namelist_file: str, diag_table_file: str, slurm: bool = False):\n    \"\"\"\n    This splits the total simulation up into jobs based on info specified in `namelist_file` and\n    then runs `run_job` for each job. If `slurm==True`, then each job will be submitted to a *Slurm*\n    queue through the `run_slurm.sh` script.\n\n    Args:\n        namelist_file: File path to namelist `nml` file for the experiment.\n            This specifies the physical parameters used for the simulation.\n            Also contains `experiment_details` section which contains the following:\n\n            - `name`: *string*. Name of experiment e.g. data saved in folder `$GFDL_DATA/{name}`\n            - `input_dir`: *string*. Directory containing any input files e.g. `namelist.nml` or `co2.nc`.\n            - `n_months_total`: *int*. Total duration of simulation in months.\n            - `n_months_job`: *int*. Approximate duration of each job of the simulation in months.\n            E.g. if `n_months_total=12` and `n_months_job=6`, the experiment would be split up into 2 jobs each\n            of length 6 months.\n            - `n_nodes`: *int*. Number of nodes to run job on (*Slurm* info).\n            - `n_cores`: *int*. Number of cores for each node to run job on (*Slurm* info).\n            - `resolution`: *string*. Horizontal resolution of experiment (options are `T21`, `T42` or `T85`).\n            - `partition`: *string*. *Slurm* queue to submit job to.\n            - `overwrite_data`: *bool*. If this is `True` and data already exists in `$GFDL_DATA/{name}`,\n                then it will be overwritten. If it is `False` and the data exists, an error will occur.\n            - `compile`: *bool*. If `True`, it will recompile the codebase before running the experiment.\n            - `max_walltime`: *string*. Maximum time that job can run on *Slurm*. E.g. 1 hour would be '01:00:00'.\n            - `delete_restart_files`: *bool*. If `True`, only the restart file for the final month will be kept.\n                Otherwise, a restart file will be generated for every month.\n        diag_table_file: File path to the diagnostic table file for the experiment.\n            This specifies the outputs of the experiment.\n        slurm: If `True`, will split each job to a *Slurm* queue. Otherwise, it will just loop over each\n            job locally.\n    \"\"\"\n    exp_details = f90nml.read(namelist_file)['experiment_details']\n\n    if exp_details['n_months_total'] % exp_details['n_months_job'] == 0:\n        # If you give n_months_job which is exact multiple of n_months_total, do this many jobs\n        n_jobs = int(np.ceil(exp_details['n_months_total'] / exp_details['n_months_job']))\n    else:\n        # Split simulation equally between jobs so no more than 1.5 multiplied by exp_details['n_months_job'] months\n        # would be in each job.\n        n_jobs = int(np.ceil(exp_details['n_months_total'] / (1.5 * exp_details['n_months_job'])))\n    # month_jobs[i] are the months to simulate in job i.\n    month_jobs = np.array_split(np.arange(1, exp_details['n_months_total'] + 1), n_jobs)\n\n    # Specify which node to run\n    if 'nodelist' not in exp_details:\n        # if not given, allow all available nodes\n        slurm_script = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'run_slurm_no_namelist.sh')\n        exp_details['nodelist'] = ''    # just set to empty string\n    else:\n        print(f\"Submitting to node: {exp_details['nodelist']}\")\n        slurm_script = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'run_slurm.sh')\n    # Run this base.py script aas main if using slurm\n    # Because doing this, cannot have any relative imports in this file\n    run_job_script = os.path.realpath(__file__)     # run this script as main if using slurm\n\n    # Get directory name to save debugging and timing info\n    # If directory already exists, will add a number e.g. console_output_1 until finds one that exists.\n    dir_output = os.path.join(os.environ['GFDL_DATA'], exp_details['name'], 'console_output')\n    dir_output = get_unique_dir_name(dir_output)\n    os.makedirs(dir_output, exist_ok=False)\n\n    prev_job_id = ''     # Empty string so don't provide any argument to first slurm script\n    # Iterate over all jobs\n    for month_job in month_jobs:\n        if slurm:\n            # Different script for first vs subsequent scripts, as they are dependent on previous job\n            cmd = (\n                f\"bash {slurm_script if prev_job_id == '' else slurm_script.replace('.sh','_depend.sh')} \"\n                f\"{exp_details['name']} {month_job[0]} {len(month_job)} \"\n                f\"{exp_details['partition']} {exp_details['n_nodes']} {exp_details['n_cores']} \"\n                f\"{namelist_file} {diag_table_file} {exp_details['max_walltime']} {run_job_script} {dir_output} \"\n                f\"{exp_details['nodelist']} {prev_job_id} \"\n            )\n            output = subprocess.check_output(cmd, shell=True).decode(\"utf-8\").strip()   # get job just submitted info\n            print(f\"{output}{prev_job_id if prev_job_id=='' else ' (dependency: job '+prev_job_id+')'}\")\n            prev_job_id = output.split()[-1]  # Save this job id (last word) for the next submission\n        else:\n            run_job(namelist_file, diag_table_file, month_job[0], len(month_job))\n</code></pre>"},{"location":"code/run/base/#isca_tools.run.base.run_job","title":"<code>run_job(namelist_file, diag_table_file, month_start, month_duration)</code>","text":"<p>Runs simulation for <code>month_duration</code> months starting with the month indicated by <code>month_start</code>. Will throw an error if no data found for months prior to <code>month_start</code>.</p> <p>Output will be saved in <code>$GFDL_DATA/{exp_name}</code> with a different folder for each month.</p> <p>Extended from original script. Args:     namelist_file: File path to namelist <code>nml</code> file for the experiment.         This specifies the physical parameters used for the simulation.         Also contains <code>experiment_details</code> section which contains the following:</p> <pre><code>    - `name`: *string*. Name of experiment e.g. data saved in folder `$GFDL_DATA/{name}`\n    - `input_dir`: *string*. Directory containing any input files e.g. `namelist.nml` or `co2.nc`.\n    - `n_months_total`: *int*. Total duration of simulation in months.\n    - `n_months_job`: *int*. Approximate duration of each job of the simulation in months.\n    E.g. if `n_months_total=12` and `n_months_job=6`, the experiment would be split up into 2 jobs each\n    of length 6 months.\n    - `n_nodes`: *int*. Number of nodes to run job on (*Slurm* info).\n    - `n_cores`: *int*. Number of cores for each node to run job on (*Slurm* info).\n    - `resolution`: *string*. Horizontal resolution of experiment (options are `T21`, `T42` or `T85`).\n    - `partition`: *string*. *Slurm* queue to submit job to.\n    - `overwrite_data`: *bool*. If this is `True` and data already exists in `$GFDL_DATA/{name}`,\n        then it will be overwritten. If it is `False` and the data exists, an error will occur.\n    - `compile`: *bool*. If `True`, it will recompile the codebase before running the experiment.\n    - `max_walltime`: *string*. Maximum time that job can run on *Slurm*. E.g. 1 hour would be '01:00:00'.\n    - `delete_restart_files`: *bool*. If `True`, only the restart file for the final month will be kept.\n        Otherwise, a restart file will be generated for every month.\ndiag_table_file: File path to the diagnostic table file for the experiment.\n    This specifies the outputs of the experiment.\nmonth_start: Index of month at which this job starts the simulation (starting with 1).\nmonth_duration: How many months to run simulation for in this job.\n</code></pre> Source code in <code>isca_tools/run/base.py</code> <pre><code>def run_job(namelist_file: str, diag_table_file: str, month_start: int, month_duration: int):\n    \"\"\"\n    Runs simulation for `month_duration` months starting with the month indicated by `month_start`.\n    Will throw an error if no data found for months prior to `month_start`.\n\n    Output will be saved in `$GFDL_DATA/{exp_name}` with a different folder for each month.\n\n    Extended from [original script](https://github.com/ExeClim/Isca/blob/master/exp/run_isca/isca).\n    Args:\n        namelist_file: File path to namelist `nml` file for the experiment.\n            This specifies the physical parameters used for the simulation.\n            Also contains `experiment_details` section which contains the following:\n\n            - `name`: *string*. Name of experiment e.g. data saved in folder `$GFDL_DATA/{name}`\n            - `input_dir`: *string*. Directory containing any input files e.g. `namelist.nml` or `co2.nc`.\n            - `n_months_total`: *int*. Total duration of simulation in months.\n            - `n_months_job`: *int*. Approximate duration of each job of the simulation in months.\n            E.g. if `n_months_total=12` and `n_months_job=6`, the experiment would be split up into 2 jobs each\n            of length 6 months.\n            - `n_nodes`: *int*. Number of nodes to run job on (*Slurm* info).\n            - `n_cores`: *int*. Number of cores for each node to run job on (*Slurm* info).\n            - `resolution`: *string*. Horizontal resolution of experiment (options are `T21`, `T42` or `T85`).\n            - `partition`: *string*. *Slurm* queue to submit job to.\n            - `overwrite_data`: *bool*. If this is `True` and data already exists in `$GFDL_DATA/{name}`,\n                then it will be overwritten. If it is `False` and the data exists, an error will occur.\n            - `compile`: *bool*. If `True`, it will recompile the codebase before running the experiment.\n            - `max_walltime`: *string*. Maximum time that job can run on *Slurm*. E.g. 1 hour would be '01:00:00'.\n            - `delete_restart_files`: *bool*. If `True`, only the restart file for the final month will be kept.\n                Otherwise, a restart file will be generated for every month.\n        diag_table_file: File path to the diagnostic table file for the experiment.\n            This specifies the outputs of the experiment.\n        month_start: Index of month at which this job starts the simulation (starting with 1).\n        month_duration: How many months to run simulation for in this job.\n    \"\"\"\n    namelist = f90nml.read(namelist_file)\n    diag_table = DiagTable.from_file(diag_table_file)\n    exp_details = namelist['experiment_details']\n\n    if 'column_nml' in namelist:\n        # Different code base if using column\n        cb = ColumnCodeBase.from_directory(GFDL_BASE)\n        if 'idealized_moist_phys_nml' in namelist and 'do_socrates_radiation' in namelist['idealized_moist_phys_nml']:\n            if namelist['idealized_moist_phys_nml']['do_socrates_radiation']:\n                # Different codebase if using socrates\n                cb = SocColumnCodeBase.from_directory(GFDL_BASE)\n    else:\n        cb = IscaCodeBase.from_directory(GFDL_BASE)\n        if 'idealized_moist_phys_nml' in namelist and 'do_socrates_radiation' in namelist['idealized_moist_phys_nml']:\n            if namelist['idealized_moist_phys_nml']['do_socrates_radiation']:\n                # Different codebase if using socrates\n                cb = SocratesCodeBase.from_directory(GFDL_BASE)\n\n    if exp_details['compile']:\n        cb.compile()\n\n    exp = Experiment(exp_details['name'], codebase=cb)\n    # When passing to experiment, will have additional namelist called 'experiment_details' but does not\n    # seem to be an issue.\n    exp.namelist = namelist\n    exp.diag_table = diag_table\n    exp.inputfiles = [namelist_file, diag_table_file]\n\n    # Get any additional input files e.g. co2 concentration or land - all of which should have a .nc suffix.\n    nc_files = get_file_suffix(exp_details['input_dir'], '.nc')\n    if len(nc_files) &gt; 0:\n        nc_files = [os.path.join(exp_details['input_dir'], val) for val in nc_files]\n        exp.inputfiles += nc_files\n\n    exp.set_resolution(exp_details['resolution'])  # set resolution\n    if month_start == 1:\n        # If first month, then there is no restart file to use\n        use_restart = False\n    else:\n        use_restart = True\n    exp.run(month_start, use_restart=use_restart, num_cores=exp_details['n_cores'],\n            overwrite_data=exp_details['overwrite_data'])\n    for i in range(month_start + 1, month_start + month_duration):\n        # For all months but first, use latest restart file to set up simulation.\n        exp.run(i, num_cores=exp_details['n_cores'], overwrite_data=exp_details['overwrite_data'])\n        if exp_details['delete_restart_files']:\n            # delete restart file for month previous to current month as no longer needed and quite large file\n            restart_file = os.path.join(os.environ['GFDL_DATA'], exp_details['name'], 'restarts',\n                                        'res%04d.tar.gz' % (i - 1))\n            if os.path.exists(restart_file):\n                os.remove(restart_file)\n            if i == month_start + 1 and use_restart:\n                # If not the first job, delete the last restart file of the last job\n                restart_file = os.path.join(os.environ['GFDL_DATA'], exp_details['name'], 'restarts',\n                                            'res%04d.tar.gz' % (i - 2))\n                if os.path.exists(restart_file):\n                    os.remove(restart_file)\n</code></pre>"},{"location":"code/run/cmip_time/","title":"Cmip Time","text":""},{"location":"code/run/cmip_time/#isca_tools.run.cmip_time.FakeDT","title":"<code>FakeDT</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>isca_tools/run/cmip_time.py</code> <pre><code>class FakeDT(object):\n    def __init__(self, dates: np.ndarray, units: str ='hours since 1800-01-01 00:00:00',\n                 calendar: str ='standard'):\n        \"\"\"\n        An object created to mimic the behavior of a *pandas* `DatetimeIndex` object, but\n        one that allows for dates from non-standard calendars (e.g. 360 day or no leap).\n\n        Copied from [Isca code](https://github.com/ExeClim/Isca/blob/master/src/extra/python/scripts/cmip_time.py).\n\n        Args:\n            dates: Array or list of `cftime.Datetime` values to be converted into a `FakeDT` object\n            units: Time units (e.g. `'hours since...'`)\n            calendar: Calendar to which `dates` belong.\n        \"\"\"\n        self.dates = np.array(dates)\n        self.units = units\n        self.calendar = calendar\n        self.ndates = len(self.dates)\n        self.dtype = type(dates[0])\n        if self.ndates == 1:\n            self.year = dates.year\n            self.month = dates.month\n            self.day = dates.day\n            self.hour = dates.hour\n            self.minute = dates.minute\n            try:\n                self.dayofyear = dates.timetuple().tm_yday\n            except AttributeError:\n                self.dayofyear = dates.timetuple()[7]\n        else:\n            self.year = np.array([dk.year for dk in dates])\n            self.month = np.array([dk.month for dk in dates])\n            self.day = np.array([dk.day for dk in dates])\n            self.hour = np.array([dk.hour for dk in dates])\n            self.minute = np.array([dk.minute for dk in dates])\n            try:\n                self.dayofyear = np.array([dk.timetuple().tm_yday for dk in dates])\n            except AttributeError:\n                self.dayofyear = np.array([dk.timetuple()[7] for dk in dates])\n\n    def __getitem__(self, idx):\n        # If &lt;idx&gt; is array_like, return a new FakeDT object restricted to those\n        # indicies, if not, just return the member at a particular location\n        if isinstance(idx, (list, np.ma.MaskedArray, np.ndarray)):\n            return FakeDT(self.dates[idx], self.units, self.calendar)\n        else:\n            return self.dates[idx]\n\n    def __str__(self):\n        if self.ndates == 1:\n            return \"[ {}, dtype={} ]\".format(self.dates, type(self.dates))\n        else:\n            out_s = \"[ \"\n            for k in range(self.ndates - 1):\n                if k % 5 == 0:\n                    out_s += \"{},\\n\".format(self.dates[k])\n                else:\n                    out_s += \"{}, \".format(self.dates[k])\n            out_s += \"{}, dtype={} ]\".format(self.dates[-1], type(self.dates))\n        return out_s\n\n    def __reduce__(self):\n        # Special method for pickle to output in binary format\n        return (self.__class__, (self.dates, self.units, self.calendar))\n\n    def __len__(self):\n        return self.ndates\n\n    def get_loc(self, date: cftime.datetime) -&gt; int:\n        \"\"\"\n        FakeDT class method for returning the index of a particular date\n        raises KeyError if the date is not found. Uses bisection method\n\n        Args:\n            date: `netcdftime.datetime` or `datetime.datetime` date for which to search\n\n        Returns:\n            Index of `date` in `self.dates`\n        \"\"\"\n        a, b = 0, len(self.dates) - 1\n        niter = 0\n        # Compare dates using the .timetuple() method, since this works if &lt;date&gt; is\n        # a datetime or netcdftime .datetime, otherwise only == works, not &gt; or &lt;\n        while True and niter &lt; len(self.dates):\n            if self.dates[a] == date:\n                return a\n            elif self.dates[b] == date:\n                return b\n            elif self.dates[a].timetuple() &lt; date.timetuple() \\\n                    and self.dates[b].timetuple() &gt; date.timetuple():\n                c = a + (b - a) / 2\n                if self.dates[c] == date:\n                    return c\n                elif self.dates[c].timetuple() &gt; date.timetuple():\n                    b = c\n                elif self.dates[c].timetuple() &lt; date.timetuple():\n                    a = c\n            else:\n                # First error string only raised if 'c' has been assigned\n                if 'c' in locals():\n                    raise KeyError('Date not found {}, a({}): {}, b({}): {}, '\n                                   'c({}):{}'.format(date, a, self.dates[a],\n                                                     b, self.dates[b],\n                                                     c, self.dates[c]))\n                else:\n                    raise KeyError('Date not found {}, a({}): {},'\n                                   ' b({}): {}'.format(date, a, self.dates[a],\n                                                       b, self.dates[b]))\n            niter += 1\n        return c\n</code></pre>"},{"location":"code/run/cmip_time/#isca_tools.run.cmip_time.FakeDT.__init__","title":"<code>__init__(dates, units='hours since 1800-01-01 00:00:00', calendar='standard')</code>","text":"<p>An object created to mimic the behavior of a pandas <code>DatetimeIndex</code> object, but one that allows for dates from non-standard calendars (e.g. 360 day or no leap).</p> <p>Copied from Isca code.</p> <p>Parameters:</p> Name Type Description Default <code>dates</code> <code>ndarray</code> <p>Array or list of <code>cftime.Datetime</code> values to be converted into a <code>FakeDT</code> object</p> required <code>units</code> <code>str</code> <p>Time units (e.g. <code>'hours since...'</code>)</p> <code>'hours since 1800-01-01 00:00:00'</code> <code>calendar</code> <code>str</code> <p>Calendar to which <code>dates</code> belong.</p> <code>'standard'</code> Source code in <code>isca_tools/run/cmip_time.py</code> <pre><code>def __init__(self, dates: np.ndarray, units: str ='hours since 1800-01-01 00:00:00',\n             calendar: str ='standard'):\n    \"\"\"\n    An object created to mimic the behavior of a *pandas* `DatetimeIndex` object, but\n    one that allows for dates from non-standard calendars (e.g. 360 day or no leap).\n\n    Copied from [Isca code](https://github.com/ExeClim/Isca/blob/master/src/extra/python/scripts/cmip_time.py).\n\n    Args:\n        dates: Array or list of `cftime.Datetime` values to be converted into a `FakeDT` object\n        units: Time units (e.g. `'hours since...'`)\n        calendar: Calendar to which `dates` belong.\n    \"\"\"\n    self.dates = np.array(dates)\n    self.units = units\n    self.calendar = calendar\n    self.ndates = len(self.dates)\n    self.dtype = type(dates[0])\n    if self.ndates == 1:\n        self.year = dates.year\n        self.month = dates.month\n        self.day = dates.day\n        self.hour = dates.hour\n        self.minute = dates.minute\n        try:\n            self.dayofyear = dates.timetuple().tm_yday\n        except AttributeError:\n            self.dayofyear = dates.timetuple()[7]\n    else:\n        self.year = np.array([dk.year for dk in dates])\n        self.month = np.array([dk.month for dk in dates])\n        self.day = np.array([dk.day for dk in dates])\n        self.hour = np.array([dk.hour for dk in dates])\n        self.minute = np.array([dk.minute for dk in dates])\n        try:\n            self.dayofyear = np.array([dk.timetuple().tm_yday for dk in dates])\n        except AttributeError:\n            self.dayofyear = np.array([dk.timetuple()[7] for dk in dates])\n</code></pre>"},{"location":"code/run/cmip_time/#isca_tools.run.cmip_time.FakeDT.get_loc","title":"<code>get_loc(date)</code>","text":"<p>FakeDT class method for returning the index of a particular date raises KeyError if the date is not found. Uses bisection method</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>datetime</code> <p><code>netcdftime.datetime</code> or <code>datetime.datetime</code> date for which to search</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index of <code>date</code> in <code>self.dates</code></p> Source code in <code>isca_tools/run/cmip_time.py</code> <pre><code>def get_loc(self, date: cftime.datetime) -&gt; int:\n    \"\"\"\n    FakeDT class method for returning the index of a particular date\n    raises KeyError if the date is not found. Uses bisection method\n\n    Args:\n        date: `netcdftime.datetime` or `datetime.datetime` date for which to search\n\n    Returns:\n        Index of `date` in `self.dates`\n    \"\"\"\n    a, b = 0, len(self.dates) - 1\n    niter = 0\n    # Compare dates using the .timetuple() method, since this works if &lt;date&gt; is\n    # a datetime or netcdftime .datetime, otherwise only == works, not &gt; or &lt;\n    while True and niter &lt; len(self.dates):\n        if self.dates[a] == date:\n            return a\n        elif self.dates[b] == date:\n            return b\n        elif self.dates[a].timetuple() &lt; date.timetuple() \\\n                and self.dates[b].timetuple() &gt; date.timetuple():\n            c = a + (b - a) / 2\n            if self.dates[c] == date:\n                return c\n            elif self.dates[c].timetuple() &gt; date.timetuple():\n                b = c\n            elif self.dates[c].timetuple() &lt; date.timetuple():\n                a = c\n        else:\n            # First error string only raised if 'c' has been assigned\n            if 'c' in locals():\n                raise KeyError('Date not found {}, a({}): {}, b({}): {}, '\n                               'c({}):{}'.format(date, a, self.dates[a],\n                                                 b, self.dates[b],\n                                                 c, self.dates[c]))\n            else:\n                raise KeyError('Date not found {}, a({}): {},'\n                               ' b({}): {}'.format(date, a, self.dates[a],\n                                                   b, self.dates[b]))\n        niter += 1\n    return c\n</code></pre>"},{"location":"code/run/cmip_time/#isca_tools.run.cmip_time.day_number_to_date","title":"<code>day_number_to_date(time_in, calendar='360_day', time_units='days since 0001-01-01 00:00:00')</code>","text":"<p>Aim is to make the time array have attributes like .month, or .year etc. This doesn't work with normal datetime objects, so FakeDT does this for you. First step is to turn input times into an array of datetime objects, and then FakeDT makes the array have the attributes of the elements themselves.</p> <p>Based on Isca code.</p> <p>Parameters:</p> Name Type Description Default <code>time_in</code> <code>ndarray</code> <p>Array of dates with units of <code>&lt;time units&gt;</code> in <code>units_in</code>.</p> required <code>calendar</code> <code>str</code> <p>Calendar that the dates in <code>time_in</code> correspond to. Valid options are  <code>standard</code>, <code>gregorian</code>, <code>proleptic_gregorian</code>, <code>noleap</code>, <code>365_day</code>, <code>360_day</code>, <code>julian</code>, <code>all_leap</code>, <code>366_day</code>.</p> <code>'360_day'</code> <code>time_units</code> <code>str</code> <p>time units in the form <code>'&lt;time_units&gt; since &lt;ref_date&gt; &lt;ref_time&gt;'</code>. <code>&lt;time units&gt;</code> can be <code>days</code>, <code>hours</code>, <code>minutes</code>, <code>seconds</code>. <code>&lt;ref_date&gt;</code> is in the form <code>yyyy-mm-dd</code>. <code>&lt;ref_time&gt;</code> is in the form <code>hh:mm:ss</code>.</p> <code>'days since 0001-01-01 00:00:00'</code> <p>Returns:</p> Type Description <code>FakeDT</code> <p>Dates given in <code>time_in</code> converted to the <code>FakeDT</code> format.</p> Source code in <code>isca_tools/run/cmip_time.py</code> <pre><code>def day_number_to_date(time_in: np.ndarray, calendar: str = '360_day',\n                       time_units: str = 'days since 0001-01-01 00:00:00') -&gt; FakeDT:\n    \"\"\"\n    Aim is to make the time array have attributes like .month, or .year etc. This doesn't work with\n    normal datetime objects, so FakeDT does this for you. First step is to turn input times\n    into an array of datetime objects, and then FakeDT makes the array have the attributes of the\n    elements themselves.\n\n    Based on [Isca code](https://github.com/ExeClim/Isca/blob/master/src/extra/python/scripts/calendar_calc.py).\n\n    Args:\n        time_in: Array of dates with units of `&lt;time units&gt;` in `units_in`.\n        calendar: Calendar that the dates in `time_in` correspond to.\n            Valid options are  `standard`, `gregorian`, `proleptic_gregorian`, `noleap`, `365_day`, `360_day`,\n            `julian`, `all_leap`, `366_day`.\n        time_units: time units in the form `'&lt;time_units&gt; since &lt;ref_date&gt; &lt;ref_time&gt;'`.&lt;/br&gt;\n            `&lt;time units&gt;` can be `days`, `hours`, `minutes`, `seconds`.&lt;/br&gt;\n            `&lt;ref_date&gt;` is in the form `yyyy-mm-dd`.&lt;/br&gt;\n            `&lt;ref_time&gt;` is in the form `hh:mm:ss`.&lt;/br&gt;\n\n    Returns:\n        Dates given in `time_in` converted to the `FakeDT` format.\n    \"\"\"\n    dates = cftime.num2date(time_in, time_units, calendar)\n    cdftime = FakeDT(dates, units=time_units, calendar=calendar)\n    return cdftime\n</code></pre>"},{"location":"code/run/create_files/","title":"Create Files","text":""},{"location":"code/run/create_files/#isca_tools.run.create_files.create_grid_file","title":"<code>create_grid_file(example_exp_file=None, output_file_path=None, res=None)</code>","text":"<p>Function to create a grid file e.g. <code>t21_grid.nc</code> for a specified resolution, or output data from previous experiment. This grid file is required to create timeseries files and includes the variables <code>lon</code>, <code>lonb</code>, <code>lat</code>, <code>latb</code>, <code>pfull</code> and <code>phalf</code>.</p> Pressure values <p>The <code>'_exp.nc'</code> files in the <code>grid_files</code> folder for each resolution (used when <code>res</code> is specified) contain only 2 pressure values. To change this, the <code>example_exp_file</code> will need to be specified.</p> <p>Function is extended from an Isca script but also includes pressure level info.</p> <p>Parameters:</p> Name Type Description Default <code>example_exp_file</code> <code>Optional[str]</code> <p>Path to the example experiment file (should end with <code>.nc</code>). If <code>res</code> is specified, it will be loaded from the grid_dir with name <code>t{res}_exp.nc</code>.</p> <code>None</code> <code>output_file_path</code> <code>Optional[str]</code> <p>Where to save the grid file (should end with <code>.nc</code>). If <code>res</code> is specified, it will be saved to the grid_dir with name <code>t{res}_grid.nc</code>.</p> <code>None</code> <code>res</code> <code>Optional[Literal[21, 42, 85]]</code> <p>Experiment resolution. Must be either <code>21</code>, <code>42</code> or <code>85</code>.</p> <code>None</code> Source code in <code>isca_tools/run/create_files.py</code> <pre><code>def create_grid_file(example_exp_file: Optional[str] = None, output_file_path: Optional[str] = None,\n                     res: Optional[Literal[21, 42, 85]] = None):\n    \"\"\"\n    Function to create a grid file e.g. `t21_grid.nc` for a specified resolution,\n    or output data from previous experiment.\n    This grid file is required to create timeseries files and includes the variables `lon`, `lonb`, `lat`, `latb`,\n    `pfull` and `phalf`.\n\n    ??? note \"Pressure values\"\n        The `'_exp.nc'` files in the `grid_files` folder for each resolution (used when `res` is specified)\n        contain only 2 pressure values.\n        To change this, the `example_exp_file` will need to be specified.\n\n    Function is extended from an\n    [Isca script](https://github.com/ExeClim/Isca/blob/master/src/extra/python/scripts/gfdl_grid_files/grid_file_generator.py)\n    but also includes pressure level info.\n\n    Args:\n        example_exp_file: Path to the example experiment file (should end with `.nc`).&lt;/br&gt;\n            If `res` is specified, it will be loaded from the grid_dir with name `t{res}_exp.nc`.\n        output_file_path: Where to save the grid file (should end with `.nc`).&lt;/br&gt;\n            If `res` is specified, it will be saved to the grid_dir with name `t{res}_grid.nc`.\n        res: Experiment resolution. Must be either `21`, `42` or `85`.\n\n    \"\"\"\n    if example_exp_file is None and res is not None:\n        # Read in an example data output file for the specified resolution that has suffix '_exp'\n        # This data was produced using the gridfile_namelist.nml and gridfile_diag_table input files.\n        grid_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'grid_files')\n        example_exp_file = os.path.join(grid_dir, f\"t{res}_exp.nc\")\n        if output_file_path is None:\n            output_file_path = os.path.join(grid_dir, f\"t{res}_grid.nc\")\n    elif example_exp_file is None:\n        raise ValueError('Neither example_exp_file nor res was specified.')\n    elif output_file_path is None:\n        raise ValueError('output_file_path was not specified.')\n    if not os.path.exists(example_exp_file):\n        raise ValueError(f\"The file {example_exp_file} does not exist.\")\n    resolution_file = Dataset(example_exp_file, 'r', format='NETCDF3_CLASSIC')\n\n    # Load longitude/latitude information from the file\n    lons = resolution_file.variables['lon'][:]\n    lats = resolution_file.variables['lat'][:]\n    lonsb = resolution_file.variables['lonb'][:]  # longitude edges\n    latsb = resolution_file.variables['latb'][:]  # latitude edges\n    # lonb always has one more value than lon, because lonb refers to longitude edges. Same with lat and latb.\n\n    # Load in pressure information from the file\n    pfull = resolution_file.variables['pfull'][:]\n    phalf = resolution_file.variables['phalf'][:]  # always one more value in phalf than in pfull\n\n    # Save grid data for this resolution to the same folder as input\n\n    if os.path.exists(output_file_path):\n        raise ValueError(f\"The file {output_file_path} already exists.\")\n    else:\n        print('Grid file written to: ' + output_file_path)\n    output_file = Dataset(output_file_path, 'w', format='NETCDF3_CLASSIC')\n\n    output_file.createDimension('lat', lats.shape[0])\n    output_file.createDimension('lon', lons.shape[0])\n    output_file.createDimension('latb', latsb.shape[0])\n    output_file.createDimension('lonb', lonsb.shape[0])\n    output_file.createDimension('pfull', pfull.shape[0])\n    output_file.createDimension('phalf', phalf.shape[0])\n\n    # Create variable for each dimension and give units and axis.\n    latitudes = output_file.createVariable('lat', 'f4', ('lat',))\n    latitudes.units = 'degrees_N'.encode('utf-8')\n    latitudes.cartesian_axis = 'Y'\n    latitudes.long_name = 'latitude'\n\n    longitudes = output_file.createVariable('lon', 'f4', ('lon',))\n    longitudes.units = 'degrees_E'.encode('utf-8')\n    longitudes.cartesian_axis = 'X'\n    longitudes.long_name = 'longitude'\n\n    latitudesb = output_file.createVariable('latb', 'f4', ('latb',))\n    latitudes.edges = 'latb'\n    latitudesb.units = 'degrees_N'.encode('utf-8')\n    latitudesb.cartesian_axis = 'Y'\n    latitudesb.long_name = 'latitude edges'\n\n    longitudesb = output_file.createVariable('lonb', 'f4', ('lonb',))\n    longitudes.edges = 'lonb'\n    longitudesb.units = 'degrees_E'.encode('utf-8')\n    longitudesb.cartesian_axis = 'X'\n    longitudesb.long_name = 'longitude edges'\n\n    pfulls = output_file.createVariable('pfull', 'f4', ('pfull',))\n    pfulls.units = 'hPa'\n    pfulls.cartesian_axis = 'Z'\n    pfulls.positive = 'down'\n    pfulls.long_name = 'full pressure level'\n\n    phalfs = output_file.createVariable('phalf', 'f4', ('phalf',))\n    phalfs.units = 'hPa'\n    phalfs.cartesian_axis = 'Z'\n    phalfs.positive = 'down'\n    phalfs.long_name = 'half pressure level'\n\n    # Assign values to each dimension variable.\n    latitudes[:] = lats\n    longitudes[:] = lons\n    latitudesb[:] = latsb\n    longitudesb[:] = lonsb\n    pfulls[:] = pfull\n    phalfs[:] = phalf\n\n    output_file.close()\n</code></pre>"},{"location":"code/run/create_files/#isca_tools.run.create_files.create_time_arr","title":"<code>create_time_arr(duration_days, time_spacing, start_year=0, start_month=1, start_day=1, start_time='00:00:00', calendar='360_day')</code>","text":"<p>Creates a <code>time_arr</code> which indicates the times in the simulation when a specified variable e.g. \\(CO_2\\) concentration may be varied.</p> <p>Function is extended from an <code>create_time_arr</code> given by Isca.</p> <p>Parameters:</p> Name Type Description Default <code>duration_days</code> <code>int</code> <p>Length of time in days that a variable is varying in the simulation.</p> required <code>time_spacing</code> <code>int</code> <p>Spacing between change in variable value in days. E.g. if <code>time_spacing=360</code> then you would change the value of the variable every year if <code>calendar='360_day'</code>.</p> required <code>start_year</code> <code>int</code> <p>Start year of simulation, must be either <code>0</code> or a 4 digit integer e.g. <code>2000</code>.</p> <code>0</code> <code>start_month</code> <code>int</code> <p>Start month of simulation, January is <code>1</code>.</p> <code>1</code> <code>start_day</code> <code>int</code> <p>Start day of simulation, first day of month is <code>1</code>.</p> <code>1</code> <code>start_time</code> <code>str</code> <p>Start time of simulation in the form <code>hh:mm:ss</code>.</p> <code>'00:00:00'</code> <code>calendar</code> <code>str</code> <p>Calendar used for simulation. Valid options are  <code>standard</code>, <code>gregorian</code>, <code>proleptic_gregorian</code>, <code>noleap</code>, <code>365_day</code>, <code>360_day</code>, <code>julian</code>, <code>all_leap</code>, <code>366_day</code>.</p> <code>'360_day'</code> <p>Returns:</p> Type Description <code>FakeDT</code> <p><code>time_arr</code>: <code>cftime.datetime [duration_days/time_spacing]</code>. A <code>FakeDT</code> object containing a <code>cftime.datetime</code> date for each date in <code>day_number</code>.</p> <code>ndarray</code> <p><code>day_number</code>: <code>int [duration_days/time_spacing]</code>. Index of days of simulation on which variable changes.</p> <code>str</code> <p><code>time_units</code>: time units in the form <code>'days since &lt;ref_date&gt; &lt;ref_time&gt;'</code>. <code>&lt;ref_date&gt;</code> is in the form <code>yyyy-mm-dd</code>. <code>&lt;ref_time&gt;</code> is in the form <code>hh:mm:ss</code>.</p> Source code in <code>isca_tools/run/create_files.py</code> <pre><code>def create_time_arr(duration_days: int, time_spacing: int, start_year: int = 0, start_month: int = 1,\n                    start_day: int = 1, start_time: str = '00:00:00',\n                    calendar: str = '360_day') -&gt; Tuple[FakeDT, np.ndarray, str]:\n    \"\"\"\n    Creates a `time_arr` which indicates the times in the simulation when a specified variable e.g. $CO_2$ concentration\n    may be varied.\n\n    Function is extended from an\n    [`create_time_arr`](https://github.com/ExeClim/Isca/blob/master/src/extra/python/scripts/create_timeseries.py)\n    given by *Isca*.\n\n    Args:\n        duration_days: Length of time in days that a variable is varying in the simulation.\n        time_spacing: Spacing between change in variable value in days.\n            E.g. if `time_spacing=360` then you would change the value of the variable every year if\n            `calendar='360_day'`.\n        start_year: Start year of simulation, must be either `0` or a 4 digit integer e.g. `2000`.\n        start_month: Start month of simulation, January is `1`.\n        start_day: Start day of simulation, first day of month is `1`.\n        start_time: Start time of simulation in the form `hh:mm:ss`.\n        calendar: Calendar used for simulation.\n            Valid options are  `standard`, `gregorian`, `proleptic_gregorian`, `noleap`, `365_day`, `360_day`,\n            `julian`, `all_leap`, `366_day`.\n\n    Returns:\n        `time_arr`: `cftime.datetime [duration_days/time_spacing]`.\n            A `FakeDT` object containing a `cftime.datetime` date for each date in `day_number`.\n        `day_number`: `int [duration_days/time_spacing]`.\n            Index of days of simulation on which variable changes.\n        `time_units`: time units in the form `'days since &lt;ref_date&gt; &lt;ref_time&gt;'`.&lt;/br&gt;\n            `&lt;ref_date&gt;` is in the form `yyyy-mm-dd`.&lt;/br&gt;\n            `&lt;ref_time&gt;` is in the form `hh:mm:ss`.&lt;/br&gt;\n    \"\"\"\n    day_number = np.arange(0, duration_days, time_spacing)\n    time_units = f\"days since {start_year:04d}-{start_month:02d}-{start_day:02d} {start_time}\"\n    time_arr = day_number_to_date(day_number, calendar, time_units)\n    return time_arr, day_number, time_units\n</code></pre>"},{"location":"code/run/create_files/#isca_tools.run.create_files.write_var","title":"<code>write_var(file_name, exp_dir, var_array, lat_var, lon_var, time_var=None, pressure_var=None, lat_interpolate=False, lon_interpolate=False, time_interpolate=False, pressure_interpolate=False, var_name=None, namelist_file='namelist.nml', grid_file=None)</code>","text":"<p>Creates a .nc file containing the value of <code>var_name</code> as a function of time, pressure, latitude and longitude to be used during a simulation using pressure, latitude and longitude information from the <code>t{res}_grid.nc</code> file.</p> <p>Output file will contain array of dimension <code>[n_time_out, n_pressure_out, n_lat_out, n_lon_out]</code>. With the time and pressure dimension only included if <code>time_var</code> and <code>pressure_var</code> are specified.</p> Var saved in Isca output data <p>Note that Isca does interpolation on these files, e.g. if provide <code>var</code> at day 0 and day 1, the value of <code>var</code> output by Isca will be given for time=0.5 days, and will be an average between day 0 and day 1, because the day=0.5 value is an average of all time steps between day 0 and day 1.</p> Pressure values <p>The default <code>grid_file</code> will only have 2 pressure values. To change this, a new <code>grid_file</code> should be created using <code>create_grid_file</code> with the <code>example_exp_file</code> specified to a output <code>.nc</code> file with the desired pressure information.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>.nc file containing the value of <code>var_name</code> as a function of time, pressure, latitude and longitude will be saved with this name in the folder given by <code>exp_dir</code>.</p> required <code>exp_dir</code> <code>str</code> <p>Path to directory of the experiment.</p> required <code>var_array</code> <code>ndarray</code> <p><code>float [n_time_in x n_pressure_in x n_lat_in x n_lon_in]</code>. <code>var_array[i, j, k, n]</code> is the value of <code>var_name</code> at <code>time_var[i]</code>, <code>pressure_var[j]</code>, <code>lat_var[k]</code>, <code>lon_var[n]</code>.Do not have to give <code>time_var</code> or <code>pressure_var</code> though, in which case a 2 or 3-dimensional array is expected i.e. if neither provided then <code>var_array[k, n]</code> is value of <code>var_name</code> at <code>lat_var[k]</code>, <code>lon_var[n]</code>.</p> required <code>lat_var</code> <code>ndarray</code> <p><code>float [n_lat_in]</code>. Latitudes in degrees, that provided variable info for in <code>var_array</code>.</p> required <code>lon_var</code> <code>ndarray</code> <p><code>float [n_lon_in]</code>. Longitudes in degrees (from 0 to 360), that provided variable info for in <code>var_array</code>.</p> required <code>time_var</code> <code>Optional[ndarray]</code> <p><code>int [n_time_in]</code>. Time in days, that provided variable info for in <code>var_array</code>. First day would be 0, second day 1...</p> <code>None</code> <code>pressure_var</code> <code>Optional[ndarray]</code> <p><code>float [n_pressure_in]</code>. Pressure in hPa, that provided variable info for in <code>var_array</code>.</p> <code>None</code> <code>lat_interpolate</code> <code>bool</code> <p>Output file will have <code>var</code> defined at <code>lat_out</code>, with <code>n_lat_out</code> latitudes, specified by resolution indicated in <code>experiment_details</code> section of <code>namelist_file</code> within <code>exp_dir</code>. If <code>False</code>, will require that <code>lat_var</code> contains all these <code>lat_out</code>. Otherwise, will set value of <code>var</code> at <code>lat_out</code> in output file to nearest latitude in <code>lat_var</code>.</p> <code>False</code> <code>lon_interpolate</code> <code>bool</code> <p>Output file will have <code>var</code> defined at <code>lon_out</code>, with <code>n_lon_out</code> longitudes, specified by resolution indicated in <code>experiment_details</code> section of <code>namelist_file</code> within <code>exp_dir</code>. If <code>False</code>, will require that <code>lon_var</code> contains all these <code>lon_out</code>. Otherwise, will set value of <code>var</code> at <code>lon_out</code> in output file to nearest longitude in <code>lon_var</code>.</p> <code>False</code> <code>time_interpolate</code> <code>Union[bool, str]</code> <p>Output file will have <code>var</code> defined at <code>time_out</code>, with <code>n_time_out</code> days, specified by <code>n_months_total</code> indicated in <code>experiment_details</code> section of <code>namelist_file</code> within <code>exp_dir</code>. If <code>False</code>, will require that <code>time_var</code> contains all these <code>time_out</code>. If <code>True</code>, will set value of <code>var</code> at <code>time_out</code> in output file to nearest time in <code>time_var</code>. If <code>wrap</code>, similar to <code>True</code>, but will assume <code>var</code> is periodic with period of <code>time_var[-1]+1</code>. E.g. if provide <code>time_var=np.arange(360)</code>, so give each value for a year. Then the output value of <code>var</code> for <code>time_out=360</code> will be set to <code>var_array</code> at <code>time_var=360%(359+1)=0</code>.</p> <code>False</code> <code>pressure_interpolate</code> <code>bool</code> <p>Output file will have <code>var</code> defined at <code>pressure_out</code>, with <code>n_pressure_out</code> pressures, specified by the <code>'_exp.nc'</code> files in the <code>grid_files</code> folder (typically only 2 values). If <code>False</code>, will require that <code>pressure_var</code> contains all these <code>pressure_out</code>. Otherwise, will set value of <code>var</code> at <code>pressure_out</code> in output file to nearest pressure in <code>pressure_var</code>. May need to create new <code>grid_file</code> if need additional pressure values in output file.</p> <code>False</code> <code>var_name</code> <code>Optional[str]</code> <p>Name of variable that file is being created for e.g. <code>'co2'</code>. If not provided, will set to <code>file_name</code> (without the <code>.nc</code> extension).</p> <code>None</code> <code>namelist_file</code> <code>str</code> <p>Name of namelist <code>nml</code> file for the experiment, within the <code>exp_dir</code>. This specifies the physical parameters used for the simulation.</p> <code>'namelist.nml'</code> <code>grid_file</code> <code>Optional[str]</code> <p>Path to file with desired coordinate information for this experiment. Can be created with <code>create_grid_file</code> function. If not provided, will use default grid file corresponding to the resolution specified in <code>namelist_file</code>.</p> <code>None</code> Source code in <code>isca_tools/run/create_files.py</code> <pre><code>def write_var(file_name: str, exp_dir: str,\n              var_array: np.ndarray,\n              lat_var: np.ndarray, lon_var: np.ndarray,\n              time_var: Optional[np.ndarray] = None, pressure_var: Optional[np.ndarray] = None,\n              lat_interpolate: bool = False, lon_interpolate: bool = False,\n              time_interpolate: Union[bool, str] = False, pressure_interpolate: bool = False,\n              var_name: Optional[str] = None, namelist_file: str='namelist.nml',\n              grid_file: Optional[str] = None):\n    \"\"\"\n    Creates a *.nc* file containing the value of `var_name` as a function of time, pressure, latitude and longitude to\n    be used during a simulation using pressure, latitude and longitude information from the `t{res}_grid.nc` file.\n\n    Output file will contain array of dimension `[n_time_out, n_pressure_out, n_lat_out, n_lon_out]`. With the\n    time and pressure dimension only included if `time_var` and `pressure_var` are specified.\n\n    ??? note \"Var saved in Isca output data\"\n        Note that Isca does interpolation on these files, e.g. if provide `var` at day 0 and day 1, the value of `var`\n        output by Isca will be given for time=0.5 days, and will be an average between day 0 and day 1, because\n        the day=0.5 value is an average of all time steps between day 0 and day 1.\n\n    ??? note \"Pressure values\"\n        The default `grid_file` will only have 2 pressure values.\n        To change this, a new `grid_file` should be created using `create_grid_file` with the\n        `example_exp_file` specified to a output `.nc` file with the desired pressure information.\n\n    Args:\n        file_name: *.nc* file containing the value of `var_name` as a function of time, pressure,\n            latitude and longitude will be saved with this name in the folder given by `exp_dir`.\n        exp_dir: Path to directory of the experiment.\n        var_array: `float [n_time_in x n_pressure_in x n_lat_in x n_lon_in]`.&lt;/br&gt;\n            `var_array[i, j, k, n]` is the value of `var_name` at `time_var[i]`, `pressure_var[j]`, `lat_var[k]`,\n            `lon_var[n]`.&lt;/br&gt;Do not have to give `time_var` or `pressure_var` though,\n            in which case a 2 or 3-dimensional array is expected i.e. if neither provided then `var_array[k, n]`\n            is value of `var_name` at `lat_var[k]`, `lon_var[n]`.\n        lat_var: `float [n_lat_in]`.&lt;/br&gt;\n            Latitudes in degrees, that provided variable info for in `var_array`.\n        lon_var: `float [n_lon_in]`.&lt;/br&gt;\n            Longitudes in degrees (from 0 to 360), that provided variable info for in `var_array`.\n        time_var: `int [n_time_in]`.&lt;/br&gt;\n            Time in days, that provided variable info for in `var_array`. First day would be 0, second day 1...\n        pressure_var: `float [n_pressure_in]`.&lt;/br&gt;\n            Pressure in hPa, that provided variable info for in `var_array`.\n        lat_interpolate: Output file will have `var` defined at `lat_out`, with `n_lat_out` latitudes, specified by\n            resolution indicated in `experiment_details` section of `namelist_file` within `exp_dir`.&lt;/br&gt;\n            If `False`, will require that `lat_var` contains all these `lat_out`. Otherwise, will set value of `var`\n            at `lat_out` in output file to nearest latitude in `lat_var`.\n        lon_interpolate: Output file will have `var` defined at `lon_out`, with `n_lon_out` longitudes, specified by\n            resolution indicated in `experiment_details` section of `namelist_file` within `exp_dir`.&lt;/br&gt;\n            If `False`, will require that `lon_var` contains all these `lon_out`. Otherwise, will set value of `var`\n            at `lon_out` in output file to nearest longitude in `lon_var`.\n        time_interpolate: Output file will have `var` defined at `time_out`, with `n_time_out` days, specified by\n            `n_months_total` indicated in `experiment_details` section of `namelist_file` within `exp_dir`.&lt;/br&gt;\n            If `False`, will require that `time_var` contains all these `time_out`. If `True`, will set value of `var`\n            at `time_out` in output file to nearest time in `time_var`.&lt;/br&gt;\n            If `wrap`, similar to `True`, but will assume `var` is periodic with period of `time_var[-1]+1`.\n            E.g. if provide `time_var=np.arange(360)`, so give each value for a year.\n            Then the output value of `var` for `time_out=360` will be set to `var_array` at `time_var=360%(359+1)=0`.\n        pressure_interpolate: Output file will have `var` defined at `pressure_out`, with `n_pressure_out` pressures,\n            specified by the `'_exp.nc'` files in the `grid_files` folder (typically only 2 values).&lt;/br&gt;\n            If `False`, will require that `pressure_var` contains all these `pressure_out`.\n            Otherwise, will set value of `var` at `pressure_out` in output file to nearest pressure in `pressure_var`.&lt;/br&gt;\n            May need to create new `grid_file` if need additional pressure values in output file.\n        var_name: Name of variable that file is being created for e.g. `'co2'`.&lt;/br&gt;\n            If not provided, will set to `file_name` (without the `.nc` extension).\n        namelist_file: Name of namelist `nml` file for the experiment, within the `exp_dir`.\n            This specifies the physical parameters used for the simulation.\n        grid_file: Path to file with desired coordinate information for this experiment.\n            Can be created with `create_grid_file` function.&lt;/br&gt;\n            If not provided, will use default grid file corresponding to the resolution specified in `namelist_file`.\n\n    \"\"\"\n    if var_name is None:\n        var_name = file_name.replace('.nc', '')\n    namelist = load_namelist(namelist_file=os.path.join(exp_dir, namelist_file))\n    res = int(namelist['experiment_details']['resolution'][1:])     # resolution for experiment read in\n\n    # Create copy of base file for this resolution and save to file_name\n    if grid_file is None:\n        grid_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'grid_files')\n        grid_file = os.path.join(grid_dir, f\"t{res}_grid.nc\")\n        if not os.path.exists(grid_file):\n            # Create base nc file with all the dimensions if it does not exist.\n            create_grid_file(res=res)\n    dataset_base = xr.open_dataset(grid_file)\n\n    var_dims = ('lat','lon',)\n\n    # Find which latitudes in var_val_array to output onto grid file with specific resolution\n    input_lat_inds_for_out = np.argmin(np.abs(dataset_base.lat.to_numpy() - lat_var[:, np.newaxis]), axis=0)\n    interpolated_lat_ind = np.where([np.round(dataset_base.lat.to_numpy()[i], 0)\n                                     not in np.round(lat_var, 0) for i in range(dataset_base.lat.size)])[0]\n    if len(interpolated_lat_ind)&gt;0:\n        if not lat_interpolate:\n            raise ValueError(f\"Simulation in {exp_dir}\\nis for resolution=T{res}, but lat_var does not include\"\n                             f\" {np.round(dataset_base.lat.to_numpy()[interpolated_lat_ind], 0)}.\\n\"\n                             f\"Closest is {np.round(lat_var[input_lat_inds_for_out[interpolated_lat_ind]], 0)}.\\n\"\n                             f\"May need to create new grid file with different resolution using create_grid_file function,\"\n                             f\"or use lat_interpolate=True.\")\n        if lat_interpolate:\n            warnings.warn(f'\\nValues in var_array for lat={np.round(lat_var[input_lat_inds_for_out[interpolated_lat_ind]], 0)}\\n'\n                f'output onto lat={np.round(dataset_base.lat.to_numpy()[interpolated_lat_ind], 0)} in output file which has resolution=T{res}.')\n\n    # Find which longitudes in var_val_array to output onto grid file with specific resolution\n    input_lon_inds_for_out = np.argmin(np.abs(dataset_base.lon.to_numpy() - lon_var[:, np.newaxis]), axis=0)\n    interpolated_lon_ind = np.where([np.round(dataset_base.lon.to_numpy()[i], 0)\n                                     not in np.round(lon_var, 0) for i in range(dataset_base.lon.size)])[0]\n    if len(interpolated_lon_ind)&gt;0:\n        if not lon_interpolate:\n            raise ValueError(f\"Simulation in {exp_dir}\\nis for resolution=T{res}, but lon_var does not include\"\n                             f\" {np.round(dataset_base.lon.to_numpy()[interpolated_lon_ind], 0)}.\\n\"\n                             f\"Closest is {np.round(lon_var[input_lon_inds_for_out[interpolated_lon_ind]], 0)}.\\n\"\n                             f\"May need to create new grid file with different resolution using create_grid_file function,\"\n                             f\"or use lon_interpolate=True.\")\n        if lat_interpolate:\n            warnings.warn(f'\\nValues in var_array for lon={np.round(lon_var[input_lon_inds_for_out[interpolated_lon_ind]], 0)}\\n'\n                f'output onto lon={np.round(dataset_base.lon.to_numpy()[interpolated_lon_ind], 0)} in output file which has resolution=T{res}.')\n\n    # Find which pressure values in var_val_array to output onto grid file with specific resolution\n    if pressure_var is None:\n        dataset_base = dataset_base.drop_dims(['pfull', 'phalf'])       # get rid of pressure dimension\n    else:\n        var_dims = ('pfull', ) + var_dims\n        input_pressure_inds_for_out = np.argmin(np.abs(dataset_base.pfull.to_numpy() -\n                                                       pressure_var[:, np.newaxis]), axis=0)\n        interpolated_pressure_ind = np.where([np.round(dataset_base.pfull.to_numpy()[i], 0)\n                                         not in np.round(pressure_var, 0) for i in range(dataset_base.pfull.size)])[0]\n        if len(interpolated_pressure_ind)&gt;0:\n            if not pressure_interpolate:\n                raise ValueError(f\"Using resolution file={grid_file}\\nwhich contains pfull=\"\n                                 f\"{np.round(dataset_base.pfull.to_numpy()[interpolated_pressure_ind], 0)}hPa.\"\n                                 f\"\\nThese are not provided in pressure_var. Closest is \"\n                                 f\"{np.round(pressure_var[input_pressure_inds_for_out[interpolated_pressure_ind]], 0)}hPa\\n\"\n                                 f\"May need to create new grid file with different pressure levels using create_grid_file function,\"\n                                 f\"or use pressure_interpolate=True.\")\n            else:\n                warnings.warn(f'\\nValues in var_array for pressure='\n                              f'{np.round(pressure_var[input_pressure_inds_for_out[interpolated_pressure_ind]], 0)}hPa\\n'\n                              f'output onto pfull={np.round(dataset_base.pfull.to_numpy()[interpolated_pressure_ind], 0)}hPa'\n                              f' in output file.')\n    # make sure output file has .nc suffix\n    file_name = file_name.replace('.nc', '')\n    file_name = file_name + '.nc'\n    file_name = os.path.join(exp_dir, file_name)\n    if os.path.exists(file_name):\n        raise ValueError(f\"The file {file_name} already exists. Delete or re-name this to continue.\")\n    dataset_base.to_netcdf(file_name)\n\n    # Load copy of base file in append mode so can add\n    out_file = Dataset(file_name, 'a', format='NETCDF3_CLASSIC')\n\n    if time_var is not None:\n        if time_var.size != var_array.shape[0]:\n            raise ValueError(f'time_var has {time_var.size} dimensions, but first dimension of var_array has '\n                             f'{var_array.shape[0]} elements')\n        if time_var.dtype != int:\n            raise ValueError(f\"time_var={time_var} must be an integer type. 0 refers to first day.\")\n        var_dims = ('time',) + var_dims\n        # Load in namelist file to get details about the calendar used for the experiment\n        calendar = namelist['main_nml']['calendar']\n        if calendar.lower() == 'thirty_day':\n            calendar = '360_day'\n        if calendar.lower() == 'no_calendar':\n            raise ValueError(f\"Calendar for this experiment is {calendar}.\\n\"\n                             f\"Not sure what calendar to pass to create_time_arr function.\")\n        if 'current_date' not in namelist['main_nml']:\n            # default start date is 0 year, first month, first day I THINK - NOT SURE.\n            current_date = [0, 1, 1, 0, 0, 0]\n        else:\n            current_date = namelist['main_nml']['current_date']\n\n        # Add time as a dimension and variable\n        out_file.createDimension('time', 0)  # Key point is to have the length of the time axis 0, or 'unlimited'.\n                                             # This seems necessary to get the code to run properly.\n        # Time calendar details\n        start_time = f'{current_date[3]:02d}:{current_date[4]:02d}:{current_date[5]:02d}'\n        duration_days = namelist['experiment_details']['n_months_total'] * namelist['main_nml']['days']\n        # last value in day_number must be more than last day in simulation so can interpolate variable value on all days.\n        day_number = np.arange(0, duration_days+1)\n        time_units = f\"days since {current_date[0]:04d}-{current_date[1]:02d}-{current_date[2]:02d} {start_time}\"\n\n        times = out_file.createVariable('time', 'd', ('time',))\n        times.units = time_units\n        if calendar == '360_day':\n            calendar = 'thirty_day_months'\n        times.calendar = calendar.upper()\n        times.calendar_type = calendar.upper()\n        times.cartesian_axis = 'T'\n        times[:] = day_number\n\n        # What indices of input variable do we use for output variable - i.e. for each time in output, interpolate\n        # to find corresponding nearest time in input\n        if time_interpolate == 'wrap':\n            time_period = time_var[-1]+1\n            input_time_inds_for_out = np.argmin(np.abs(day_number % time_period - time_var[:, np.newaxis]), axis=0)\n            interpolated_time_ind = np.where([day_number[i]%time_period not in time_var for i in range(day_number.size)])[0]\n        else:\n            input_time_inds_for_out = np.argmin(np.abs(day_number - time_var[:, np.newaxis]), axis=0)\n            interpolated_time_ind = np.where([day_number[i] not in time_var for i in range(day_number.size)])[0]\n        if len(interpolated_time_ind) &gt; 0:\n            if not time_interpolate:\n                raise ValueError(f\"Simulation in {exp_dir}\\nis for {duration_days}, but time_var does not include\"\n                                 f\"days {day_number[interpolated_time_ind]}.\\n\"\n                                 f\"Closest is {time_var[input_time_inds_for_out[interpolated_time_ind]]}).\\n\"\n                                 f\"Need to provide value in var_val_array for each day of simulation, \"\n                                 f\"or use time_interpolate=True.\")\n            else:\n                warnings.warn(f'\\nNot all times provided in time_var, had to do some interpolation\\nValues in '\n                              f'var_array for days={time_var[input_time_inds_for_out[interpolated_time_ind]]}\\n'\n                              f'output onto days={day_number[interpolated_time_ind]} in output file.')\n\n\n    # Add variable info to file - allow to vary in 4 dimensions.\n    var_out = out_file.createVariable(var_name, 'f4', var_dims)\n    # Output variable\n    if ('time' in var_dims) and ('pfull' in var_dims):\n        var_out[:] = var_array[np.ix_(input_time_inds_for_out, input_pressure_inds_for_out,\n                                      input_lat_inds_for_out, input_lon_inds_for_out)]\n    elif 'time' in var_dims:\n        var_out[:] = var_array[np.ix_(input_time_inds_for_out, input_lat_inds_for_out,\n                                      input_lon_inds_for_out)]\n    elif 'pfull' in var_dims:\n        var_out[:] = var_array[np.ix_(input_pressure_inds_for_out, input_lat_inds_for_out,\n                                      input_lon_inds_for_out)]\n    else:\n        var_out[:] = var_array[np.ix_(input_lat_inds_for_out, input_lon_inds_for_out)]\n    out_file.close()\n    print('Output written to: ' + file_name)\n</code></pre>"},{"location":"code/thesis/adiabat_theory/","title":"Adiabat Theory","text":""},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.decompose_temp_adiabat_anomaly","title":"<code>decompose_temp_adiabat_anomaly(temp_surf_mean, temp_surf_quant, sphum_mean, sphum_quant, temp_ft_mean, temp_ft_quant, pressure_surf, pressure_ft)</code>","text":"<p>The theory for \\(\\delta T(x)\\) involves the adiabatic temperature anomaly, \\(\\Delta T_A\\). This can be decomposed into more physically meaningful quantities:</p> \\[\\Delta T_A(x) = T_A(x) - \\overline{T_A} = \\overline{T_{CE}} - T_{CE}(x) + \\Delta T_{FT}(x)\\] <p>where:</p> <ul> <li>\\(\\overline{T_{CE}} = \\overline{T_{FT}} - \\overline{T_A}\\) represents the deviation of the mean free tropospheric temperature from the adiabatic temperature. If at convective equilibrium, this would be zero. If the mean day had CAPE, this would be negative, as the lapse rate would be steeper than that expected by convection.</li> <li>\\(T_{CE}(x) = T_{FT}(x) - T_A(x)\\) represents the deviation of the free tropospheric temperature from the adiabatic temperature conditioned on percentile \\(x\\) of near-surface temperature.</li> <li>\\(\\Delta T_{FT}(x) = T_{FT}(x) - \\overline{T_{FT}}\\) represents the gradient of the free tropospheric temperature. Near the tropics, we expect a weak temperature gradient (WTG) so this term would be small.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>temp_surf_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average near surface temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K.</p> required <code>temp_surf_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_surf_quant[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface temperature of experiment <code>i</code>. Units: K. Note that <code>quant_use</code> is not provided as not needed by this function, but is likely to be <code>np.arange(1, 100)</code> - leave out <code>x=0</code> as doesn't really make sense to consider \\(0^{th}\\) percentile of a quantity.</p> required <code>sphum_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average near surface specific humidity of each simulation. Units: kg/kg.</p> required <code>sphum_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant[i, j]</code> is near-surface specific humidity, averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: kg/kg.</p> required <code>temp_ft_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average temperature at <code>pressure_ft</code> in Kelvin.</p> required <code>temp_ft_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_ft_quant[i, j]</code> is temperature at <code>pressure_ft</code>, averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: kg/kg.</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level in Pa.</p> required <p>Returns:</p> Name Type Description <code>temp_adiabat_anom</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> Adiabatic temperature anomaly at <code>pressure_ft</code>, \\(\\Delta T_A(x) = T_A(x) - \\overline{T_A}\\).</p> <code>temp_ce_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Deviation of mean temperature at <code>pressure_ft</code> from mean adiabatic temperature: \\(\\overline{T_{CE}} = \\overline{T_{FT}} - \\overline{T_A}\\).</p> <code>temp_ce_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> Deviation of temperature at <code>pressure_ft</code> from adiabatic temperature: \\(T_{CE}(x) = T_{FT}(x) - T_A(x)\\). Conditioned on percentile of near-surface temperature.</p> <code>temp_ft_anom</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> Temperature anomaly at <code>pressure_ft</code>, \\(\\Delta T_{FT}(x) = T_{FT}(x) - \\overline{T_{FT}}\\).</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def decompose_temp_adiabat_anomaly(temp_surf_mean: np.ndarray, temp_surf_quant: np.ndarray, sphum_mean: np.ndarray,\n                                   sphum_quant: np.ndarray, temp_ft_mean: np.ndarray, temp_ft_quant: np.ndarray,\n                                   pressure_surf: float, pressure_ft: float\n                                   ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    The theory for $\\delta T(x)$ involves the adiabatic temperature anomaly, $\\Delta T_A$. This can be decomposed\n    into more physically meaningful quantities:\n\n    $$\\Delta T_A(x) = T_A(x) - \\overline{T_A} = \\overline{T_{CE}} - T_{CE}(x) + \\Delta T_{FT}(x)$$\n\n    where:\n\n    * $\\overline{T_{CE}} = \\overline{T_{FT}} - \\overline{T_A}$ represents the deviation of the mean free tropospheric\n    temperature from the adiabatic temperature. If at convective equilibrium, this would be zero. If the mean day had\n    CAPE, this would be negative, as the lapse rate would be steeper than that expected by convection.\n    * $T_{CE}(x) = T_{FT}(x) - T_A(x)$ represents the deviation of the free tropospheric\n    temperature from the adiabatic temperature conditioned on percentile $x$ of near-surface temperature.\n    * $\\Delta T_{FT}(x) = T_{FT}(x) - \\overline{T_{FT}}$ represents the gradient of the free tropospheric temperature.\n    Near the tropics, we expect a weak temperature gradient (WTG) so this term would be small.\n\n    Args:\n        temp_surf_mean: `float [n_exp]`&lt;/br&gt;\n            Average near surface temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*.\n        temp_surf_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_surf_quant[i, j]` is the percentile `quant_use[j]` of near surface temperature of\n            experiment `i`. Units: *K*.&lt;/br&gt;\n            Note that `quant_use` is not provided as not needed by this function, but is likely to be\n            `np.arange(1, 100)` - leave out `x=0` as doesn't really make sense to consider $0^{th}$ percentile\n            of a quantity.\n        sphum_mean: `float [n_exp]`&lt;/br&gt;\n            Average near surface specific humidity of each simulation. Units: *kg/kg*.\n        sphum_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant[i, j]` is near-surface specific humidity, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *kg/kg*.\n        temp_ft_mean: `float [n_exp]`&lt;/br&gt;\n            Average temperature at `pressure_ft` in Kelvin.\n        temp_ft_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_ft_quant[i, j]` is temperature at `pressure_ft`, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *kg/kg*.\n        pressure_surf:\n            Pressure at near-surface in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level in *Pa*.\n\n    Returns:\n        temp_adiabat_anom: `float [n_exp, n_quant]`&lt;/br&gt;\n            Adiabatic temperature anomaly at `pressure_ft`, $\\Delta T_A(x) = T_A(x) - \\overline{T_A}$.\n        temp_ce_mean: `float [n_exp]`&lt;/br&gt;\n            Deviation of mean temperature at `pressure_ft` from mean adiabatic temperature:\n            $\\overline{T_{CE}} = \\overline{T_{FT}} - \\overline{T_A}$.\n        temp_ce_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            Deviation of temperature at `pressure_ft` from adiabatic temperature: $T_{CE}(x) = T_{FT}(x) - T_A(x)$.\n            Conditioned on percentile of near-surface temperature.\n        temp_ft_anom: `float [n_exp, n_quant]`&lt;/br&gt;\n            Temperature anomaly at `pressure_ft`, $\\Delta T_{FT}(x) = T_{FT}(x) - \\overline{T_{FT}}$.\n\n    \"\"\"\n    n_exp, n_quant = temp_surf_quant.shape\n    temp_adiabat_mean = np.zeros_like(temp_surf_mean)\n    temp_adiabat_quant = np.zeros_like(temp_surf_quant)\n    for i in range(n_exp):\n        temp_adiabat_mean[i] = get_temp_adiabat(temp_surf_mean[i], sphum_mean[i], pressure_surf, pressure_ft)\n        for j in range(n_quant):\n            temp_adiabat_quant[i, j] = get_temp_adiabat(temp_surf_quant[i, j], sphum_quant[i, j], pressure_surf,\n                                                        pressure_ft)\n    temp_adiabat_anom = temp_adiabat_quant - temp_adiabat_mean[:, np.newaxis]\n    temp_ce_quant = temp_ft_quant - temp_adiabat_quant\n    temp_ce_mean = temp_ft_mean - temp_adiabat_mean\n    temp_ft_anom = temp_ft_quant - temp_ft_mean[:, np.newaxis]\n    return temp_adiabat_anom, temp_ce_mean, temp_ce_quant, temp_ft_anom\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.decompose_temp_ft_anom_change","title":"<code>decompose_temp_ft_anom_change(temp_ft_av, temp_ft_x, temp_ft_p, quant_p=np.arange(100, dtype=int), simple=True)</code>","text":"<p>We can decompose the change in free tropospheric temperature anomaly, conditioned on near-surface temperature percentile \\(x\\) into the change in the corresponding free tropospheric temperature percentile \\(p_x\\), byt accounting for how \\(p_x\\) changes with warming:</p> <p>\\(\\delta \\Delta T_{FT}(x) \\approx \\delta \\Delta T_{FT}[p_x] + \\overline{\\eta}\\delta \\Delta p_x + \\Delta \\eta(p_x) \\delta \\overline{p} + \\Delta \\eta(p_x)\\delta \\Delta p_x + \\Delta (\\delta \\eta(p_x) \\delta p_x)\\)</p> <p>where:</p> <ul> <li>\\(p_x\\) is defined such that \\(T_{FT}(x) = T_{FT}[p_x]\\) and \\(\\overline{p}\\) such that \\(\\overline{T_{FT}} = T_{FT}[\\overline{p}]\\).</li> <li>\\(\\eta(p_x) = \\frac{\\partial T_{FT}}{\\partial p}\\bigg|_{p_x}\\); \\(\\overline{\\eta} = \\frac{\\partial T_{FT}}{\\partial p}\\bigg|_{\\overline{p}}\\) and \\(\\Delta \\eta(p_x) = \\eta(p_x) - \\overline{\\eta}\\).</li> <li>\\(\\delta \\Delta p_x = \\delta (p_x - \\overline{p})\\)</li> <li>\\(\\delta \\Delta T_{FT}[p_x] = \\delta (T_{FT}[p_x] - T_{FT}[\\overline{p}])\\) keeping \\(p_x\\) and \\(\\overline{p}\\) constant.</li> <li>\\(\\Delta (\\delta \\eta(p_x) \\delta p_x) = \\delta \\eta(p_x) \\delta p_x - \\delta \\overline{\\eta}\\delta \\overline{p}\\)</li> </ul> <p>The only approximation in the above is saying that \\(\\eta(p)\\) is constant between \\(p=p_x\\) and \\(p=p_x+\\delta p_x\\). Keeping only the first two terms on the RHS, also provides a good approximation. This is achieved by setting <code>simple=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>temp_ft_av</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average free tropospheric temperature for each experiment, likely to be \\(T_{FT}(x=50)\\).</p> required <code>temp_ft_x</code> <code>ndarray</code> <p><code>float [n_exp, n_quant_x]</code> Free tropospheric temperature conditioned on near-surface temperature percentile, \\(x\\), for each experiment: \\(T_{FT}(x)\\). \\(x\\) can differ from <code>quant_px</code>, but likely to be the same: <code>np.arange(100)</code>.</p> required <code>temp_ft_p</code> <code>ndarray</code> <p><code>float [n_exp, n_quant_p]</code> <code>temp_ft_p[i, j]</code> is the \\(p=\\)<code>quant_p[j]</code>\\(^{th}\\) percentile of free tropospheric temperature for experiment <code>i</code>: \\(T_{FT}[p]\\).</p> required <code>quant_p</code> <code>ndarray</code> <p><code>float [n_quant_p]</code> Corresponding quantiles to <code>temp_ft_p</code>.</p> <code>arange(100, dtype=int)</code> <code>simple</code> <code>bool</code> <p>If <code>True</code>, <code>temp_ft_change_theory</code> will be \\(\\delta \\Delta T_{FT}[p_x] + \\overline{\\eta}\\delta \\Delta p_x\\). If <code>False</code>, will also include \\(\\Delta \\eta(p_x) \\delta \\overline{p} + \\Delta \\eta(p_x)\\Delta p_x + \\Delta (\\delta \\eta(p_x) \\delta p_x)\\).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>temp_ft_change</code> <code>ndarray</code> <p><code>float [n_quant_x]</code> Simulated \\(\\delta \\Delta T_{FT}(x)\\)</p> <code>temp_ft_change_theory</code> <code>ndarray</code> <p><code>float [n_quant_x]</code> Theoretical \\(\\delta \\Delta T_{FT}(x)\\)</p> <code>temp_ft_change_cont</code> <code>dict</code> <p>Dictionary recording the five terms in the theory for \\(\\delta \\Delta T_{FT}(x)\\). The key name indicates which variable is causing the \\(x\\) variation:</p> <ul> <li><code>ft_dist</code>: \\(\\delta \\Delta T_{FT}[p_x]\\)</li> <li><code>p_x</code>: \\(\\overline{\\eta}\\delta \\Delta p_x\\)</li> <li><code>eta0</code>: \\(\\Delta \\eta(p_x) \\delta \\overline{p}\\)</li> <li><code>eta0_p_x</code>: \\(\\Delta \\eta(p_x)\\delta \\Delta p_x\\)</li> <li><code>eta_p_x</code>: \\(\\Delta (\\delta \\eta(p_x) \\delta p_x)\\)</li> </ul> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def decompose_temp_ft_anom_change(temp_ft_av: np.ndarray, temp_ft_x: np.ndarray, temp_ft_p: np.ndarray,\n                                  quant_p: np.ndarray = np.arange(100, dtype=int), simple: bool = True\n                                  ) -&gt; Tuple[np.ndarray, np.ndarray, dict]:\n    \"\"\"\n    We can decompose the change in free tropospheric temperature anomaly, conditioned on near-surface temperature\n    percentile $x$ into the change in the corresponding free tropospheric temperature percentile $p_x$, byt accounting\n    for how $p_x$ changes with warming:\n\n    $\\delta \\Delta T_{FT}(x) \\\\approx \\delta \\Delta T_{FT}[p_x] + \\overline{\\eta}\\delta \\Delta p_x +\n    \\Delta \\eta(p_x) \\delta \\overline{p} + \\Delta \\eta(p_x)\\delta \\Delta p_x + \\Delta (\\delta \\eta(p_x) \\delta p_x)$\n\n    where:\n\n    * $p_x$ is defined such that $T_{FT}(x) = T_{FT}[p_x]$ and $\\overline{p}$ such that\n    $\\overline{T_{FT}} = T_{FT}[\\overline{p}]$.\n    * $\\eta(p_x) = \\\\frac{\\\\partial T_{FT}}{\\\\partial p}\\\\bigg|_{p_x}$;\n    $\\overline{\\eta} = \\\\frac{\\\\partial T_{FT}}{\\\\partial p}\\\\bigg|_{\\overline{p}}$ and\n    $\\Delta \\eta(p_x) = \\eta(p_x) - \\overline{\\eta}$.\n    * $\\delta \\Delta p_x = \\delta (p_x - \\overline{p})$\n    * $\\delta \\Delta T_{FT}[p_x] = \\delta (T_{FT}[p_x] - T_{FT}[\\overline{p}])$ keeping $p_x$ and $\\overline{p}$\n    constant.\n    * $\\Delta (\\delta \\eta(p_x) \\delta p_x) = \\delta \\eta(p_x) \\delta p_x - \\delta \\overline{\\eta}\\delta \\overline{p}$\n\n    The only approximation in the above is saying that $\\eta(p)$ is constant between $p=p_x$ and $p=p_x+\\delta p_x$.\n    Keeping only the first two terms on the RHS, also provides a good approximation.\n    This is achieved by setting `simple=True`.\n\n    Args:\n        temp_ft_av: `float [n_exp]`&lt;/br&gt;\n            Average free tropospheric temperature for each experiment, likely to be $T_{FT}(x=50)$.\n        temp_ft_x: `float [n_exp, n_quant_x]`&lt;/br&gt;\n            Free tropospheric temperature conditioned on near-surface temperature percentile, $x$, for each\n            experiment: $T_{FT}(x)$. $x$ can differ from `quant_px`, but likely to be the same: `np.arange(100)`.\n        temp_ft_p: `float [n_exp, n_quant_p]`&lt;/br&gt;\n            `temp_ft_p[i, j]` is the $p=$`quant_p[j]`$^{th}$ percentile of free tropospheric temperature for\n            experiment `i`: $T_{FT}[p]$.\n        quant_p: `float [n_quant_p]`&lt;/br&gt;\n            Corresponding quantiles to `temp_ft_p`.\n        simple: If `True`, `temp_ft_change_theory` will be\n            $\\delta \\Delta T_{FT}[p_x] + \\overline{\\eta}\\delta \\Delta p_x$. If `False`, will also include\n            $\\Delta \\eta(p_x) \\delta \\overline{p} + \\Delta \\eta(p_x)\\Delta p_x + \\Delta (\\delta \\eta(p_x) \\delta p_x)$.\n\n    Returns:\n        temp_ft_change: `float [n_quant_x]`&lt;/br&gt;\n            Simulated $\\delta \\Delta T_{FT}(x)$\n        temp_ft_change_theory: `float [n_quant_x]`&lt;/br&gt;\n            Theoretical $\\delta \\Delta T_{FT}(x)$\n        temp_ft_change_cont: Dictionary recording the five terms in the theory for $\\delta \\Delta T_{FT}(x)$.\n            The key name indicates which variable is causing the $x$ variation:\n\n            * `ft_dist`: $\\delta \\Delta T_{FT}[p_x]$\n            * `p_x`: $\\overline{\\eta}\\delta \\Delta p_x$\n            * `eta0`: $\\Delta \\eta(p_x) \\delta \\overline{p}$\n            * `eta0_p_x`: $\\Delta \\eta(p_x)\\delta \\Delta p_x$\n            * `eta_p_x`: $\\Delta (\\delta \\eta(p_x) \\delta p_x)$\n\n    \"\"\"\n    n_exp, n_quant_x = temp_ft_x.shape\n\n    # Get FT percentile corresponding to each FT temperature conditioned on near-surface percentile\n    p_av = np.zeros(n_exp)\n    p_av_ind = np.zeros(n_exp, dtype=int)\n    p_x = np.zeros((n_exp, n_quant_x))\n    p_x_ind = np.zeros((n_exp, n_quant_x), dtype=int)\n    for i in range(n_exp):\n        p_av[i], p_av_ind[i] = get_p_x(temp_ft_av[i], temp_ft_p[i], quant_p)\n        p_x[i], p_x_ind[i] = get_p_x(temp_ft_x[i], temp_ft_p[i], quant_p)\n\n    # Get eta on corresponding to p_x in the reference (coldest) simulation\n    eta_av0 = np.zeros(n_exp)\n    eta_px0 = np.zeros((n_exp, n_quant_x))\n    for i in range(n_exp):\n        eta_use = np.gradient(temp_ft_p[i], quant_p)\n        eta_av0[i] = eta_use[p_av_ind[0]]\n        for j in range(n_quant_x):\n            eta_px0[i, j] = eta_use[p_x_ind[0, j]]\n\n    # Isolate x dependence into different terms\n    p_x_anom = p_x - p_av[:, np.newaxis]\n    eta_px0_anom = eta_px0 - eta_av0[:, np.newaxis]\n    temp_ft_change = temp_ft_x[1] - temp_ft_x[0] - (temp_ft_av[1] - temp_ft_av[0])\n    temp_ft_change_cont = {'ft_dist': temp_ft_p[1, p_x_ind[0]] - temp_ft_p[0, p_x_ind[0]] -\n                                      (temp_ft_p[1, p_av_ind[0]] - temp_ft_p[0, p_av_ind[0]]),\n                           'p_x': eta_av0[0] * (p_x_anom[1] - p_x_anom[0]),\n                           'eta0': eta_px0_anom[0] * (p_av[1] - p_av[0]),\n                           'eta0_p_x': eta_px0_anom[0] * (p_x_anom[1] - p_x_anom[0]),\n                           'eta_p_x': (eta_px0[1] - eta_px0[0]) * (p_x[1] - p_x[0]) -\n                                      (eta_av0[1] - eta_av0[0]) * (p_av[1] - p_av[0])}\n\n    # Theory is sum of these terms\n    # not exact because we approximate an integral by taking out assuming integrand constant.\n    temp_ft_change_theory = temp_ft_change_cont['ft_dist'] + temp_ft_change_cont['p_x']\n    if not simple:\n        temp_ft_change_theory = temp_ft_change_theory + temp_ft_change_cont['eta0'] + \\\n                                temp_ft_change_cont['eta0_p_x'] + temp_ft_change_cont['eta_p_x']\n\n    return temp_ft_change, temp_ft_change_theory, temp_ft_change_cont\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.get_gamma_factors","title":"<code>get_gamma_factors(temp_surf, sphum, temp_ft, pressure_surf, pressure_ft)</code>","text":"<p>Calculates the sensitivity \\(\\gamma\\) parameters such that the theoretical scaling factor is given by:</p> \\[ \\begin{align} \\begin{split} &amp;\\left(1 + \\mu(x)\\frac{\\delta r_s(x)}{r_s(x)}\\right)\\frac{\\delta T_s(x)}{\\overline{\\delta T_s}} \\approx 1 + \\overline{\\mu} \\frac{\\delta \\overline{r_s}}{\\overline{r_s}} + \\\\ &amp;\\left[\\gamma_{T}\\frac{\\Delta T_s(x)}{\\overline{T_s}} - \\gamma_{Tr}\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\frac{\\Delta r_s(x)}{\\overline{r_s}} - \\gamma_{r} \\frac{\\Delta r_s(x)}{\\overline{r_s}}- \\gamma_{\\epsilon} \\frac{\\Delta \\epsilon(x)}{\\overline{\\beta_{s1}}\\overline{T_s}}\\right] \\left(1 + \\overline{\\mu} \\frac{\\delta \\overline{r_s}}{\\overline{r_s}}\\right) +\\\\ &amp;\\left[-\\gamma_{T\\delta r}\\frac{\\Delta T_s(x)}{\\overline{T_s}} + \\gamma_{Tr\\delta r}\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\frac{\\Delta r_s(x)}{\\overline{r_s}} + \\gamma_{r\\delta r} \\frac{\\Delta r_s(x)}{\\overline{r_s}}- \\gamma_{\\epsilon \\delta r} \\frac{\\Delta \\epsilon(x)}{\\overline{\\beta_{s1}}\\overline{T_s}}\\right] \\frac{\\delta \\overline{r_s}}{\\delta \\overline{T_s}} +\\\\ &amp;\\left[-\\gamma_{T\\delta \\epsilon}\\frac{\\Delta T_s(x)}{\\overline{T_s}} - \\gamma_{Tr\\delta \\epsilon}\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\frac{\\Delta r_s(x)}{\\overline{r_s}} - \\gamma_{r\\delta \\epsilon} \\frac{\\Delta r_s(x)}{\\overline{r_s}}+ \\gamma_{\\epsilon \\delta \\epsilon} \\frac{\\Delta \\epsilon(x)}{\\overline{\\beta_{s1}}\\overline{T_s}}\\right] \\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}} +\\\\ &amp;- \\gamma_{\\delta \\Delta r} \\frac{\\delta \\Delta r_s(x)}{\\delta \\overline{T_s}} - \\gamma_{T \\delta \\Delta r} \\frac{\\Delta T_s(x)}{\\overline{T_s}}\\frac{\\delta \\Delta r_s(x)}{\\delta \\overline{T_s}} + \\gamma_{FT} \\frac{\\delta \\Delta T_{FT}(x)}{\\delta \\overline{T_s}} +     \\gamma_{\\delta \\Delta \\epsilon} \\frac{\\delta \\Delta \\epsilon(x)}{\\delta \\overline{T_s}} \\end{split} \\end{align} \\] <p>Parameters:</p> Name Type Description Default <code>temp_surf</code> <code>float</code> <p>Temperature at <code>pressure_surf</code> in K.</p> required <code>sphum</code> <code>float</code> <p>Specific humidity at <code>pressure_surf</code> in kg/kg.</p> required <code>temp_ft</code> <code>float</code> <p>Temperature at <code>pressure_ft</code> in K. Can also use adiabatic temperature here (computed from <code>temp_surf</code> and <code>sphum</code>), if want \\(\\gamma\\) factors to be independent of free tropospheric temperature (i.e. depend on two not three quantities).</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface, \\(p_s\\) in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level, \\(p_{FT}\\) in Pa.</p> required <p>Returns:</p> Name Type Description <code>gamma</code> <code>dict</code> <p>The theory (ignoring \\(\\mu\\) factors) is a sum of 16 terms. The first four \\(\\gamma\\) factors multiply a climatological anomaly term (\\(\\Delta\\)) but not a change (\\(\\delta\\)) term, these are recorded in <code>gamma['temp_mean_change']</code> with the keys <code>t0</code>, <code>t0_r0</code>, <code>r0</code> and <code>e0</code>, They are all dimensionless.</p> <p>The next four preceed \\(\\frac{\\delta \\overline{r_s}}{\\delta \\overline{T_s}}\\) as well as a \\(\\Delta\\) term. These are recorded in <code>gamma['r_mean_change']</code>. Again they have keys <code>t0</code>, <code>t0_r0</code>, <code>r0</code> and <code>e0</code>. These all have units of \\(K\\).</p> <p>The next four preceed \\(\\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}}\\), as well as a \\(\\Delta\\) term. These are recorded in <code>gamma['e_mean_change']</code>. Again they have keys <code>t0</code>, <code>t0_r0</code>, <code>r0</code> and <code>e0</code>. These all have units of \\(K kg J^{-1}\\).</p> <p>The final four all preceed \\(\\delta \\Delta\\) quantities and are recorded in <code>gamma['anomaly_change']</code>. They have keys <code>r</code>, <code>t0_r</code>, <code>ft</code> and <code>e</code>. They have units of \\(K\\), \\(K\\), dimensionless and  \\(K kg J^{-1}\\) respectively.</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def get_gamma_factors(temp_surf: float, sphum: float, temp_ft: float, pressure_surf: float,\n                      pressure_ft: float) -&gt; dict:\n    \"\"\"\n    Calculates the sensitivity $\\gamma$ parameters such that the theoretical scaling factor is given by:\n\n    $$\n    \\\\begin{align}\n    \\\\begin{split}\n    &amp;\\\\left(1 + \\mu(x)\\\\frac{\\delta r_s(x)}{r_s(x)}\\\\right)\\\\frac{\\delta T_s(x)}{\\overline{\\delta T_s}} \\\\approx\n    1 + \\overline{\\mu} \\\\frac{\\delta \\overline{r_s}}{\\overline{r_s}} + \\\\\\\\\n    &amp;\\\\left[\\gamma_{T}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}} -\n    \\gamma_{Tr}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\\\frac{\\Delta r_s(x)}{\\overline{r_s}}\n    - \\gamma_{r} \\\\frac{\\Delta r_s(x)}{\\overline{r_s}}-\n    \\gamma_{\\epsilon} \\\\frac{\\Delta \\epsilon(x)}{\\overline{\\\\beta_{s1}}\\overline{T_s}}\\\\right]\n    \\\\left(1 + \\overline{\\mu} \\\\frac{\\delta \\overline{r_s}}{\\overline{r_s}}\\\\right) +\\\\\\\\\n    &amp;\\\\left[-\\gamma_{T\\delta r}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}} +\n    \\gamma_{Tr\\delta r}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\\\frac{\\Delta r_s(x)}{\\overline{r_s}}\n    + \\gamma_{r\\delta r} \\\\frac{\\Delta r_s(x)}{\\overline{r_s}}-\n    \\gamma_{\\epsilon \\delta r} \\\\frac{\\Delta \\epsilon(x)}{\\overline{\\\\beta_{s1}}\\overline{T_s}}\\\\right]\n    \\\\frac{\\delta \\overline{r_s}}{\\delta \\overline{T_s}} +\\\\\\\\\n    &amp;\\\\left[-\\gamma_{T\\delta \\epsilon}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}} -\n    \\gamma_{Tr\\delta \\epsilon}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\\\frac{\\Delta r_s(x)}{\\overline{r_s}}\n    - \\gamma_{r\\delta \\epsilon} \\\\frac{\\Delta r_s(x)}{\\overline{r_s}}+\n    \\gamma_{\\epsilon \\delta \\epsilon} \\\\frac{\\Delta \\epsilon(x)}{\\overline{\\\\beta_{s1}}\\overline{T_s}}\\\\right]\n    \\\\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}} +\\\\\\\\\n    &amp;- \\gamma_{\\delta \\Delta r} \\\\frac{\\delta \\Delta r_s(x)}{\\delta \\overline{T_s}} -\n    \\gamma_{T \\delta \\Delta r} \\\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\\\frac{\\delta \\Delta r_s(x)}{\\delta \\overline{T_s}}\n    + \\gamma_{FT} \\\\frac{\\delta \\Delta T_{FT}(x)}{\\delta \\overline{T_s}} + \\\n    \\gamma_{\\delta \\Delta \\epsilon} \\\\frac{\\delta \\Delta \\epsilon(x)}{\\delta \\overline{T_s}}\n    \\\\end{split}\n    \\\\end{align}\n    $$\n\n    Args:\n        temp_surf:\n            Temperature at `pressure_surf` in *K*.\n        sphum:\n            Specific humidity at `pressure_surf` in *kg/kg*.\n        temp_ft:\n            Temperature at `pressure_ft` in *K*. Can also use adiabatic temperature here (computed from `temp_surf` and\n            `sphum`), if want $\\gamma$ factors to be independent of free tropospheric temperature\n            (i.e. depend on two not three quantities).\n        pressure_surf:\n            Pressure at near-surface, $p_s$ in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level, $p_{FT}$ in *Pa*.\n\n    Returns:\n        gamma:\n            The theory (ignoring $\\mu$ factors) is a sum of 16 terms. The first four $\\gamma$ factors multiply\n            a climatological anomaly term ($\\Delta$) but not a change\n            ($\\delta$) term, these are recorded in `gamma['temp_mean_change']` with the keys\n            `t0`, `t0_r0`, `r0` and `e0`, They are all dimensionless.\n\n            The next four preceed $\\\\frac{\\delta \\overline{r_s}}{\\delta \\overline{T_s}}$ as well as a $\\Delta$ term.\n            These are recorded in `gamma['r_mean_change']`. Again they have keys `t0`, `t0_r0`, `r0` and `e0`.\n            These all have units of $K$.\n\n            The next four preceed $\\\\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}}$, as well as a $\\Delta$\n            term. These are recorded in `gamma['e_mean_change']`. Again they have keys `t0`, `t0_r0`, `r0` and `e0`.\n            These all have units of $K kg J^{-1}$.\n\n            The final four all preceed $\\delta \\Delta$ quantities and are recorded in `gamma['anomaly_change']`.\n            They have keys `r`, `t0_r`, `ft` and `e`. They have units of $K$, $K$, dimensionless and  $K kg J^{-1}$\n            respectively.\n    \"\"\"\n    # Get parameters required for prefactors in the theory\n    _, _, _, beta_ft1, beta_ft2, beta_ft3, _ = get_theory_prefactor_terms(temp_ft, pressure_surf, pressure_ft)\n    _, q_sat_surf, alpha_s, beta_s1, beta_s2, beta_s3, mu = get_theory_prefactor_terms(temp_surf, pressure_surf,\n                                                                                       pressure_ft, sphum)\n    rh = sphum / q_sat_surf\n\n    # Record coefficients of each term in equation for delta T_s(x)\n    # label is anomaly that causes variation with x.\n\n    gamma = {'t_mean_change': {}, 'r_mean_change': {}, 'e_mean_change': {}, 'anomaly_change': {}}\n    # temp_s_mean_change terms\n    key = 't_mean_change'\n    gamma_e0 = beta_ft2 * beta_s1 / beta_ft1 ** 2 * temp_surf / temp_ft\n    gamma[key]['t0'] = gamma_e0 - beta_s2 / beta_s1\n    gamma[key]['t0_r0'] = beta_s2 / beta_s1 - mu * gamma_e0\n    gamma[key]['r0'] = mu * (1 - gamma_e0 / (alpha_s * temp_surf))\n    gamma[key]['e0'] = gamma_e0\n\n    # Anomaly change terms\n    key = 'anomaly_change'\n    gamma_r_change = mu / alpha_s / rh\n    gamma_e_change = 1 / beta_s1\n    gamma[key]['r'] = gamma_r_change\n    gamma[key]['t0_r'] = gamma_r_change * alpha_s * temp_surf\n    gamma[key]['ft'] = beta_ft1 / beta_s1\n    gamma[key]['e'] = gamma_e_change\n\n    # r_s_mean change terms\n    key = 'r_mean_change'\n    gamma[key]['t0'] = gamma_r_change * gamma_e0 * (alpha_s * temp_surf / gamma_e0 - 1)\n    gamma[key]['t0_r0'] = gamma_r_change * gamma_e0 * mu\n    gamma[key]['r0'] = gamma_r_change * gamma_e0 * mu / (alpha_s * temp_surf)\n    gamma[key]['e0'] = gamma_r_change * gamma_e0\n\n    # epsilon mean change terms\n    key = 'e_mean_change'\n    gamma[key]['t0'] = gamma_e_change * gamma_e0\n    gamma[key]['t0_r0'] = gamma_e_change * gamma_e0 * mu\n    gamma[key]['r0'] = gamma_e_change * gamma_e0 * mu / (alpha_s * temp_surf)\n    gamma[key]['e0'] = gamma_e_change * gamma_e0\n    return gamma\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.get_p_x","title":"<code>get_p_x(temp, temp_ft_p, quant_p)</code>","text":"<p>Find the quantile of <code>temp</code> in the <code>temp_ft_p</code> dataset, which is defined such that <code>temp_ft_p[i]</code> is the <code>quant_p[i]</code>\\(^{th}\\) quantile.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>Union[float, ndarray]</code> <p><code>float [n_temp]</code> Temperatures to find quantile for in <code>temp_ft_p</code>.</p> required <code>temp_ft_p</code> <code>ndarray</code> <p><code>float [n_quant_p]</code> Array of temperatures defined such that <code>temp_ft_p[i]</code> is the <code>quant_p[i]</code>\\(^{th}\\) quantile.</p> required <code>quant_p</code> <code>ndarray</code> <p><code>float [n_quant_p]</code> Corresponding quantiles to <code>temp_ft_p</code>.</p> required <p>Returns:</p> Name Type Description <code>p_x</code> <code>Union[float, ndarray]</code> <p><code>float [n_temp]</code> <code>p_x[i]</code> is the quantile of <code>temp[i]</code> in <code>temp_ft_p</code>.</p> <code>p_x_ind</code> <code>Union[int, ndarray]</code> <p><code>int [n_temp]</code> <code>p_x_ind[i]</code> is the index of value in <code>quant_p</code> closest to <code>p_x[i]</code>.</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def get_p_x(temp: Union[float, np.ndarray], temp_ft_p: np.ndarray, quant_p: np.ndarray\n            ) -&gt; Tuple[Union[float, np.ndarray], Union[int, np.ndarray]]:\n    \"\"\"\n    Find the quantile of `temp` in the `temp_ft_p` dataset, which is defined such that `temp_ft_p[i]` is\n    the `quant_p[i]`$^{th}$ quantile.\n\n    Args:\n        temp: `float [n_temp]`&lt;/br&gt;\n            Temperatures to find quantile for in `temp_ft_p`.\n        temp_ft_p: `float [n_quant_p]`&lt;/br&gt;\n            Array of temperatures defined such that `temp_ft_p[i]` is the `quant_p[i]`$^{th}$ quantile.\n        quant_p: `float [n_quant_p]`&lt;/br&gt;\n            Corresponding quantiles to `temp_ft_p`.\n\n    Returns:\n        p_x: `float [n_temp]`&lt;/br&gt;\n            `p_x[i]` is the quantile of `temp[i]` in `temp_ft_p`.\n        p_x_ind: `int [n_temp]`&lt;/br&gt;\n            `p_x_ind[i]` is the index of value in `quant_p` closest to `p_x[i]`.\n    \"\"\"\n    interp_func = scipy.interpolate.interp1d(temp_ft_p, quant_p, bounds_error=True)\n    p_x = interp_func(temp)\n    if isinstance(temp, float):\n        return float(p_x), np.abs(quant_p - p_x).argmin()\n    else:\n        return p_x, np.asarray([np.abs(quant_p - p_x[i]).argmin() for i in range(p_x.size)])\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.get_scaling_factor_theory","title":"<code>get_scaling_factor_theory(temp_surf_mean, temp_surf_quant, sphum_mean, sphum_quant, pressure_surf, pressure_ft, temp_ft_mean, temp_ft_quant, z_ft_mean, z_ft_quant, non_linear=False, z_form=False, use_temp_adiabat=False, strict_conv_eqb=False, simple=False)</code>","text":"<p>Calculates the theoretical near-surface temperature change for percentile \\(x\\), \\(\\delta T_s(x)\\), relative to the mean temperature change, \\(\\overline{\\delta T_s}\\). In the most complicated case, with <code>non_linear = True</code>, this is:</p> \\[ \\begin{align} \\begin{split} &amp;\\left(1 + \\mu(x)\\frac{\\delta r_s(x)}{r_s(x)}\\right)\\frac{\\delta T_s(x)}{\\overline{\\delta T_s}} \\approx 1 + \\overline{\\mu} \\frac{\\delta \\overline{r_s}}{\\overline{r_s}} + \\\\ &amp;\\left[\\gamma_{T}\\frac{\\Delta T_s(x)}{\\overline{T_s}} - \\gamma_{Tr}\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\frac{\\Delta r_s(x)}{\\overline{r_s}} - \\gamma_{r} \\frac{\\Delta r_s(x)}{\\overline{r_s}}- \\gamma_{\\epsilon} \\frac{\\Delta \\epsilon(x)}{\\overline{\\beta_{s1}}\\overline{T_s}}\\right] \\left(1 + \\overline{\\mu} \\frac{\\delta \\overline{r_s}}{\\overline{r_s}}\\right) +\\\\ &amp;\\left[-\\gamma_{T\\delta r}\\frac{\\Delta T_s(x)}{\\overline{T_s}} + \\gamma_{Tr\\delta r}\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\frac{\\Delta r_s(x)}{\\overline{r_s}} + \\gamma_{r\\delta r} \\frac{\\Delta r_s(x)}{\\overline{r_s}}- \\gamma_{\\epsilon \\delta r} \\frac{\\Delta \\epsilon(x)}{\\overline{\\beta_{s1}}\\overline{T_s}}\\right] \\frac{\\delta \\overline{r_s}}{\\delta \\overline{T_s}} +\\\\ &amp;\\left[-\\gamma_{T\\delta \\epsilon}\\frac{\\Delta T_s(x)}{\\overline{T_s}} - \\gamma_{Tr\\delta \\epsilon}\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\frac{\\Delta r_s(x)}{\\overline{r_s}} - \\gamma_{r\\delta \\epsilon} \\frac{\\Delta r_s(x)}{\\overline{r_s}}+ \\gamma_{\\epsilon \\delta \\epsilon} \\frac{\\Delta \\epsilon(x)}{\\overline{\\beta_{s1}}\\overline{T_s}}\\right] \\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}} +\\\\ &amp;- \\gamma_{\\delta \\Delta r} \\frac{\\delta \\Delta r_s(x)}{\\delta \\overline{T_s}} - \\gamma_{T \\delta \\Delta r} \\frac{\\Delta T_s(x)}{\\overline{T_s}}\\frac{\\delta \\Delta r_s(x)}{\\delta \\overline{T_s}} + \\gamma_{FT} \\frac{\\delta \\Delta T_{FT}(x)}{\\delta \\overline{T_s}} +     \\gamma_{\\delta \\Delta \\epsilon} \\frac{\\delta \\Delta \\epsilon(x)}{\\delta \\overline{T_s}} \\end{split} \\end{align} \\] <p>If <code>z_form = True</code>, then all \\(\\gamma\\) and \\(\\mu\\) parameters are multiplied by \\(\\frac{\\overline{\\beta_{s1}}}{\\overline{\\beta_{s1}} + \\beta_{FT1}}\\), and \\(\\delta \\Delta T_{FT}(x)\\) is replaced with \\(\\frac{g}{R^{\\dagger}}\\delta \\Delta z_{FT}(x)\\).</p> <p>If <code>non_linear = False</code>, then \\(\\overline{\\mu}\\) and \\(\\mu(x)\\) are set to zero.</p> <p>If <code>use_temp_adiabat = True</code>, then the mean adiabatic temperature, \\(\\overline{T_A}\\), is used in the computation of the \\(\\gamma\\) parameters (and \\(\\beta_{FT1}\\) if <code>z_form=True</code>), rather than the mean free tropospheric temperature, \\(\\overline{T_{FT}}\\) if it is <code>False</code>.</p> <p>If <code>simple = True</code>, will return:</p> \\[ \\begin{align} \\begin{split} &amp;\\left(1 + \\mu(x)\\frac{\\delta r_s(x)}{r_s(x)}\\right)\\frac{\\delta T_s(x)}{\\overline{\\delta T_s}} \\approx \\\\ &amp;\\left(\\gamma_{T}\\frac{\\Delta T_s(x)}{\\overline{T_s}} - \\gamma_{r} \\frac{\\Delta r_s(x)}{\\overline{r_s}} - \\gamma_{\\delta \\Delta r} \\frac{\\delta \\Delta r_s(x)}{\\delta \\overline{T_s}} + \\gamma_{FT} \\frac{\\delta \\Delta T_{FT}(x)}{\\delta \\overline{T_s}}\\right) \\left(1 + \\overline{\\mu} \\frac{\\delta \\overline{r_s}}{\\overline{r_s}}\\right) \\end{split} \\end{align} \\] <p>Parameters:</p> Name Type Description Default <code>temp_surf_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average (can use mean or median) near surface temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K. We assume <code>n_exp=2</code>.</p> required <code>temp_surf_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_surf_quant[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface temperature of experiment <code>i</code>. Units: K. Note that <code>quant_use</code> is not provided as not needed by this function, but is likely to be <code>np.arange(1, 100)</code> - leave out <code>x=0</code> as doesn't really make sense to consider \\(0^{th}\\) percentile of a quantity.</p> required <code>sphum_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average near surface specific humidity of each simulation. Units: kg/kg.</p> required <code>sphum_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant[i, j]</code> is near-surface specific humidity, averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: kg/kg.</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level in Pa.</p> required <code>temp_ft_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average temperature at <code>pressure_ft</code> in Kelvin.</p> required <code>temp_ft_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_ft_quant[i, j]</code> is temperature at <code>pressure_ft</code>, averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: kg/kg.</p> required <code>z_ft_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average geopotential height at <code>pressure_ft</code> in m.</p> required <code>z_ft_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>z_ft_quant[i, j]</code> is geopotential height at <code>pressure_ft</code>, averaged over all days with near-surface temperature corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: m.</p> required <code>non_linear</code> <code>bool</code> <p>If <code>True</code>, will also include \\(\\mu\\delta r\\) terms in theory.</p> <code>False</code> <code>z_form</code> <code>bool</code> <p>If <code>True</code>, will return \\(z\\) version of theory.</p> <code>False</code> <code>use_temp_adiabat</code> <code>bool</code> <p>If <code>True</code>, then the mean adiabatic temperature, \\(\\overline{T_A}\\), is used in the computation of the \\(\\gamma\\) parameters, rather than the mean free tropospheric temperature, \\(\\overline{T_{FT}}\\) if it is <code>False</code>.</p> <code>False</code> <code>strict_conv_eqb</code> <code>bool</code> <p>If <code>True</code>, will ignore all \\(\\epsilon\\) terms in theory</p> <code>False</code> <code>simple</code> <code>bool</code> <p>If <code>True</code>, will exclude no-linear \\(\\Delta T_s \\Delta r_s\\) and \\(\\Delta T_s \\delta \\Delta r_s\\) terms. Will also exclude \\(\\frac{\\delta \\overline{r_s}}{\\delta \\overline{T_s}}\\) and \\(\\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}}\\) terms. Will not affect value of \\(\\mu\\) though.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scaling_factor</code> <code>ndarray</code> <p><code>float [n_quant]</code> <code>scaling_factor[i]</code> refers to the theoretical temperature difference between experiments for percentile <code>quant_use[i]</code>, relative to the mean temperature change, \\(\\delta \\overline{T_s}\\).</p> <code>info_coef</code> <code>dict</code> <p>The linear theory is a sum of 16 terms. The first four don't have any change (\\(\\delta\\)) factor, these are recorded in <code>info_coef['temp_mean_change']</code>. The next four preceed \\(\\frac{\\delta \\overline{r_s}}{\\delta \\overline{T_s}}\\), these are recorded in <code>info_coef['r_mean_change']</code>. The next four preceed \\(\\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}}\\), these are recorded in <code>info_coef['e_mean_change']</code>. The final four all preceed \\(\\delta \\Delta\\) quantities and are recorded in <code>info_coef['anomaly_change']</code>. <code>info_coef</code> is independent of the value of <code>non_linear</code> used.</p> <code>info_change</code> <code>dict</code> <p>Complementary dictionary to <code>info_coef</code> with same keys that gives the relavent change to a quantity i.e. <code>info_change['r_mean_change']</code> is \\(\\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}}\\). <code>info_change['anomaly_change']</code> is a dictionary of the four \\(\\delta \\Delta\\) changes. If <code>non_linear = True</code>, each value in <code>info_change</code> is divided by \\(\\left(1 + \\mu(x)\\frac{\\delta r_s(x)}{r_s(x)}\\right)\\).</p> <code>info_cont</code> <code>dict</code> <p>Dictionary containing same keys as <code>info_coef</code>. Each term is <code>info_coef</code> $   imes$ <code>info_change</code> so it gives the contribution in \\(K/K\\) to the scaling factor for each of the 16 terms.</p> <code>mu_factor</code> <code>Union[float, ndarray]</code> <p><code>float</code> or <code>float [n_quant]</code> The quantity \\(1 + \\mu(x)\\frac{\\delta r_s(x)}{r_s(x)}\\). Will be 1 if <code>non_linear = False</code>.</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def get_scaling_factor_theory(temp_surf_mean: np.ndarray, temp_surf_quant: np.ndarray, sphum_mean: np.ndarray,\n                              sphum_quant: np.ndarray, pressure_surf: float, pressure_ft: float,\n                              temp_ft_mean: np.ndarray, temp_ft_quant: np.ndarray, z_ft_mean: np.ndarray,\n                              z_ft_quant: np.ndarray, non_linear: bool = False,\n                              z_form: bool = False, use_temp_adiabat: bool = False,\n                              strict_conv_eqb: bool = False, simple: bool = False) -&gt; Tuple[\n    np.ndarray, dict, dict, dict, Union[float, np.ndarray]]:\n    \"\"\"\n    Calculates the theoretical near-surface temperature change for percentile $x$, $\\delta T_s(x)$, relative\n    to the mean temperature change, $\\overline{\\delta T_s}$. In the most complicated case, with `non_linear = True`,\n    this is:\n\n    $$\n    \\\\begin{align}\n    \\\\begin{split}\n    &amp;\\\\left(1 + \\mu(x)\\\\frac{\\delta r_s(x)}{r_s(x)}\\\\right)\\\\frac{\\delta T_s(x)}{\\overline{\\delta T_s}} \\\\approx\n    1 + \\overline{\\mu} \\\\frac{\\delta \\overline{r_s}}{\\overline{r_s}} + \\\\\\\\\n    &amp;\\\\left[\\gamma_{T}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}} -\n    \\gamma_{Tr}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\\\frac{\\Delta r_s(x)}{\\overline{r_s}}\n    - \\gamma_{r} \\\\frac{\\Delta r_s(x)}{\\overline{r_s}}-\n    \\gamma_{\\epsilon} \\\\frac{\\Delta \\epsilon(x)}{\\overline{\\\\beta_{s1}}\\overline{T_s}}\\\\right]\n    \\\\left(1 + \\overline{\\mu} \\\\frac{\\delta \\overline{r_s}}{\\overline{r_s}}\\\\right) +\\\\\\\\\n    &amp;\\\\left[-\\gamma_{T\\delta r}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}} +\n    \\gamma_{Tr\\delta r}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\\\frac{\\Delta r_s(x)}{\\overline{r_s}}\n    + \\gamma_{r\\delta r} \\\\frac{\\Delta r_s(x)}{\\overline{r_s}}-\n    \\gamma_{\\epsilon \\delta r} \\\\frac{\\Delta \\epsilon(x)}{\\overline{\\\\beta_{s1}}\\overline{T_s}}\\\\right]\n    \\\\frac{\\delta \\overline{r_s}}{\\delta \\overline{T_s}} +\\\\\\\\\n    &amp;\\\\left[-\\gamma_{T\\delta \\epsilon}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}} -\n    \\gamma_{Tr\\delta \\epsilon}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\\\frac{\\Delta r_s(x)}{\\overline{r_s}}\n    - \\gamma_{r\\delta \\epsilon} \\\\frac{\\Delta r_s(x)}{\\overline{r_s}}+\n    \\gamma_{\\epsilon \\delta \\epsilon} \\\\frac{\\Delta \\epsilon(x)}{\\overline{\\\\beta_{s1}}\\overline{T_s}}\\\\right]\n    \\\\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}} +\\\\\\\\\n    &amp;- \\gamma_{\\delta \\Delta r} \\\\frac{\\delta \\Delta r_s(x)}{\\delta \\overline{T_s}} -\n    \\gamma_{T \\delta \\Delta r} \\\\frac{\\Delta T_s(x)}{\\overline{T_s}}\\\\frac{\\delta \\Delta r_s(x)}{\\delta \\overline{T_s}}\n    + \\gamma_{FT} \\\\frac{\\delta \\Delta T_{FT}(x)}{\\delta \\overline{T_s}} + \\\n    \\gamma_{\\delta \\Delta \\epsilon} \\\\frac{\\delta \\Delta \\epsilon(x)}{\\delta \\overline{T_s}}\n    \\\\end{split}\n    \\\\end{align}\n    $$\n\n    If `z_form = True`, then all $\\gamma$ and $\\mu$ parameters are multiplied by\n    $\\\\frac{\\overline{\\\\beta_{s1}}}{\\overline{\\\\beta_{s1}} + \\\\beta_{FT1}}$, and $\\delta \\Delta T_{FT}(x)$ is replaced\n    with $\\\\frac{g}{R^{\\dagger}}\\delta \\Delta z_{FT}(x)$.\n\n    If `non_linear = False`, then $\\overline{\\mu}$ and $\\mu(x)$ are set to zero.\n\n    If `use_temp_adiabat = True`, then the mean adiabatic temperature, $\\overline{T_A}$, is used in the computation of\n    the $\\gamma$ parameters (and $\\\\beta_{FT1}$ if `z_form=True`), rather than the mean free tropospheric temperature,\n    $\\overline{T_{FT}}$ if it is `False`.\n\n    If `simple = True`, will return:\n\n    $$\n    \\\\begin{align}\n    \\\\begin{split}\n    &amp;\\\\left(1 + \\mu(x)\\\\frac{\\delta r_s(x)}{r_s(x)}\\\\right)\\\\frac{\\delta T_s(x)}{\\overline{\\delta T_s}}\n    \\\\approx \\\\\\\\\n    &amp;\\\\left(\\gamma_{T}\\\\frac{\\Delta T_s(x)}{\\overline{T_s}}\n    - \\gamma_{r} \\\\frac{\\Delta r_s(x)}{\\overline{r_s}} -\n    \\gamma_{\\delta \\Delta r} \\\\frac{\\delta \\Delta r_s(x)}{\\delta \\overline{T_s}}\n    + \\gamma_{FT} \\\\frac{\\delta \\Delta T_{FT}(x)}{\\delta \\overline{T_s}}\\\\right)\n    \\\\left(1 + \\overline{\\mu} \\\\frac{\\delta \\overline{r_s}}{\\overline{r_s}}\\\\right)\n    \\\\end{split}\n    \\\\end{align}\n    $$\n\n    Args:\n        temp_surf_mean: `float [n_exp]`&lt;/br&gt;\n            Average (can use mean or median) near surface temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*. We assume `n_exp=2`.\n        temp_surf_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_surf_quant[i, j]` is the percentile `quant_use[j]` of near surface temperature of\n            experiment `i`. Units: *K*.&lt;/br&gt;\n            Note that `quant_use` is not provided as not needed by this function, but is likely to be\n            `np.arange(1, 100)` - leave out `x=0` as doesn't really make sense to consider $0^{th}$ percentile\n            of a quantity.\n        sphum_mean: `float [n_exp]`&lt;/br&gt;\n            Average near surface specific humidity of each simulation. Units: *kg/kg*.\n        sphum_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant[i, j]` is near-surface specific humidity, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *kg/kg*.\n        pressure_surf:\n            Pressure at near-surface in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level in *Pa*.\n        temp_ft_mean: `float [n_exp]`&lt;/br&gt;\n            Average temperature at `pressure_ft` in Kelvin.\n        temp_ft_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_ft_quant[i, j]` is temperature at `pressure_ft`, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *kg/kg*.\n        z_ft_mean: `float [n_exp]`&lt;/br&gt;\n            Average geopotential height at `pressure_ft` in *m*.\n        z_ft_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `z_ft_quant[i, j]` is geopotential height at `pressure_ft`, averaged over all days with near-surface\n            temperature corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *m*.\n        non_linear: If `True`, will also include $\\mu\\delta r$ terms in theory.\n        z_form: If `True`, will return $z$ version of theory.\n        use_temp_adiabat: If `True`, then the mean adiabatic temperature, $\\overline{T_A}$, is used in the computation\n            of the $\\gamma$ parameters, rather than the mean free tropospheric temperature, $\\overline{T_{FT}}$\n            if it is `False`.\n        strict_conv_eqb: If `True`, will ignore all $\\epsilon$ terms in theory\n        simple: If `True`, will exclude no-linear $\\Delta T_s \\Delta r_s$ and $\\Delta T_s \\delta \\Delta r_s$ terms.\n            Will also exclude $\\\\frac{\\delta \\overline{r_s}}{\\delta \\overline{T_s}}$ and\n            $\\\\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}}$ terms. Will not affect value of $\\mu$ though.\n\n    Returns:\n        scaling_factor: `float [n_quant]`&lt;/br&gt;\n            `scaling_factor[i]` refers to the theoretical temperature difference between experiments\n            for percentile `quant_use[i]`, relative to the mean temperature change, $\\delta \\overline{T_s}$.\n        info_coef: The linear theory is a sum of 16 terms. The first four don't have any change ($\\delta$)\n            factor, these are recorded in `info_coef['temp_mean_change']`.\n            The next four preceed $\\\\frac{\\delta \\overline{r_s}}{\\delta \\overline{T_s}}$, these are recorded in\n            `info_coef['r_mean_change']`.\n            The next four preceed $\\\\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}}$, these are recorded in\n            `info_coef['e_mean_change']`.\n            The final four all preceed $\\delta \\Delta$ quantities and are recorded in `info_coef['anomaly_change']`.\n            `info_coef` is independent of the value of `non_linear` used.\n        info_change: Complementary dictionary to `info_coef` with same keys that gives the relavent change to a\n            quantity i.e. `info_change['r_mean_change']` is $\\\\frac{\\delta \\overline{\\epsilon}}{\\delta \\overline{T_s}}$.\n            `info_change['anomaly_change']` is a dictionary of the four $\\delta \\Delta$ changes.\n            If `non_linear = True`, each value in `info_change` is divided by\n            $\\\\left(1 + \\mu(x)\\\\frac{\\delta r_s(x)}{r_s(x)}\\\\right)$.\n        info_cont: Dictionary containing same keys as `info_coef`. Each term is `info_coef` $\\times$ `info_change`\n            so it gives the contribution in $K/K$ to the scaling factor for each of the 16 terms.\n        mu_factor: `float` or `float [n_quant]`&lt;/br&gt;\n            The quantity $1 + \\mu(x)\\\\frac{\\delta r_s(x)}{r_s(x)}$. Will be 1 if `non_linear = False`.\n    \"\"\"\n    # Compute relative humidities\n    r_mean = sphum_mean / sphum_sat(temp_surf_mean, pressure_surf)\n    r_quant = sphum_quant / sphum_sat(temp_surf_quant, pressure_surf)\n    r_anom = r_quant - r_mean[:, np.newaxis]\n\n    # Compute epsilon\n    # Quantify deviation from convective equilibrium in MSE space\n    epsilon_mean = (moist_static_energy(temp_surf_mean, sphum_mean, height=0) -\n                    moist_static_energy(temp_ft_mean, sphum_sat(temp_ft_mean, pressure_ft), z_ft_mean)) * 1000\n    epsilon_quant = (moist_static_energy(temp_surf_quant, sphum_quant, height=0) -\n                     moist_static_energy(temp_ft_quant, sphum_sat(temp_ft_quant, pressure_ft), z_ft_quant)) * 1000\n    epsilon_anom = epsilon_quant - epsilon_mean[:, np.newaxis]\n\n    # Get factors needed for theory\n    if use_temp_adiabat:\n        # use adiabatic temperature for current climate to compute gamma params so only depends on surface quantities\n        temp_mean_ft_beta_use = get_temp_adiabat(temp_surf_mean[0], sphum_mean[0], pressure_surf, pressure_ft)\n    else:\n        temp_mean_ft_beta_use = temp_ft_mean[0]\n    gamma = get_gamma_factors(temp_surf_mean[0], sphum_mean[0], temp_mean_ft_beta_use, pressure_surf, pressure_ft)\n    _, _, alpha_s, beta_s1, _, _, _ = get_theory_prefactor_terms(temp_surf_mean[0], pressure_surf, pressure_ft,\n                                                                 sphum_mean[0])\n    if non_linear:\n        _, _, alpha_s_x, beta_s1_x, _, _, _ = get_theory_prefactor_terms(temp_surf_quant[0], pressure_surf, pressure_ft,\n                                                                         sphum_quant[0])\n        mu_x = L_v * alpha_s_x * sphum_quant[0] / beta_s1_x\n        mu = L_v * alpha_s * sphum_mean[0] / beta_s1\n    else:\n        mu = 0\n        mu_x = 0\n\n    if z_form:\n        R_mod, _, _, beta_ft1, _, _, _ = get_theory_prefactor_terms(temp_mean_ft_beta_use, pressure_surf, pressure_ft)\n        temp_ft_anom_change = g / R_mod * np.diff(z_ft_quant - z_ft_mean[:, np.newaxis], axis=0)[0]\n        # Need to multiply all mu and gamma factors by beta_s1/(beta_s1+beta_ft1) if z form of theory\n        mu = mu * beta_s1 / (beta_s1 + beta_ft1)\n        mu_x = mu_x * beta_s1 / (beta_s1 + beta_ft1)\n        for key1 in gamma:\n            for key2 in gamma[key1]:\n                gamma[key1][key2] = gamma[key1][key2] * beta_s1 / (beta_s1 + beta_ft1)\n    else:\n        temp_ft_anom_change = np.diff(temp_ft_quant - temp_ft_mean[:, np.newaxis], axis=0)[0]\n    mu_factor = 1 + mu * (r_mean[1] - r_mean[0]) / r_mean[0]\n    mu_factor_x = 1 + mu_x * (r_quant[1] - r_quant[0]) / r_quant[0]\n\n    anom_norm0 = {'t0': (temp_surf_quant - temp_surf_mean[:, np.newaxis])[0] / temp_surf_mean[0],\n                  'r0': r_anom[0] / r_mean[0], 'e0': epsilon_anom[0] / (beta_s1 * temp_surf_mean[0])}\n    anom_norm0['t0_r0'] = anom_norm0['t0'] * anom_norm0['r0']\n\n    # Record the sign of each term, and also normalize relative humidity change terms by climatological mean\n    # relative humidity\n    coef_sign = {'t_mean_change': {key2: 1 if key2 == 't0' else -1 for key2 in gamma['t_mean_change']},\n                 'r_mean_change': {key2: -1 if key2 in ['t0', 'e0'] else 1\n                                   for key2 in gamma['r_mean_change']},\n                 'e_mean_change': {key2: 1 if key2 == 'e0' else -1 for key2 in gamma['e_mean_change']},\n                 'anomaly_change': {key2: -1 if 'r' in key2 else 1 for key2 in gamma['anomaly_change']}}\n\n    # Each term is a coefficient evaluated in the current climate, multiplied by a change between climates.\n    # Record the climatological coefficient in info_coef\n    info_coef = {key: {} for key in gamma}\n    for key1 in gamma:\n        for key2 in gamma[key1]:\n            if 'anomaly' in key1:\n                if 't0' in key2:\n                    info_coef[key1][key2] = coef_sign[key1][key2] * gamma[key1][key2] * anom_norm0['t0']\n                else:\n                    info_coef[key1][key2] = coef_sign[key1][key2] * gamma[key1][key2]\n            else:\n                info_coef[key1][key2] = coef_sign[key1][key2] * gamma[key1][key2] * anom_norm0[key2]\n\n    if strict_conv_eqb:\n        # Ignore effect of epsilon\n        for key1 in gamma:\n            for key2 in gamma[key1]:\n                if 'e_mean' in key1 or key2 == 'e' or key2 == 'e0':\n                    info_coef[key1][key2] = 0\n    if simple:\n        for key1 in gamma:\n            for key2 in gamma[key1]:\n                if 'r_mean' in key1 or 'e_mean' in key1:\n                    # only keep temp mean changes\n                    info_coef[key1][key2] = 0\n                if 't0_r' in key2:\n                    # remove non-linear terms\n                    info_coef[key1][key2] = 0\n\n    # Record the change between climates in info_change\n    info_change = {'t_mean_change': (temp_surf_mean[1] - temp_surf_mean[0]) * mu_factor,\n                   'r_mean_change': r_mean[1] - r_mean[0],\n                   'e_mean_change': epsilon_mean[1] - epsilon_mean[0],\n                   'anomaly_change': {'r': r_anom[1] - r_anom[0],\n                                      't0_r': r_anom[1] - r_anom[0],\n                                      'ft': temp_ft_anom_change,\n                                      'e': epsilon_anom[1] - epsilon_anom[0]}}\n\n    # info_change - Normalise by mean temp change to get scale factor estimate\n    # info_cont - Multiply coef by change to get overall contribution of each term\n    info_cont = {key1: {} for key1 in gamma}\n    for key1 in gamma:\n        if 'mean' in key1:\n            info_change[key1] = info_change[key1] / (temp_surf_mean[1] - temp_surf_mean[0]) / mu_factor_x\n        for key2 in gamma[key1]:\n            if 'mean' in key1:\n                info_cont[key1][key2] = info_coef[key1][key2] * info_change[key1]\n            else:\n                info_change[key1][key2] = info_change[key1][key2] / (temp_surf_mean[1] - temp_surf_mean[0]\n                                                                     ) / mu_factor_x\n                info_cont[key1][key2] = info_coef[key1][key2] * info_change[key1][key2]\n\n    final_answer = mu_factor / mu_factor_x + sum([sum([info_cont[key1][key2] for key2 in info_coef[key1]])\n                                                  for key1 in info_coef])\n\n    return final_answer, info_coef, info_change, info_cont, mu_factor_x\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.get_temp_adiabat","title":"<code>get_temp_adiabat(temp_surf, sphum_surf, pressure_surf, pressure_ft, guess_temp_adiabat=273, epsilon=0)</code>","text":"<p>This returns the adiabatic temperature at <code>pressure_ft</code>, \\(T_{A, FT}\\), such that surface moist static energy equals free troposphere saturated moist static energy (plus any additional CAPE, quantified through \\(\\epsilon\\)):</p> <p>\\(h(T_s, q_s, p_{FT}) = h^*(T_{A}, p_{FT}) + \\epsilon\\).</p> <p>Parameters:</p> Name Type Description Default <code>temp_surf</code> <code>Union[float, ndarray]</code> <p>Temperature at <code>pressure_surf</code> in Kelvin. If array, must be same size as <code>sphum_surf</code></p> required <code>sphum_surf</code> <code>Union[float, ndarray]</code> <p>Specific humidity at <code>pressure_surf</code> in kg/kg. If array, must be same size as <code>temp_surf</code></p> required <code>pressure_surf</code> <code>Union[float, ndarray]</code> <p>Pressure at near-surface in Pa. Either single value or one for each <code>temp_surf</code>.</p> required <code>pressure_ft</code> <code>Union[float, ndarray]</code> <p>Pressure at free troposphere level in Pa. Either single value or one for each <code>temp_surf</code>.</p> required <code>guess_temp_adiabat</code> <code>float</code> <p>Initial guess for what adiabatic temperature at <code>pressure_ft</code> should be.</p> <code>273</code> <code>epsilon</code> <code>Union[float, ndarray]</code> <p>\\(h_s-h^*_{FT}\\) in kJ/kg. Quantifies how much larger near-surface MSE is than free tropospheric saturated. If array, must be same size as <code>temp_surf</code> and <code>sphum_surf</code>.</p> <code>0</code> <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Adiabatic temperature at <code>pressure_ft</code> in Kelvin. If array, will be same size as <code>temp_surf</code> and <code>sphum_surf</code>.</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def get_temp_adiabat(temp_surf: Union[float, np.ndarray], sphum_surf: Union[float, np.ndarray],\n                     pressure_surf: Union[float, np.ndarray], pressure_ft: Union[float, np.ndarray],\n                     guess_temp_adiabat: float = 273, epsilon: Union[float, np.ndarray] = 0) -&gt; Union[\n    float, np.ndarray]:\n    \"\"\"\n    This returns the adiabatic temperature at `pressure_ft`, $T_{A, FT}$, such that surface moist static\n    energy equals free troposphere saturated moist static energy\n    (plus any additional CAPE, quantified through $\\epsilon$):\n\n    $h(T_s, q_s, p_{FT}) = h^*(T_{A}, p_{FT}) + \\epsilon$.\n\n    Args:\n        temp_surf:\n            Temperature at `pressure_surf` in Kelvin. If array, must be same size as `sphum_surf`\n        sphum_surf:\n            Specific humidity at `pressure_surf` in *kg/kg*. If array, must be same size as `temp_surf`\n        pressure_surf:\n            Pressure at near-surface in *Pa*. Either single value or one for each `temp_surf`.\n        pressure_ft:\n            Pressure at free troposphere level in *Pa*. Either single value or one for each `temp_surf`.\n        guess_temp_adiabat:\n            Initial guess for what adiabatic temperature at `pressure_ft` should be.\n        epsilon:\n            $h_s-h^*_{FT}$ in *kJ/kg*. Quantifies how much larger near-surface MSE is than free tropospheric saturated.\n            If array, must be same size as `temp_surf` and `sphum_surf`.\n\n    Returns:\n        Adiabatic temperature at `pressure_ft` in Kelvin. If array, will be same size as `temp_surf` and `sphum_surf`.\n    \"\"\"\n    if isinstance(temp_surf, numbers.Number):\n        return scipy.optimize.fsolve(temp_adiabat_fit_func, guess_temp_adiabat,\n                                     args=(temp_surf, sphum_surf, pressure_surf, pressure_ft, epsilon))\n    elif isinstance(temp_surf, np.ndarray):\n        return scipy.optimize.fsolve(temp_adiabat_fit_func, np.full_like(temp_surf, guess_temp_adiabat),\n                                     args=(temp_surf, sphum_surf, pressure_surf, pressure_ft, epsilon))\n    else:\n        raise ValueError('Invalid value for `temp_surf`: must be float or np.ndarray')\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.get_temp_adiabat_surf","title":"<code>get_temp_adiabat_surf(humidity_surf, temp_ft, z_ft, pressure_surf, pressure_ft, rh_form=True, guess_temp_surf=283, epsilon=0)</code>","text":"<p>This returns the temperature at <code>pressure_surf</code>, \\(T_s\\), such that near-surface moist static energy equals free troposphere saturated moist static energy (plus any additional CAPE quantified through \\(\\epsilon\\)):</p> <p>\\(h(T_s, q_s, p_{FT}) = h^*(T_{FT}, p_{FT}) + \\epsilon\\)</p> <p>given the near-surface specific humidity \\(q_s\\) or relative humidity \\(r_s\\).</p> <p>Parameters:</p> Name Type Description Default <code>humidity_surf</code> <code>float</code> <p>Specific humidity in kg/kg or relative humidity at <code>pressure_surf</code>.</p> required <code>temp_ft</code> <code>float</code> <p>Temperature at <code>pressure_ft</code> in Kelvin.</p> required <code>z_ft</code> <code>Optional[float]</code> <p>Geopotential height at <code>pressure_ft</code> in m. If <code>None</code> will approximate as \\(z_{FT} \\approx \\frac{R^{\\dagger}}{g}(T_s + T_A)\\).</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level in Pa.</p> required <code>rh_form</code> <code>bool</code> <p>If <code>True</code> (recommended), will return surface temperature for a given relative humidity. Otherwise, will return for a given specific humidity.</p> <code>True</code> <code>guess_temp_surf</code> <code>float</code> <p>Initial guess for what adiabatic temperature at <code>pressure_surf</code> should be.</p> <code>283</code> <code>epsilon</code> <code>float</code> <p>Proxy for CAPE in kJ/kg. Quantifies how much larger near-surface MSE is than free tropospheric saturated.</p> <code>0</code> <p>Returns:</p> Type Description <code>float</code> <p>Convectively maintained temperature at <code>pressure_surf</code> in Kelvin.</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def get_temp_adiabat_surf(humidity_surf: float, temp_ft: float, z_ft: Optional[float],\n                          pressure_surf: float, pressure_ft: float, rh_form: bool = True,\n                          guess_temp_surf: float = 283, epsilon: float = 0) -&gt; float:\n    \"\"\"\n    This returns the temperature at `pressure_surf`, $T_s$, such that near-surface moist static\n    energy equals free troposphere saturated moist static energy\n    (plus any additional CAPE quantified through $\\epsilon$):\n\n    $h(T_s, q_s, p_{FT}) = h^*(T_{FT}, p_{FT}) + \\epsilon$\n\n    given the near-surface specific humidity $q_s$ or relative humidity $r_s$.\n\n    Args:\n        humidity_surf:\n            Specific humidity in *kg/kg* or relative humidity at `pressure_surf`.\n        temp_ft:\n            Temperature at `pressure_ft` in Kelvin.\n        z_ft:\n            Geopotential height at `pressure_ft` in *m*. If `None` will approximate as\n            $z_{FT} \\\\approx \\\\frac{R^{\\\\dagger}}{g}(T_s + T_A)$.\n        pressure_surf:\n            Pressure at near-surface in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level in *Pa*.\n        rh_form:\n            If `True` (recommended), will return surface temperature for a given relative humidity.\n            Otherwise, will return for a given specific humidity.\n        guess_temp_surf:\n            Initial guess for what adiabatic temperature at `pressure_surf` should be.\n        epsilon:\n            Proxy for CAPE in *kJ/kg*. Quantifies how much larger near-surface MSE is than free tropospheric saturated.\n\n    Returns:\n        Convectively maintained temperature at `pressure_surf` in Kelvin.\n    \"\"\"\n    if rh_form:\n\n        def solve_temp_adiabat(temp_guess: float, temp_ft: float, humidity_surf: float,\n                               z_ft: Optional[float], pressure_surf: float, pressure_ft: float,\n                               epsilon: float = 0) -&gt; float:\n            \"\"\"Solve for surface temperature satisfying the adiabatic moist static energy balance.\"\"\"\n            sol = scipy.optimize.fsolve(\n                temp_adiabat_surf_fit_func,\n                x0=temp_guess,\n                args=(temp_ft, humidity_surf, None, pressure_surf, pressure_ft, epsilon)\n            )\n            return sol[0]\n\n        def solve_temp_adiabat_with_z(temp_guess: float, temp_ft: float, humidity_surf: float,\n                                      z_ft: float, pressure_surf: float, pressure_ft: float,\n                                      epsilon: float = 0) -&gt; float:\n            \"\"\"Solve for surface temperature satisfying the adiabatic moist static energy balance.\"\"\"\n            sol = scipy.optimize.fsolve(\n                temp_adiabat_surf_fit_func,\n                x0=temp_guess,\n                args=(temp_ft, humidity_surf, z_ft, pressure_surf, pressure_ft, epsilon)\n            )\n            return sol[0]\n\n        T_solved = xr.apply_ufunc(\n            solve_temp_adiabat if z_ft is None else solve_temp_adiabat_with_z,\n            guess_temp_surf, temp_ft, humidity_surf, z_ft, pressure_surf, pressure_ft, epsilon,\n            input_core_dims=[[], [], [], [], [], [], []],  # all scalar-like per location\n            output_core_dims=[[]],\n            vectorize=True,  # broadcast over all non-core dims (lat, lon)\n            dask=\"parallelized\",  # if arrays are Dask-backed\n            output_dtypes=[float],\n        )\n        return T_solved\n    else:\n        if z_ft is None:\n            R_mod = R * np.log(pressure_surf / pressure_ft) / 2\n            mse_ft_sat = moist_static_energy(temp_ft, sphum_sat(temp_ft, pressure_ft), height=0,\n                                             c_p_const=c_p + R_mod) * 1000 + epsilon * 1000\n            return (mse_ft_sat - L_v * humidity_surf) / (c_p - R_mod)\n        else:\n            mse_ft_sat = moist_static_energy(temp_ft, sphum_sat(temp_ft, pressure_ft), height=z_ft) * 1000 + \\\n                         epsilon * 1000\n            return (mse_ft_sat - L_v * humidity_surf) / c_p\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.get_theory_prefactor_terms","title":"<code>get_theory_prefactor_terms(temp, pressure_surf, pressure_ft, sphum=None)</code>","text":"<p>Returns prefactors to do modified moist static energy, \\(\\delta h^{\\dagger}\\) taylor expansions.</p> <p>Note units of returned variables are in J rather than kJ unlike most MSE stuff.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>Union[ndarray, float]</code> <p><code>float</code> or <code>float [n_temp]</code> Temperatures to compute prefactor terms for.</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface, \\(p_s\\) in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level, \\(p_{FT}\\) in Pa.</p> required <code>sphum</code> <code>Optional[Union[ndarray, float]]</code> <p>If given, will return surface prefactor terms, otherwise will return free troposphere values. <code>float</code> or <code>float [n_temp]</code> Specific humidity corresponding to the temperature i.e. specific humidity conditioned on same days as temperature given.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>R_mod</code> <code>float</code> <p>Modified gas constant, \\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\) Units: J/kg/K</p> <code>q_sat</code> <code>Union[float, ndarray]</code> <p><code>float</code> or <code>float [n_temp]</code> Saturated specific humidity. \\(q^*(T, p_s)\\) if <code>sphum</code> given, otherwise \\(q^*(T, p_{FT})\\) Units: kg/kg</p> <code>alpha</code> <code>Union[float, ndarray]</code> <p><code>float</code> or <code>float [n_temp]</code> Clausius clapeyron parameter. \\(\\alpha(T, p_s)\\) if <code>sphum</code> given, otherwise \\(\\alpha(T, p_{FT})\\) Units: K\\(^{-1}\\)</p> <code>beta_1</code> <code>Union[float, ndarray]</code> <p><code>float</code> or <code>float [n_temp]</code> \\(\\frac{d(h^{\\dagger}+\\epsilon)}{dT_s}(T, p_s)\\) if <code>sphum</code> given, otherwise \\(\\frac{h^{\\dagger}}{dT_{FT}}(T, p_{FT})\\). Units: J/kg/K</p> <code>beta_2</code> <code>Union[float, ndarray]</code> <p><code>float</code> or <code>float [n_temp]</code> \\(T_s\\frac{d^2(h^{\\dagger}+\\epsilon)}{dT_s^2}(T, p_s)\\) if <code>sphum</code> given, otherwise \\(T_{FT}\\frac{d^2h^{\\dagger}}{dT_{FT}^2}(T, p_{FT})\\). Units: J/kg/K</p> <code>beta_3</code> <code>Union[float, ndarray]</code> <p><code>float</code> or <code>float [n_temp]</code> \\(T_s^2\\frac{d^3(h^{\\dagger}+\\epsilon)}{dT_s^3}(T, p_s)\\) if <code>sphum</code> given, otherwise \\(T_{FT}^2\\frac{d^3h^{\\dagger}}{dT_{FT}^3}(T, p_{FT})\\). Units: J/kg/K</p> <code>mu</code> <code>Union[float, ndarray]</code> <p><code>float</code> or <code>float [n_temp]</code> \\(\\mu = 1 - \\frac{c_p - R^{\\dagger}}{c_p - R^{\\dagger} + L_v \\alpha_s q_s} = \\frac{L_v \\alpha_s q_s}{\\beta_{s1}}\\) If <code>sphum</code> is not given, will set to <code>np.nan</code>.</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def get_theory_prefactor_terms(temp: Union[np.ndarray, float], pressure_surf: float,\n                               pressure_ft: float, sphum: Optional[Union[np.ndarray, float]] = None\n                               ) -&gt; Tuple[float, Union[float, np.ndarray], Union[float, np.ndarray],\nUnion[float, np.ndarray], Union[float, np.ndarray], Union[float, np.ndarray], Union[float, np.ndarray]]:\n    \"\"\"\n    Returns prefactors to do modified moist static energy, $\\delta h^{\\dagger}$ taylor expansions.\n\n    Note units of returned variables are in *J* rather than *kJ* unlike most MSE stuff.\n\n    Args:\n        temp: `float` or `float [n_temp]`&lt;/br&gt;\n            Temperatures to compute prefactor terms for.\n        pressure_surf:\n            Pressure at near-surface, $p_s$ in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level, $p_{FT}$ in *Pa*.\n        sphum: If given, will return surface prefactor terms, otherwise will return free troposphere values.&lt;/br&gt;\n            `float` or `float [n_temp]`&lt;/br&gt;\n            Specific humidity corresponding to the temperature i.e. specific humidity conditioned on same\n            days as temperature given.\n\n    Returns:\n        R_mod: Modified gas constant, $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$&lt;/br&gt;\n            Units: *J/kg/K*\n        q_sat: `float` or `float [n_temp]`&lt;/br&gt;\n            Saturated specific humidity. $q^*(T, p_s)$ if `sphum` given, otherwise $q^*(T, p_{FT})$&lt;/br&gt;\n            Units: *kg/kg*\n        alpha: `float` or `float [n_temp]`&lt;/br&gt;\n            Clausius clapeyron parameter. $\\\\alpha(T, p_s)$ if `sphum` given, otherwise $\\\\alpha(T, p_{FT})$&lt;/br&gt;\n            Units: K$^{-1}$\n        beta_1: `float` or `float [n_temp]`&lt;/br&gt;\n            $\\\\frac{d(h^{\\\\dagger}+\\epsilon)}{dT_s}(T, p_s)$ if `sphum` given, otherwise\n            $\\\\frac{h^{\\\\dagger}}{dT_{FT}}(T, p_{FT})$.&lt;/br&gt;\n            Units: *J/kg/K*\n        beta_2: `float` or `float [n_temp]`&lt;/br&gt;\n            $T_s\\\\frac{d^2(h^{\\\\dagger}+\\epsilon)}{dT_s^2}(T, p_s)$ if `sphum` given, otherwise\n            $T_{FT}\\\\frac{d^2h^{\\\\dagger}}{dT_{FT}^2}(T, p_{FT})$.&lt;/br&gt;\n            Units: *J/kg/K*\n        beta_3: `float` or `float [n_temp]`&lt;/br&gt;\n            $T_s^2\\\\frac{d^3(h^{\\\\dagger}+\\epsilon)}{dT_s^3}(T, p_s)$ if `sphum` given, otherwise\n            $T_{FT}^2\\\\frac{d^3h^{\\\\dagger}}{dT_{FT}^3}(T, p_{FT})$.&lt;/br&gt;\n            Units: *J/kg/K*\n        mu: `float` or `float [n_temp]`&lt;/br&gt;\n            $\\\\mu = 1 - \\\\frac{c_p - R^{\\\\dagger}}{c_p - R^{\\\\dagger} + L_v \\\\alpha_s q_s} =\n            \\\\frac{L_v \\\\alpha_s q_s}{\\\\beta_{s1}}$&lt;/br&gt;\n            If `sphum` is not given, will set to `np.nan`.&lt;/br&gt;\n\n    \"\"\"\n    R_mod = R * np.log(pressure_surf / pressure_ft) / 2\n    if sphum is None:\n        # if sphum not given, then free troposphere used\n        c_p_use = c_p + R_mod\n        pressure_use = pressure_ft\n        sphum = sphum_sat(temp, pressure_use)\n        mu = np.nan * sphum\n    else:\n        c_p_use = c_p - R_mod\n        pressure_use = pressure_surf\n        mu = None  # set so compute later\n    alpha = clausius_clapeyron_factor(temp, pressure_use)\n    beta_1 = c_p_use + L_v * alpha * sphum\n    beta_2 = L_v * alpha * sphum * (alpha * temp - 2)\n    beta_3 = L_v * alpha * sphum * ((alpha * temp) ** 2 - 6 * alpha * temp + 6)\n    q_sat = sphum_sat(temp, pressure_use)\n    if isinstance(mu, type(None)):\n        mu = L_v * sphum * alpha / beta_1\n    return R_mod, q_sat, alpha, beta_1, beta_2, beta_3, mu\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.get_z_ft_approx","title":"<code>get_z_ft_approx(temp_surf, temp_ft, pressure_surf, pressure_ft, z_surf=0)</code>","text":"<p>Returns an approximation for geopotential height, \\(z_{FT}\\) at pressure \\(p_{FT}\\) according to:</p> \\[z_{FT} \\approx \\frac{R^{\\dagger}}{g}(T_s + T_{FT}) + z_s\\] <p>where \\(R^{\\dagger} = \\ln(p_s/p_{FT})/2\\) and \\(s\\) refers to the surface. This assumes hydrostatic balance and fixed lapse rate between the surface and \\(p_{FT}\\).</p> <p>Parameters:</p> Name Type Description Default <code>temp_surf</code> <code>Union[float, ndarray]</code> <p><code>float [n_temp]</code> Temperature at <code>pressure_surf</code> in Kelvin.</p> required <code>temp_ft</code> <code>Union[float, ndarray]</code> <p><code>float [n_temp]</code> Temperature at <code>pressure_ft</code> in Kelvin.</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level in Pa.</p> required <code>z_surf</code> <code>Union[float, ndarray]</code> <p><code>float [n_temp]</code> Geopotential height at <code>pressure_surf</code> in m. Can also give <code>0</code> to ignore this contribution.</p> <code>0</code> <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p><code>float [n_temp]</code> Geopotential height at <code>pressure_ft</code> in m.</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def get_z_ft_approx(temp_surf: Union[float, np.ndarray], temp_ft: Union[float, np.ndarray],\n                    pressure_surf: float, pressure_ft: float,\n                    z_surf: Union[float, np.ndarray] = 0) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Returns an approximation for geopotential height, $z_{FT}$ at pressure $p_{FT}$ according to:\n\n    $$z_{FT} \\\\approx \\\\frac{R^{\\\\dagger}}{g}(T_s + T_{FT}) + z_s$$\n\n    where $R^{\\dagger} = \\\\ln(p_s/p_{FT})/2$ and $s$ refers to the surface. This assumes hydrostatic balance\n    and fixed lapse rate between the surface and $p_{FT}$.\n\n    Args:\n        temp_surf: `float [n_temp]`&lt;/br&gt;\n            Temperature at `pressure_surf` in Kelvin.\n        temp_ft: `float [n_temp]`&lt;/br&gt;\n            Temperature at `pressure_ft` in Kelvin.\n        pressure_surf:\n            Pressure at near-surface in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level in *Pa*.\n        z_surf: `float [n_temp]`&lt;/br&gt;\n            Geopotential height at `pressure_surf` in *m*. Can also give `0` to ignore this contribution.\n\n    Returns:\n        `float [n_temp]`&lt;/br&gt;\n            Geopotential height at `pressure_ft` in *m*.\n    \"\"\"\n    R_mod = R * np.log(pressure_surf / pressure_ft) / 2\n    return R_mod / g * (temp_surf + temp_ft) + z_surf\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.mse_mod_anom_change_ft_expansion","title":"<code>mse_mod_anom_change_ft_expansion(temp_ft_mean, temp_ft_quant, pressure_surf, pressure_ft, taylor_terms='linear', mse_mod_mean_change=None, temp_ft_anom0=None)</code>","text":"<p>This function returns an approximation in the change in modified MSE anomaly, \\(\\delta \\Delta h^{\\dagger} = \\delta (h^{\\dagger}(x) - \\overline{h^{\\dagger}})\\), with warming - the basis of a theory for \\(\\delta T_s(x)\\).</p> <p>Doing a second order taylor expansion of \\(h^{\\dagger}\\) in the base climate, about free tropospheric temperature \\(\\overline{T_{FT}}\\), we can get:</p> \\[\\Delta h^{\\dagger}(x) \\approx \\beta_{FT1} \\Delta T_{FT} + \\frac{1}{2\\overline{T_{FT}}} \\beta_{FT2} \\Delta T_{FT}^2\\] Terms in equation <ul> <li>\\(h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s \\approx \\left(c_p + R^{\\dagger}\\right) T_{FT} + L_v q^*_{FT}\\) where we used an approximate relation to replace \\(z_{FT}\\) in \\(h^*_{FT}\\).</li> <li>\\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\)</li> <li>\\(\\Delta T_{FT} = T_{FT}(x) - \\overline{T_{FT}}\\)</li> <li>\\(\\beta_{FT1} = \\frac{d\\overline{h^{\\dagger}}}{d\\overline{T_{FT}}} = c_p + R^{\\dagger} + L_v \\alpha_{FT} q_{FT}^*\\)</li> <li>\\(\\beta_{FT2} = \\overline{T_{FT}} \\frac{d^2\\overline{h^{\\dagger}}}{d\\overline{T_{FT}}^2} =  \\overline{T_{FT}}\\frac{d\\beta_{FT1}}{d\\overline{T_{FT}}} =  L_v \\alpha_{FT} q_{FT}^*(\\alpha_{FT} \\overline{T_{FT}} - 2)\\)</li> <li>All terms on RHS are evaluated at the free tropospheric adiabatic temperature, \\(T_{FT}\\). I.e. \\(q_{FT}^* = q^*(T_{FT}, p_{FT})\\) where \\(p_{FT}\\) is the free tropospheric pressure.</li> </ul> <p>Doing a second taylor expansion on this equation for a change with warming between simulations, \\(\\delta\\), we can decompose \\(\\delta \\Delta h^{\\dagger}(x)\\) into terms involving \\(\\delta \\Delta T_{FT}\\) and \\(\\delta \\overline{T_{FT}}\\)</p> <p>We can then use a third taylor expansion to relate \\(\\delta \\overline{T_{FT}}\\) to \\(\\delta \\overline{h^{\\dagger}}\\):</p> \\[\\delta \\overline{T_{FT}} \\approx \\frac{\\delta \\overline{h^{\\dagger}}}{\\beta_{FT1}} - \\frac{1}{2} \\frac{\\beta_{FT2}}{\\beta_{FT1}^3 \\overline{T_{FT}}} (\\delta \\overline{h^{\\dagger}})^2\\] <p>Overall, we get \\(\\delta \\Delta h^{\\dagger}\\) as a function of \\(\\delta \\Delta T_{FT}\\), \\(\\delta \\overline{h^{\\dagger}}\\) and quantities evaluated at the base climate. The <code>taylor_terms</code> variable can be used to specify how many terms we want to keep.</p> <p>The simplest equation with <code>taylor_terms = 'linear'</code> is:</p> \\[\\delta \\Delta h^{\\dagger} \\approx \\beta_{FT1} \\delta \\Delta T_{FT} + \\frac{\\beta_{FT2}}{\\beta_{FT1}}\\frac{\\Delta T_{FT}}{\\overline{T_{FT}}} \\delta \\overline{h^{\\dagger}}\\] <p>Parameters:</p> Name Type Description Default <code>temp_ft_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average free tropospheric temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K. We assume <code>n_exp=2</code>. Could also use adiabatic temperature, \\(\\overline{T_A}\\), instead if want to assume strict convective equilibrium.</p> required <code>temp_ft_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_ft_quant[i, j]</code> is free tropospheric temperature, averaged over all days with near-surface temperature corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: K. Note that <code>quant_use</code> is not provided as not needed by this function, but is likely to be <code>np.arange(1, 100)</code> - leave out <code>x=0</code> as doesn't really make sense to consider \\(0^{th}\\) percentile of a quantity. Could also use adiabatic temperature, \\(T_A(x)\\), instead if want to assume strict convective equilibrium.</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level in Pa.</p> required <code>taylor_terms</code> <code>str</code> <p>The approximations in this equation arise from the three taylor series mentioned above, we can specify how many terms we want to keep, with one of the 3 options below:</p> <ul> <li><code>linear</code>: Only keep the two terms which are linear in all three taylor series i.e. \\(\\delta \\Delta h^{\\dagger} \\approx \\beta_{FT1} \\delta \\Delta T_{FT} + \\frac{\\beta_{FT2}}{\\beta_{FT1}}\\frac{\\Delta T_{FT}}{\\overline{T_{FT}}} \\delta \\overline{h^{\\dagger}}\\)</li> <li><code>non_linear</code>: Keep LL, LLL and LNL terms.</li> <li><code>squared_0</code>: Keep terms linear and squared in first expansion and then just linear terms: LL, LLL, SL, SLL.</li> <li><code>squared</code>: Keep four additional terms corresponding to LLS, LSL, LNL and SNL terms in the taylor series. SNL means second order in the first taylor series mentioned above, non-linear (i.e. \\(\\delta \\Delta T_{FT}\\delta \\overline{h^{\\dagger}}\\) terms)  in the second and linear in the third. These 5 terms are the most significant non-linear terms.</li> <li><code>full</code>: In addition to the terms in <code>squared</code>, we keep the usually small LSS and SLS terms.</li> </ul> <code>'linear'</code> <code>mse_mod_mean_change</code> <code>Optional[float]</code> <p><code>float [n_exp]</code> Can provide the \\(\\delta \\overline{h^{\\dagger}}\\) in J/kg. Use this if you want to use a particular taylor approximation for this.</p> <code>None</code> <code>temp_ft_anom0</code> <code>Optional[ndarray]</code> <p><code>float [n_quant]</code> Can specify the \\(\\Delta T_{FT}\\) term to use in the equation. May want to do this, if want to relate \\(\\Delta T_{FT}\\) to surface quantities assuming strict convective equilibrium.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>delta_mse_mod_anomaly</code> <code>ndarray</code> <p><code>float [n_quant]</code> \\(\\delta \\Delta h^{\\dagger}\\) conditioned on each quantile of near-surface temperature. Units: kJ/kg.</p> <code>info_dict</code> <code>dict</code> <p>Dictionary with 5 keys: <code>temp_ft_anom</code>, <code>mse_mod_mean</code>, <code>mse_mod_mean_squared</code>, <code>mse_mod_mean_cubed</code>, <code>non_linear</code>. For each key, a list containing a prefactor computed in the base climate and a change between simulations is returned. I.e. for <code>info_dict[non_linear][1]</code> would be \\(\\delta \\Delta T_{FT}\\delta \\overline{h^{\\dagger}}\\) and the total contribution of non-linear terms to \\(\\delta \\Delta h^{\\dagger}\\) would be <code>info_dict[non_linear][0] * info_dict[non_linear][1]</code>. In the <code>linear</code> case this would be zero, and <code>info_dict[temp_ft_anom][0]</code>\\(=\\beta_{FT1}\\) and <code>info_dict[mse_mod_mean][0]</code>\\(= \\frac{\\beta_{FT2}}{\\beta_{FT1}}\\frac{\\Delta T_A}{\\overline{T_{FT}}}\\) would be the only non-zero prefactors. Units of prefactor multiplied by change is kJ/kg.</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def mse_mod_anom_change_ft_expansion(temp_ft_mean: np.ndarray, temp_ft_quant: np.ndarray,\n                                     pressure_surf: float, pressure_ft: float,\n                                     taylor_terms: str = 'linear', mse_mod_mean_change: Optional[float] = None,\n                                     temp_ft_anom0: Optional[np.ndarray] = None) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"\n    This function returns an approximation in the change in modified MSE anomaly,\n    $\\delta \\Delta h^{\\dagger} = \\delta (h^{\\dagger}(x) - \\overline{h^{\\dagger}})$, with warming -\n    the basis of a theory for $\\delta T_s(x)$.\n\n    Doing a second order taylor expansion of $h^{\\dagger}$ in the base climate,\n    about free tropospheric temperature $\\overline{T_{FT}}$, we can get:\n\n    $$\\\\Delta h^{\\\\dagger}(x) \\\\approx \\\\beta_{FT1} \\\\Delta T_{FT} + \\\\frac{1}{2\\\\overline{T_{FT}}}\n    \\\\beta_{FT2} \\\\Delta T_{FT}^2$$\n\n    Terms in equation:\n        * $h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s \\\\approx \\\\left(c_p + R^{\\dagger}\\\\right) T_{FT} + L_v q^*_{FT}$\n        where we used an approximate relation to replace $z_{FT}$ in $h^*_{FT}$.\n        * $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$\n        * $\\\\Delta T_{FT} = T_{FT}(x) - \\overline{T_{FT}}$\n        * $\\\\beta_{FT1} = \\\\frac{d\\\\overline{h^{\\\\dagger}}}{d\\overline{T_{FT}}} =\n        c_p + R^{\\dagger} + L_v \\\\alpha_{FT} q_{FT}^*$\n        * $\\\\beta_{FT2} = \\overline{T_{FT}} \\\\frac{d^2\\\\overline{h^{\\\\dagger}}}{d\\overline{T_{FT}}^2} =\n         \\overline{T_{FT}}\\\\frac{d\\\\beta_{FT1}}{d\\overline{T_{FT}}} =\n         L_v \\\\alpha_{FT} q_{FT}^*(\\\\alpha_{FT} \\\\overline{T_{FT}} - 2)$\n        * All terms on RHS are evaluated at the free tropospheric adiabatic temperature, $T_{FT}$. I.e.\n        $q_{FT}^* = q^*(T_{FT}, p_{FT})$ where $p_{FT}$ is the free tropospheric pressure.\n\n    Doing a second taylor expansion on this equation for a change with warming between simulations, $\\delta$, we can\n    decompose $\\delta \\\\Delta h^{\\\\dagger}(x)$ into terms involving $\\delta \\Delta T_{FT}$ and\n    $\\delta \\overline{T_{FT}}$\n\n    We can then use a third taylor expansion to relate $\\delta \\overline{T_{FT}}$ to $\\delta \\overline{h^{\\\\dagger}}$:\n\n    $$\\\\delta \\\\overline{T_{FT}} \\\\approx \\\\frac{\\\\delta \\\\overline{h^{\\\\dagger}}}{\\\\beta_{FT1}} -\n    \\\\frac{1}{2} \\\\frac{\\\\beta_{FT2}}{\\\\beta_{FT1}^3 \\\\overline{T_{FT}}} (\\\\delta \\\\overline{h^{\\\\dagger}})^2$$\n\n    Overall, we get $\\delta \\Delta h^{\\dagger}$ as a function of $\\delta \\Delta T_{FT}$,\n    $\\delta \\overline{h^{\\\\dagger}}$ and quantities evaluated at the base climate.\n    The `taylor_terms` variable can be used to specify how many terms we want to keep.\n\n    The simplest equation with `taylor_terms = 'linear'` is:\n\n    $$\\\\delta \\\\Delta h^{\\\\dagger} \\\\approx \\\\beta_{FT1} \\\\delta \\\\Delta T_{FT} +\n    \\\\frac{\\\\beta_{FT2}}{\\\\beta_{FT1}}\\\\frac{\\\\Delta T_{FT}}{\\\\overline{T_{FT}}} \\\\delta \\\\overline{h^{\\\\dagger}}$$\n\n    Args:\n        temp_ft_mean: `float [n_exp]`&lt;/br&gt;\n            Average free tropospheric temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*. We assume `n_exp=2`.\n            Could also use adiabatic temperature, $\\overline{T_A}$, instead if want to assume\n            strict convective equilibrium.\n        temp_ft_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_ft_quant[i, j]` is free tropospheric temperature, averaged over all days with near-surface temperature\n            corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *K*.&lt;/br&gt;\n            Note that `quant_use` is not provided as not needed by this function, but is likely to be\n            `np.arange(1, 100)` - leave out `x=0` as doesn't really make sense to consider $0^{th}$ percentile\n            of a quantity.\n            Could also use adiabatic temperature, $T_A(x)$, instead if want to assume\n            strict convective equilibrium.\n        pressure_surf:\n            Pressure at near-surface in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level in *Pa*.\n        taylor_terms:\n            The approximations in this equation arise from the three taylor series mentioned above, we can specify\n            how many terms we want to keep, with one of the 3 options below:\n\n            * `linear`: Only keep the two terms which are linear in all three taylor series i.e.\n            $\\\\delta \\\\Delta h^{\\\\dagger} \\\\approx \\\\beta_{FT1} \\\\delta \\\\Delta T_{FT} +\n            \\\\frac{\\\\beta_{FT2}}{\\\\beta_{FT1}}\\\\frac{\\\\Delta T_{FT}}{\\\\overline{T_{FT}}}\n            \\\\delta \\\\overline{h^{\\\\dagger}}$\n            * `non_linear`: Keep *LL*, *LLL* and *LNL* terms.\n            * `squared_0`: Keep terms linear and squared in first expansion and then just linear terms:\n            *LL*, *LLL*, *SL*, *SLL*.\n            * `squared`: Keep four additional terms corresponding to *LLS*, *LSL*, *LNL* and *SNL* terms in the\n            taylor series. SNL means second order in the first taylor series mentioned above, non-linear\n            (i.e. $\\\\delta \\\\Delta T_{FT}\\\\delta \\\\overline{h^{\\\\dagger}}$ terms)  in the second\n            and linear in the third. These 5 terms are the most significant non-linear terms.\n            * `full`: In addition to the terms in `squared`, we keep the usually small *LSS* and *SLS* terms.\n        mse_mod_mean_change: `float [n_exp]`&lt;/br&gt;\n            Can provide the $\\\\delta \\\\overline{h^{\\\\dagger}}$ in J/kg. Use this if you want to use a particular taylor\n            approximation for this.\n        temp_ft_anom0: `float [n_quant]`&lt;/br&gt;\n            Can specify the $\\Delta T_{FT}$ term to use in the equation. May want to do this, if want to\n            relate $\\Delta T_{FT}$ to surface quantities assuming strict convective equilibrium.\n\n    Returns:\n        delta_mse_mod_anomaly: `float [n_quant]`&lt;/br&gt;\n            $\\delta \\Delta h^{\\dagger}$ conditioned on each quantile of near-surface temperature. Units: *kJ/kg*.\n        info_dict: Dictionary with 5 keys: `temp_ft_anom`, `mse_mod_mean`, `mse_mod_mean_squared`,\n            `mse_mod_mean_cubed`, `non_linear`.&lt;/br&gt;\n            For each key, a list containing a prefactor computed in the base climate and a change between simulations is\n            returned. I.e. for `info_dict[non_linear][1]` would be\n            $\\\\delta \\\\Delta T_{FT}\\\\delta \\\\overline{h^{\\\\dagger}}$\n            and the total contribution of non-linear terms to $\\delta \\Delta h^{\\dagger}$ would be\n            `info_dict[non_linear][0] * info_dict[non_linear][1]`. In the `linear` case this would be zero,\n            and `info_dict[temp_ft_anom][0]`$=\\\\beta_{FT1}$ and `info_dict[mse_mod_mean][0]`$=\n            \\\\frac{\\\\beta_{FT2}}{\\\\beta_{FT1}}\\\\frac{\\\\Delta T_A}{\\\\overline{T_{FT}}}$ would be the only non-zero\n            prefactors.\n            Units of prefactor multiplied by change is *kJ/kg*.\n    \"\"\"\n    temp_ft_anom = temp_ft_quant - temp_ft_mean[:, np.newaxis]\n    delta_temp_ft_anom = temp_ft_anom[1] - temp_ft_anom[0]\n    if temp_ft_anom0 is None:\n        temp_ft_anom0 = temp_ft_anom[0]\n\n    # Parameters needed for taylor expansions - most compute using adiabatic temperature in free troposphere.\n    R_mod, _, _, beta_1, beta_2, beta_3, _ = get_theory_prefactor_terms(temp_ft_mean[0], pressure_surf, pressure_ft)\n\n    # Compute modified MSE - need in units of J/kg at the moment hence multiply by 1000\n    if mse_mod_mean_change is None:\n        mse_mod_mean = moist_static_energy(temp_ft_mean, sphum_sat(temp_ft_mean, pressure_ft), height=0,\n                                           c_p_const=c_p + R_mod) * 1000\n        mse_mod_mean_change = mse_mod_mean[1] - mse_mod_mean[0]\n\n    # Decompose Taylor Expansions - 3 in total\n    # l means linear, s means squared and n means non-linear\n    # first index is for Delta expansion i.e. base climate - quantile about mean\n    # second index is for delta expansion i.e. difference between climates\n    # third index is for conversion between delta_temp_adiabat_mean and delta_mse_mod_mean\n    # I neglect all terms that are more than squared in two or more of these taylor expansions\n    if taylor_terms.lower() not in ['linear', 'non_linear', 'squared_0', 'squared', 'full']:\n        raise ValueError(f'taylor_terms given is {taylor_terms}, but must be linear, squared_0, squared or full.')\n\n    term_ll = beta_1 * delta_temp_ft_anom\n    term_lll = beta_2 / beta_1 * temp_ft_anom0 / temp_ft_mean[0] * mse_mod_mean_change\n    if taylor_terms == 'squared_0':\n        # term_sl = beta_2 * temp_adiabat_anom[0] / temp_adiabat_mean[0] * delta_temp_adiabat_anom\n        term_sl = beta_2 * temp_ft_anom0 / temp_ft_mean[0] * delta_temp_ft_anom\n        term_sll = 0.5 * beta_3 / beta_1 * (temp_ft_anom0 / temp_ft_mean[0]) ** 2 * mse_mod_mean_change\n        term_lls = 0\n        term_lsl = 0\n        term_lnl = 0\n        term_snl = 0\n    elif 'linear' not in taylor_terms:\n        term_sl = beta_2 * temp_ft_anom0 / temp_ft_mean[0] * delta_temp_ft_anom\n        term_sll = 0.5 * beta_3 / beta_1 * (temp_ft_anom0 / temp_ft_mean[0]) ** 2 * mse_mod_mean_change\n        term_lls = -0.5 * beta_2 ** 2 / beta_1 ** 3 * temp_ft_anom0 / temp_ft_mean[\n            0] ** 2 * mse_mod_mean_change ** 2\n        term_lsl = 0.5 * beta_3 / beta_1 ** 2 * temp_ft_anom0 / temp_ft_mean[0] ** 2 * mse_mod_mean_change ** 2\n        term_lnl = beta_2 / beta_1 / temp_ft_mean[0] * delta_temp_ft_anom * mse_mod_mean_change\n        term_snl = beta_3 / beta_1 * temp_ft_anom0 / temp_ft_mean[\n            0] ** 2 * delta_temp_ft_anom * mse_mod_mean_change\n    else:\n        term_sl = 0\n        term_sll = 0\n        term_lls = 0\n        term_lsl = 0\n        term_lnl = 0 if taylor_terms == 'linear' else beta_2 / beta_1 / temp_ft_mean[\n            0] * delta_temp_ft_anom * mse_mod_mean_change\n        term_snl = 0\n    # Extra squared-squared terms\n    if taylor_terms == 'full':\n        term_lss = -0.5 * beta_3 * beta_2 / beta_1 ** 4 * temp_ft_anom0 / temp_ft_mean[\n            0] ** 3 * mse_mod_mean_change ** 3\n        term_sls = -0.25 * beta_3 * beta_2 / beta_1 ** 3 * temp_ft_anom0 ** 2 / temp_ft_mean[\n            0] ** 3 * mse_mod_mean_change ** 2\n        # The two below are very small so should exclude\n        # term_ss = 0.5 * beta_2/temp_adiabat_mean[0] * delta_temp_anom**2\n        # term_lns = -0.5 * beta_2**2/beta_1**3/temp_adiabat_mean[0]**2 * delta_temp_anom * delta_mse**2\n    else:\n        term_lss = 0\n        term_sls = 0\n\n    # Keep track of contribution to different changes\n    # Have a prefactor based on current climate and a change between simulations for each factor.\n    info_dict = {'temp_ft_anom': [(term_ll + term_sl) / delta_temp_ft_anom / 1000, delta_temp_ft_anom],\n                 'mse_mod_mean': [(term_lll + term_sll) / mse_mod_mean_change / 1000, mse_mod_mean_change],\n                 'mse_mod_mean_squared': [(term_lls + term_lsl + term_sls) / mse_mod_mean_change ** 2 / 1000,\n                                          mse_mod_mean_change ** 2],\n                 'mse_mod_mean_cubed': [term_lss / mse_mod_mean_change ** 3 / 1000, mse_mod_mean_change ** 3],\n                 'non_linear': [(term_lnl + term_snl) / (delta_temp_ft_anom * mse_mod_mean_change) / 1000,\n                                delta_temp_ft_anom * mse_mod_mean_change]\n                 }\n\n    final_answer = term_ll + term_lll + term_sl + term_lls + term_sll + term_lsl + term_lnl + term_snl + term_lss + \\\n                   term_sls\n    final_answer = final_answer / 1000  # convert to units of kJ/kg\n    return final_answer, info_dict\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.mse_mod_change_surf_expansion","title":"<code>mse_mod_change_surf_expansion(temp_surf, sphum_surf, epsilon, pressure_surf, pressure_ft, taylor_terms='linear', q_sat_s_linear_term_use=None, beta_s1_use=None)</code>","text":"<p>Does a taylor expansion of the change in modified moist static energy, \\(\\delta h^{\\dagger}\\) in terms of surface quantities. This approximates the change of \\(h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon\\) between climates.</p> \\[\\delta h^{\\dagger} \\approx (c_p - R^{\\dagger} + L_v \\alpha_s q_s)\\delta T_s + L_v q_s^* \\delta r_s - \\delta \\epsilon + L_v \\alpha_s q_s^* \\delta T_s \\delta r_s  + 0.5 L_v \\alpha_s q_s (\\alpha_s - 2 / T_s) \\delta T_s^2\\] <p>In terms of \\(\\beta\\) parameters, this can be written as:</p> \\[\\delta h^{\\dagger} \\approx \\beta_{s1} \\delta T_s + L_v q_s^* \\delta r - \\delta \\epsilon + L_v \\alpha_s q_s^* \\delta T_s \\delta r_s + \\frac{1}{2 T_s} \\beta_{s2} \\delta T_s^2\\] <p>The \\(\\delta T_s^2\\) term is only included if <code>taylor_terms == 'squared'</code>. The \\(\\delta T_s \\delta r_s\\) term is only included if <code>taylor_terms</code> is <code>squared</code> or <code>non_linear</code>.</p> <p>Parameters:</p> Name Type Description Default <code>temp_surf</code> <code>ndarray</code> <p><code>float [n_exp]</code> or <code>float [n_exp, n_quant]</code>  Near surface temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K. We assume <code>n_exp=2</code>.</p> required <code>sphum_surf</code> <code>ndarray</code> <p><code>float [n_exp]</code> or <code>float [n_exp, n_quant]</code>  Near surface specific humidity of each simulation. Units: kg/kg.</p> required <code>epsilon</code> <code>ndarray</code> <p><code>float [n_exp]</code> or <code>float [n_exp, n_quant]</code> \\(h_s - h^*_{FT}\\). Units: kJ/kg.</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level in Pa.</p> required <code>taylor_terms</code> <code>str</code> <p>How many taylor series terms to keep in the expansions for changes in modified moist static energy:</p> <ul> <li><code>linear</code>: \\(\\delta h^{\\dagger} \\approx (c_p - R^{\\dagger} + L_v \\alpha_s q_s)\\delta T_s +     L_v \\alpha q_s^* \\delta r_s - \\delta \\epsilon\\)</li> <li><code>non_linear</code>: Includes the additional term \\(L_v \\alpha_s q_s^* \\delta T_s \\delta r_s\\)</li> <li><code>squared</code>: Includes the additional term \\(0.5 L_v \\alpha_s q_s (\\alpha_s - 2 / T_s) \\delta T_s^2\\)</li> </ul> <code>'linear'</code> <code>q_sat_s_linear_term_use</code> <code>Optional[Union[ndarray, float]]</code> <p><code>float</code> or <code>float [n_quant]</code>  Can specify the \\(q^*(T, p_s)\\) value in the \\(L_v q^* \\delta r\\) term. May want to do this if want to approximate \\(q^*(T(x), p_s) \\approx q^*(\\overline{T}, p_s)\\) for this term, so can combine \\(\\delta r(x)\\) and \\(\\delta \\overline{r}\\) terms. If <code>None</code>, then will use <code>sphum_sat(temp_surf[0], pressure_surf)</code>.</p> <code>None</code> <code>beta_s1_use</code> <code>Optional[Union[ndarray, float]]</code> <p><code>float</code> or <code>float [n_quant]</code>  Can specify the \\(\\beta_{s1} = (c_p - R^{\\dagger} + L_v \\alpha q)\\) factor preceeding the \\(\\delta T_s\\) term. May want to do this if want to investigate the approximation \\(\\beta_{s1}(x) \\approx \\overline{\\beta_{s1}}\\).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>delta_mse_mod</code> <code>Union[ndarray, float]</code> <p><code>float</code> or <code>float [n_quant]</code>  Approximation of \\(\\delta h^{\\dagger}\\). Units are kJ/kg.</p> <code>info_dict</code> <code>dict</code> <p>Dictionary containing 5 keys: <code>rh</code>, <code>temp</code>, <code>epsilon</code>, <code>non_linear</code>, <code>temp_squared</code>.  For each key, there is a list with first value being the prefactor and second the change in the expansion. The sum of all these prefactors multiplied by the changes equals the full theory. Units of prefactors are kJ/kg divided by units of the change term.</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def mse_mod_change_surf_expansion(temp_surf: np.ndarray, sphum_surf: np.ndarray, epsilon: np.ndarray,\n                                  pressure_surf: float, pressure_ft: float, taylor_terms: str = 'linear',\n                                  q_sat_s_linear_term_use: Optional[Union[np.ndarray, float]] = None,\n                                  beta_s1_use: Optional[Union[np.ndarray, float]] = None\n                                  ) -&gt; Tuple[Union[np.ndarray, float], dict]:\n    \"\"\"\n    Does a taylor expansion of the change in modified moist static energy, $\\delta h^{\\dagger}$ in terms\n    of surface quantities. This approximates the change of $h^{\\dagger} = (c_p - R^{\\\\dagger})T_s + L_v q_s - \\epsilon$\n    between climates.\n\n    $$\\\\delta h^{\\\\dagger} \\\\approx (c_p - R^{\\\\dagger} + L_v \\\\alpha_s q_s)\\\\delta T_s + L_v q_s^* \\\\delta r_s\n    - \\delta \\epsilon + L_v \\\\alpha_s q_s^* \\delta T_s \\\\delta r_s  +\n    0.5 L_v \\\\alpha_s q_s (\\\\alpha_s - 2 / T_s) \\\\delta T_s^2$$\n\n    In terms of $\\\\beta$ parameters, this can be written as:\n\n    $$\\\\delta h^{\\\\dagger} \\\\approx \\\\beta_{s1} \\delta T_s + L_v q_s^* \\\\delta r - \\delta \\epsilon +\n    L_v \\\\alpha_s q_s^* \\delta T_s \\\\delta r_s + \\\\frac{1}{2 T_s} \\\\beta_{s2} \\delta T_s^2$$\n\n    The $\\delta T_s^2$ term is only included if `taylor_terms == 'squared'`.\n    The $\\delta T_s \\delta r_s$ term is only included if `taylor_terms` is `squared` or `non_linear`.\n\n    Args:\n        temp_surf: `float [n_exp]` or `float [n_exp, n_quant]` &lt;/br&gt;\n            Near surface temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*. We assume `n_exp=2`.\n        sphum_surf: `float [n_exp]` or `float [n_exp, n_quant]` &lt;/br&gt;\n            Near surface specific humidity of each simulation. Units: *kg/kg*.\n        epsilon: `float [n_exp]` or `float [n_exp, n_quant]` &lt;/br&gt;\n            $h_s - h^*_{FT}$. Units: *kJ/kg*.\n        pressure_surf:\n            Pressure at near-surface in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level in *Pa*.\n        taylor_terms:\n            How many taylor series terms to keep in the expansions for changes in modified moist static energy:\n\n            * `linear`: $\\\\delta h^{\\\\dagger} \\\\approx (c_p - R^{\\\\dagger} + L_v \\\\alpha_s q_s)\\\\delta T_s +\n                L_v \\\\alpha q_s^* \\\\delta r_s - \\delta \\epsilon$\n            * `non_linear`: Includes the additional term $L_v \\\\alpha_s q_s^* \\delta T_s \\\\delta r_s$\n            * `squared`: Includes the additional term $0.5 L_v \\\\alpha_s q_s (\\\\alpha_s - 2 / T_s) \\\\delta T_s^2$\n        q_sat_s_linear_term_use: `float` or `float [n_quant]` &lt;/br&gt;\n            Can specify the $q^*(T, p_s)$ value in the $L_v q^* \\\\delta r$ term. May want to do\n            this if want to approximate $q^*(T(x), p_s) \\\\approx q^*(\\overline{T}, p_s)$ for this term, so can combine\n            $\\\\delta r(x)$ and $\\\\delta \\overline{r}$ terms. If `None`, then will use\n            `sphum_sat(temp_surf[0], pressure_surf)`.\n        beta_s1_use: `float` or `float [n_quant]` &lt;/br&gt;\n            Can specify the $\\\\beta_{s1} = (c_p - R^{\\\\dagger} + L_v \\\\alpha q)$ factor preceeding the $\\delta T_s$\n            term. May want to do this if want to investigate the approximation\n            $\\\\beta_{s1}(x) \\\\approx \\overline{\\\\beta_{s1}}$.\n\n    Returns:\n        delta_mse_mod: `float` or `float [n_quant]` &lt;/br&gt;\n            Approximation of $\\delta h^{\\\\dagger}$. Units are *kJ/kg*.\n        info_dict: Dictionary containing 5 keys: `rh`, `temp`, `epsilon`, `non_linear`, `temp_squared`. &lt;/br&gt;\n            For each key, there is a list with first value being the prefactor and second the change in the expansion.\n            The sum of all these prefactors multiplied by the changes equals the full theory.\n            Units of prefactors are *kJ/kg* divided by units of the change term.\n    \"\"\"\n    _, _, _, beta_s1, beta_s2, _, _ = get_theory_prefactor_terms(temp_surf[0], pressure_surf, pressure_ft,\n                                                                 sphum_surf[0])\n\n    delta_temp = temp_surf[1] - temp_surf[0]\n    rh = sphum_surf / sphum_sat(temp_surf, pressure_surf)\n    delta_rh = rh[1] - rh[0]\n    delta_epsilon = epsilon[1] - epsilon[0]\n\n    q_sat_s = sphum_sat(temp_surf[0], pressure_surf)\n    alpha_s = clausius_clapeyron_factor(temp_surf[0], pressure_surf)\n\n    if q_sat_s_linear_term_use is None:\n        coef_rh = L_v * q_sat_s\n    else:\n        coef_rh = L_v * q_sat_s_linear_term_use\n    coef_temp = beta_s1 if beta_s1_use is None else beta_s1_use\n\n    if taylor_terms == 'squared':\n        # Add extra term in taylor expansion of delta_mse_mod if requested\n        coef_non_linear = L_v * q_sat_s * alpha_s\n        coef_temp_squared = 0.5 * beta_s2 / temp_surf[0]\n    elif taylor_terms == 'non_linear':\n        coef_non_linear = L_v * q_sat_s * alpha_s\n        coef_temp_squared = sphum_surf[0] * 0\n    elif taylor_terms == 'linear':\n        coef_non_linear = sphum_surf[0] * 0\n        coef_temp_squared = sphum_surf[0] * 0  # set to 0 for all values\n    else:\n        raise ValueError(f\"taylor_terms given is {taylor_terms}, but must be 'linear', 'non_linear' or 'squared'\")\n\n    final_answer = (coef_rh * delta_rh + coef_temp * delta_temp + coef_non_linear * delta_rh * delta_temp +\n                    coef_temp_squared * delta_temp ** 2) / 1000 - delta_epsilon\n    info_dict = {'rh': [coef_rh / 1000, delta_rh], 'temp': [coef_temp / 1000, delta_temp],\n                 'epsilon': [-1, delta_epsilon],\n                 'non_linear': [coef_non_linear / 1000, delta_temp * delta_rh],\n                 'temp_squared': [coef_temp_squared / 1000, delta_temp ** 2]}\n    return final_answer, info_dict\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.temp_adiabat_fit_func","title":"<code>temp_adiabat_fit_func(temp_ft_adiabat, temp_surf, sphum_surf, pressure_surf, pressure_ft, epsilon=0)</code>","text":"<p>Adiabatic Free Troposphere temperature, \\(T_{A,FT}\\), is defined such that surface moist static energy, \\(h\\) is equal to the saturated moist static energy, \\(h^*\\), evaluated at \\(T_A\\) and free troposphere pressure, \\(p_{FT}\\) i.e. \\(h(T_s, q_s, p_s) = h^*(T_{A}, p_{FT})\\).</p> <p>This develops this to the more general case, to find \\(T_A\\) such that \\(h(T_s, q_s, p_s) = h^*(T_{A}, p_{FT}) + \\epsilon\\), where \\(\\epsilon\\) quantifies the CAPE.</p> <p>Using the following approximate relationship between \\(z_A\\) and \\(T_A\\):</p> \\[z_A - z_s \\approx \\frac{R^{\\dagger}}{g}(T_s + T_A)\\] <p>where \\(R^{\\dagger} = \\ln(p_s/p_{FT})/2\\), we can obtain \\(T_A\\) by solving the following equation for modified MSE, \\(h^{\\dagger}\\):</p> \\[h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon \\approx (c_p + R^{\\dagger})T_A + L_vq^*(T_A, p_{FT})\\] <p>Where the temperature differs from the adiabatic temperature if \\(\\epsilon \\neq 0\\). This function returns the LHS minus the RHS of this equation to then give to <code>scipy.optimize.fsolve</code> to find \\(T_A\\).</p> <p>Parameters:</p> Name Type Description Default <code>temp_ft_adiabat</code> <code>float</code> <p>float Adiabatic temperature at <code>pressure_ft</code> in Kelvin.</p> required <code>temp_surf</code> <code>float</code> <p>Actual temperature at <code>pressure_surf</code> in Kelvin.</p> required <code>sphum_surf</code> <code>float</code> <p>Actual specific humidity at <code>pressure_surf</code> in kg/kg.</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level in Pa.</p> required <code>epsilon</code> <code>float</code> <p>Proxy for CAPE in kJ/kg. Quantifies how much larger near-surface MSE is than free tropospheric saturated.</p> <code>0</code> <p>Returns:</p> Type Description <code>float</code> <p>MSE discrepancy: difference between surface and free troposphere saturated adiabatic MSE.</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def temp_adiabat_fit_func(temp_ft_adiabat: float, temp_surf: float, sphum_surf: float,\n                          pressure_surf: float, pressure_ft: float, epsilon: float = 0) -&gt; float:\n    \"\"\"\n    Adiabatic Free Troposphere temperature, $T_{A,FT}$, is defined such that surface moist static energy, $h$\n    is equal to the saturated moist static energy, $h^*$, evaluated at $T_A$ and free troposphere pressure,\n    $p_{FT}$ i.e. $h(T_s, q_s, p_s) = h^*(T_{A}, p_{FT})$.\n\n    This develops this to the more general case, to find $T_A$ such that\n    $h(T_s, q_s, p_s) = h^*(T_{A}, p_{FT}) + \\epsilon$, where $\\epsilon$ quantifies the CAPE.\n\n    Using the following approximate relationship between $z_A$ and $T_A$:\n\n    $$z_A - z_s \\\\approx \\\\frac{R^{\\\\dagger}}{g}(T_s + T_A)$$\n\n    where $R^{\\dagger} = \\\\ln(p_s/p_{FT})/2$, we can obtain $T_A$\n    by solving the following equation for modified MSE, $h^{\\\\dagger}$:\n\n    $$h^{\\\\dagger} = (c_p - R^{\\\\dagger})T_s + L_v q_s - \\epsilon \\\\approx\n    (c_p + R^{\\\\dagger})T_A + L_vq^*(T_A, p_{FT})$$\n\n    Where the temperature differs from the adiabatic temperature if $\\epsilon \\\\neq 0$.\n    This function returns the LHS minus the RHS of this equation to then give to `scipy.optimize.fsolve` to find\n    $T_A$.\n\n    Args:\n        temp_ft_adiabat: float\n            Adiabatic temperature at `pressure_ft` in Kelvin.\n        temp_surf:\n            Actual temperature at `pressure_surf` in Kelvin.\n        sphum_surf:\n            Actual specific humidity at `pressure_surf` in *kg/kg*.\n        pressure_surf:\n            Pressure at near-surface in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level in *Pa*.\n        epsilon:\n            Proxy for CAPE in *kJ/kg*. Quantifies how much larger near-surface MSE is than free tropospheric saturated.\n\n    Returns:\n        MSE discrepancy: difference between surface and free troposphere saturated adiabatic MSE.\n    \"\"\"\n    R_mod = R * np.log(pressure_surf / pressure_ft) / 2\n    mse_mod_surf = moist_static_energy(temp_surf, sphum_surf, height=0, c_p_const=c_p - R_mod) - epsilon\n    mse_mod_ft = moist_static_energy(temp_ft_adiabat, sphum_sat(temp_ft_adiabat, pressure_ft), height=0,\n                                     c_p_const=c_p + R_mod)\n    return mse_mod_surf - mse_mod_ft\n</code></pre>"},{"location":"code/thesis/adiabat_theory/#isca_tools.thesis.adiabat_theory.temp_adiabat_surf_fit_func","title":"<code>temp_adiabat_surf_fit_func(temp_surf_adiabat, temp_ft, rh_surf, z_ft, pressure_surf, pressure_ft, epsilon=0)</code>","text":"<p>Adiabatic Near-surface temperature, \\(T_{A,s}\\), is defined such that surface moist static energy, \\(h\\) evaluated at \\(T_{A, s}\\) is equal to the saturated free tropospheric moist static energy, \\(h^*\\), and free troposphere pressure, \\(p_{FT}\\) i.e. \\(h(T_{A,s}, r_s, p_s) = h^*(T_{FT}, z_{FT}, p_{FT})\\).</p> <p>This develops this to the more general case, to find \\(T_{A, s}\\) such that \\(h(T_{A,s}, r_s, p_s) = h^*(T_{FT}, z_{FT}, p_{FT}) + \\epsilon\\), where \\(\\epsilon\\) quantifies the CAPE.</p> <p>Parameters:</p> Name Type Description Default <code>temp_surf_adiabat</code> <code>float</code> <p>float Adiabatic temperature at <code>pressure_surf</code> in Kelvin.</p> required <code>temp_ft</code> <code>float</code> <p>Actual temperature at <code>pressure_ft</code> in Kelvin.</p> required <code>rh_surf</code> <code>float</code> <p>Actual relative humidity at <code>pressure_surf</code>.</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level in Pa.</p> required <code>epsilon</code> <code>float</code> <p>Proxy for CAPE in kJ/kg. Quantifies how much larger near-surface MSE is than free tropospheric saturated.</p> <code>0</code> <p>Returns:</p> Type Description <code>float</code> <p>MSE discrepancy: difference between adiabatic surface and free troposphere saturated MSE.</p> Source code in <code>isca_tools/thesis/adiabat_theory.py</code> <pre><code>def temp_adiabat_surf_fit_func(temp_surf_adiabat: float, temp_ft: float, rh_surf: float, z_ft: Optional[float],\n                               pressure_surf: float, pressure_ft: float, epsilon: float = 0) -&gt; float:\n    \"\"\"\n    Adiabatic Near-surface temperature, $T_{A,s}$, is defined such that surface moist static energy, $h$\n    evaluated at $T_{A, s}$ is equal to the saturated free tropospheric moist static energy, $h^*$,\n    and free troposphere pressure, $p_{FT}$ i.e. $h(T_{A,s}, r_s, p_s) = h^*(T_{FT}, z_{FT}, p_{FT})$.\n\n    This develops this to the more general case, to find $T_{A, s}$ such that\n    $h(T_{A,s}, r_s, p_s) = h^*(T_{FT}, z_{FT}, p_{FT}) + \\epsilon$, where $\\epsilon$ quantifies the CAPE.\n\n    Args:\n        temp_surf_adiabat: float\n            Adiabatic temperature at `pressure_surf` in Kelvin.\n        temp_ft:\n            Actual temperature at `pressure_ft` in Kelvin.\n        rh_surf:\n            Actual relative humidity at `pressure_surf`.\n        pressure_surf:\n            Pressure at near-surface in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level in *Pa*.\n        epsilon:\n            Proxy for CAPE in *kJ/kg*. Quantifies how much larger near-surface MSE is than free tropospheric saturated.\n\n    Returns:\n        MSE discrepancy: difference between adiabatic surface and free troposphere saturated MSE.\n    \"\"\"\n    if z_ft is None:\n        # Compute z_ft using from temperatures\n        R_mod = R * np.log(pressure_surf / pressure_ft) / 2\n        mse_surf = moist_static_energy(temp_surf_adiabat, rh_surf * sphum_sat(temp_surf_adiabat, pressure_surf),\n                                       height=0, c_p_const=c_p - R_mod)\n        mse_sat_ft = moist_static_energy(temp_ft, sphum_sat(temp_ft, pressure_ft), height=0,\n                                         c_p_const=c_p + R_mod) + epsilon\n    else:\n        mse_surf = moist_static_energy(temp_surf_adiabat, rh_surf * sphum_sat(temp_surf_adiabat, pressure_surf),\n                                       height=0)\n        mse_sat_ft = moist_static_energy(temp_ft, sphum_sat(temp_ft, pressure_ft), height=z_ft) + epsilon\n    return mse_surf - mse_sat_ft\n</code></pre>"},{"location":"code/thesis/adiabat_theory2/","title":"Adiabat Theory 2","text":""},{"location":"code/thesis/adiabat_theory2/#isca_tools.thesis.adiabat_theory2.decompose_var_x_change","title":"<code>decompose_var_x_change(var_x, var_p, quant_p=np.arange(101, dtype=int), simple=True)</code>","text":"<p>We can decompose the change in variable \\(\\chi\\), conditioned on near-surface temperature percentile \\(x\\) into the change in the corresponding percentile of \\(\\chi\\): \\(p_x\\), but accounting for how \\(p_x\\) changes with warming:</p> <p>\\(\\delta \\chi[x] = \\delta \\chi(p_x) + [\\chi(p_x+\\delta p_x) - \\chi(p_x)] + [\\delta \\chi(p_x+\\delta p_x) - \\delta \\chi(p_x)]\\)</p> <p>where:</p> <ul> <li>\\(p_x\\) is defined such that \\(\\chi[x] = \\chi(p_x)\\).</li> <li>\\(\\delta \\chi(p_x) = \\chi^{hot}(p^{cold}_x) - \\chi^{cold}(p^{cold}_x)\\) i.e. keep \\(p_x\\) constant, at its value     in the colder simulation.</li> <li>\\(\\delta \\chi(p_x)\\) is the contribution due to change in the distribution of \\(\\chi\\) with warming, neglecting     change in percentile.</li> <li>\\(\\chi(p_x+\\delta p_x) - \\chi(p_x)\\) is the contribution due to change in percentile, neglecting change in     distribution of \\(\\chi\\).</li> <li>\\(\\delta \\chi(p_x+\\delta p_x) - \\delta \\chi(p_x)\\) is the non-linear contribution, influenced by both changes     in the distribution and percentile of \\(\\chi\\).</li> </ul> <p>Keeping only the first two linear terms on the RHS, provides a good approximation. This is achieved by setting <code>simple=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>var_x</code> <code>ndarray</code> <p><code>float [n_exp, n_quant_x]</code> Variable \\(\\chi\\) conditioned on near-surface temperature percentile, \\(x\\), for each experiment: \\(\\chi[x]\\). \\(x\\) can differ from <code>quant_px</code>, but likely to be the same: <code>np.arange(1, 100)</code>.</p> required <code>var_p</code> <code>ndarray</code> <p><code>float [n_exp, n_quant_p]</code> <code>var_p[i, j]</code> is the \\(p=\\)<code>quant_p[j]</code>\\(^{th}\\) percentile of variable \\(\\chi\\) for experiment <code>i</code>: \\(\\chi(p)\\).</p> required <code>quant_p</code> <code>ndarray</code> <p><code>float [n_quant_p]</code> Corresponding quantiles to <code>var_p</code>.</p> <code>arange(101, dtype=int)</code> <code>simple</code> <code>bool</code> <p>If <code>True</code>, <code>var_x_change_theory</code> will be \\(\\delta \\chi(p_x) + [\\chi(p_x+\\delta p_x) - \\chi(p_x)]\\). If <code>False</code>, will also include \\(\\delta \\chi(p_x+\\delta p_x) - \\delta \\chi(p_x)\\), and will exactly match <code>var_x_change</code>.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>var_x_change</code> <code>ndarray</code> <p><code>float [n_quant_x]</code> Simulated \\(\\delta \\chi[x]\\)</p> <code>var_x_change_theory</code> <code>ndarray</code> <p><code>float [n_quant_x]</code> Theoretical \\(\\delta \\chi[x]\\)</p> <code>var_x_change_cont</code> <code>dict</code> <p>Dictionary recording the five terms in the theory for \\(\\delta \\chi[x]\\). The sum of all these terms should match the simulated <code>var_x_change</code>.</p> <ul> <li><code>dist</code>: \\(\\delta \\chi(p_x)\\)</li> <li><code>p_x</code>: \\(\\chi(p_x+\\delta p_x) - \\chi(p_x)\\)</li> <li><code>nl</code>: \\(\\delta \\chi(p_x+\\delta p_x) - \\delta \\chi(p_x)\\)</li> </ul> Source code in <code>isca_tools/thesis/adiabat_theory2.py</code> <pre><code>def decompose_var_x_change(var_x: np.ndarray, var_p: np.ndarray,\n                           quant_p: np.ndarray = np.arange(101, dtype=int), simple: bool = True\n                           ) -&gt; Tuple[np.ndarray, np.ndarray, dict]:\n    \"\"\"\n    We can decompose the change in variable $\\chi$, conditioned on near-surface temperature\n    percentile $x$ into the change in the corresponding percentile of $\\chi$: $p_x$, but accounting\n    for how $p_x$ changes with warming:\n\n    $\\delta \\chi[x] = \\delta \\chi(p_x) + [\\chi(p_x+\\delta p_x) - \\chi(p_x)]\n    + [\\delta \\chi(p_x+\\delta p_x) - \\delta \\chi(p_x)]$\n\n    where:\n\n    * $p_x$ is defined such that $\\chi[x] = \\chi(p_x)$.\n    * $\\delta \\chi(p_x) = \\chi^{hot}(p^{cold}_x) - \\chi^{cold}(p^{cold}_x)$ i.e. keep $p_x$ constant, at its value\n        in the colder simulation.\n    * $\\delta \\chi(p_x)$ is the contribution due to change in the distribution of $\\chi$ with warming, neglecting\n        change in percentile.\n    * $\\chi(p_x+\\delta p_x) - \\chi(p_x)$ is the contribution due to change in percentile, neglecting change in\n        distribution of $\\chi$.\n    * $\\delta \\chi(p_x+\\delta p_x) - \\delta \\chi(p_x)$ is the non-linear contribution, influenced by both changes\n        in the distribution and percentile of $\\chi$.\n\n    Keeping only the first two linear terms on the RHS, provides a good approximation.\n    This is achieved by setting `simple=True`.\n\n    Args:\n        var_x: `float [n_exp, n_quant_x]`&lt;/br&gt;\n            Variable $\\chi$ conditioned on near-surface temperature percentile, $x$, for each\n            experiment: $\\chi[x]$. $x$ can differ from `quant_px`, but likely to be the same: `np.arange(1, 100)`.\n        var_p: `float [n_exp, n_quant_p]`&lt;/br&gt;\n            `var_p[i, j]` is the $p=$`quant_p[j]`$^{th}$ percentile of variable $\\chi$ for\n            experiment `i`: $\\chi(p)$.\n        quant_p: `float [n_quant_p]`&lt;/br&gt;\n            Corresponding quantiles to `var_p`.\n        simple: If `True`, `var_x_change_theory` will be\n            $\\delta \\chi(p_x) + [\\chi(p_x+\\delta p_x) - \\chi(p_x)]$. If `False`, will also include\n            $\\delta \\chi(p_x+\\delta p_x) - \\delta \\chi(p_x)$, and will exactly match `var_x_change`.\n\n    Returns:\n        var_x_change: `float [n_quant_x]`&lt;/br&gt;\n            Simulated $\\delta \\chi[x]$\n        var_x_change_theory: `float [n_quant_x]`&lt;/br&gt;\n            Theoretical $\\delta \\chi[x]$\n        var_x_change_cont: Dictionary recording the five terms in the theory for $\\delta \\chi[x]$.\n            The sum of all these terms should match the simulated `var_x_change`.\n\n            * `dist`: $\\delta \\chi(p_x)$\n            * `p_x`: $\\chi(p_x+\\delta p_x) - \\chi(p_x)$\n            * `nl`: $\\delta \\chi(p_x+\\delta p_x) - \\delta \\chi(p_x)$\n\n    \"\"\"\n    n_exp, n_quant_x = var_x.shape\n    # Get FT percentile corresponding to each FT temperature conditioned on near-surface percentile\n    p_x = np.zeros((n_exp, n_quant_x))\n    for i in range(n_exp):\n        p_x[i] = get_p_x(var_x[i], var_p[i], quant_p)[0]\n\n    # Interpolation so can use p_x which are not integers\n    var_p_interp_func = [scipy.interpolate.interp1d(quant_p, var_p[i], bounds_error=True) for i in range(n_exp)]\n    # Sanity check that interpolation function works\n    for i in range(n_exp):\n        if not np.allclose(var_p_interp_func[i](p_x[i]), var_x[i]):\n            raise ValueError(f'Error in interpolation for experiment {i}: maybe p_x outside 0-100 range')\n\n\n    # Isolate x dependence into different terms\n    # Use var_x sometimes and var_p sometimes, so sum of these contributions exactly equal var_x_change\n    # I.e. exploit that var_x[0] = var_p_interp_func[0](p_x[0]) and var_x[1] = var_p_interp_func[1](p_x[1])\n    var_x_change = var_x[1] - var_x[0]\n    var_x_change_cont = {'dist': var_p_interp_func[1](p_x[0]) - var_x[0],\n                         'p_x': var_p_interp_func[0](p_x[1]) - var_x[0],\n                         'nl': var_x[1] - var_p_interp_func[0](p_x[1]) -\n                               (var_p_interp_func[1](p_x[0]) - var_x[0])}\n\n    # Theory is sum of terms\n    var_x_change_theory = var_x_change_cont['dist'] + var_x_change_cont['p_x']\n    if not simple:\n        var_x_change_theory = var_x_change_theory + \\\n                                var_x_change_cont['nl']\n\n    return var_x_change, var_x_change_theory, var_x_change_cont\n</code></pre>"},{"location":"code/thesis/adiabat_theory2/#isca_tools.thesis.adiabat_theory2.decompose_var_x_change_integrate","title":"<code>decompose_var_x_change_integrate(var_av, var_x, var_p, quant_p=np.arange(100, dtype=int), simple=True)</code>","text":"<p>We can decompose the change in variable \\(\\chi\\), conditioned on near-surface temperature percentile \\(x\\) into the change in the corresponding percentile of \\(\\chi\\): \\(p_x\\), but accounting for how \\(p_x\\) changes with warming:</p> <p>\\(\\delta \\chi[x] \\approx \\delta \\chi(p_x) + \\overline{\\eta}\\delta p_x + \\Delta \\eta(p_x)\\delta p_x + \\delta \\eta(p_x) \\delta p_x\\)</p> <p>where:</p> <ul> <li>\\(p_x\\) is defined such that \\(\\chi[x] = \\chi(p_x)\\) and \\(\\overline{p}\\) such that \\(\\overline{\\chi} = \\chi[\\overline{p}]\\).</li> <li>\\(\\eta(p_x) = \\frac{\\partial \\chi}{\\partial p}\\bigg|_{p_x}\\);     \\(\\overline{\\eta} = \\frac{\\partial \\chi}{\\partial p}\\bigg|_{\\overline{p}}\\) and     \\(\\Delta \\eta(p_x) = \\eta(p_x) - \\overline{\\eta}\\).</li> <li>\\(\\delta \\chi(p_x) = \\chi^{hot}(p^{cold}_x) - \\chi^{cold}(p^{cold}_x)\\) i.e. keep \\(p_x\\) constant, at its value     in the colder simulation.</li> </ul> <p>The only approximation in the above is saying that \\(\\eta(p) + \\delta \\eta(p)\\) is constant between \\(p=p_x\\) and \\(p=p_x+\\delta p_x\\). Keeping only the first two terms on the RHS, also provides a good approximation. This is achieved by setting <code>simple=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>var_av</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average of variable \\(\\chi\\) for each experiment, ikely to be mean or median.</p> required <code>var_x</code> <code>ndarray</code> <p><code>float [n_exp, n_quant_x]</code> Variable \\(\\chi\\) conditioned on near-surface temperature percentile, \\(x\\), for each experiment: \\(\\chi[x]\\). \\(x\\) can differ from <code>quant_px</code>, but likely to be the same: <code>np.arange(1, 100)</code>.</p> required <code>var_p</code> <code>ndarray</code> <p><code>float [n_exp, n_quant_p]</code> <code>var_p[i, j]</code> is the \\(p=\\)<code>quant_p[j]</code>\\(^{th}\\) percentile of variable \\(\\chi\\) for experiment <code>i</code>: \\(\\chi(p)\\).</p> required <code>quant_p</code> <code>ndarray</code> <p><code>float [n_quant_p]</code> Corresponding quantiles to <code>var_p</code>.</p> <code>arange(100, dtype=int)</code> <code>simple</code> <code>bool</code> <p>If <code>True</code>, <code>temp_ft_change_theory</code> will be \\(\\delta \\chi(p_x) + \\overline{\\eta}\\delta p_x\\). If <code>False</code>, will also include \\(\\Delta \\eta(p_x) \\delta p_x + \\delta \\eta(p_x) \\delta p_x\\).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>var_x_change</code> <code>ndarray</code> <p><code>float [n_quant_x]</code> Simulated \\(\\delta \\chi[x]\\)</p> <code>var_x_change_theory</code> <code>ndarray</code> <p><code>float [n_quant_x]</code> Theoretical \\(\\delta \\chi[x]\\)</p> <code>var_x_change_cont</code> <code>dict</code> <p>Dictionary recording the five terms in the theory for \\(\\delta \\Delta \\chi(x)\\). The sum of all these terms should match the simulated <code>var_x_change</code>.</p> <ul> <li><code>var_p</code>: \\(\\delta \\chi(p_x)\\)</li> <li><code>p_x</code>: \\(\\overline{\\eta}\\delta p_x\\)</li> <li><code>nl_eta0</code>: \\(\\Delta \\eta(p_x) \\delta p_x\\)</li> <li><code>nl_change</code>: \\(\\delta \\eta(p_x) \\delta p_x\\)</li> <li><code>approx_integral</code>: Accounts for approximation made during the integral:     \\(\\int_{p_x}^{p_x + \\delta p_x} \\eta(p) + \\delta \\eta(p) dp -     (\\eta(p_x) + \\delta \\eta(p_x))\\delta p_x\\)</li> </ul> Source code in <code>isca_tools/thesis/adiabat_theory2.py</code> <pre><code>def decompose_var_x_change_integrate(var_av: np.ndarray, var_x: np.ndarray, var_p: np.ndarray,\n                                     quant_p: np.ndarray = np.arange(100, dtype=int), simple: bool = True\n                                     ) -&gt; Tuple[np.ndarray, np.ndarray, dict]:\n    \"\"\"\n    We can decompose the change in variable $\\chi$, conditioned on near-surface temperature\n    percentile $x$ into the change in the corresponding percentile of $\\chi$: $p_x$, but accounting\n    for how $p_x$ changes with warming:\n\n    $\\delta \\chi[x] \\\\approx \\delta \\chi(p_x) + \\overline{\\eta}\\delta p_x +\n    \\Delta \\eta(p_x)\\delta p_x + \\delta \\eta(p_x) \\delta p_x$\n\n    where:\n\n    * $p_x$ is defined such that $\\chi[x] = \\chi(p_x)$ and $\\overline{p}$ such that\n    $\\overline{\\chi} = \\chi[\\overline{p}]$.\n    * $\\eta(p_x) = \\\\frac{\\\\partial \\chi}{\\\\partial p}\\\\bigg|_{p_x}$;\n        $\\overline{\\eta} = \\\\frac{\\\\partial \\chi}{\\\\partial p}\\\\bigg|_{\\overline{p}}$ and\n        $\\Delta \\eta(p_x) = \\eta(p_x) - \\overline{\\eta}$.\n    * $\\delta \\chi(p_x) = \\chi^{hot}(p^{cold}_x) - \\chi^{cold}(p^{cold}_x)$ i.e. keep $p_x$ constant, at its value\n        in the colder simulation.\n\n    The only approximation in the above is saying that $\\eta(p) + \\delta \\eta(p)$ is constant between\n    $p=p_x$ and $p=p_x+\\delta p_x$.\n    Keeping only the first two terms on the RHS, also provides a good approximation.\n    This is achieved by setting `simple=True`.\n\n    Args:\n        var_av: `float [n_exp]`&lt;/br&gt;\n            Average of variable $\\chi$ for each experiment, ikely to be mean or median.\n        var_x: `float [n_exp, n_quant_x]`&lt;/br&gt;\n            Variable $\\chi$ conditioned on near-surface temperature percentile, $x$, for each\n            experiment: $\\chi[x]$. $x$ can differ from `quant_px`, but likely to be the same: `np.arange(1, 100)`.\n        var_p: `float [n_exp, n_quant_p]`&lt;/br&gt;\n            `var_p[i, j]` is the $p=$`quant_p[j]`$^{th}$ percentile of variable $\\chi$ for\n            experiment `i`: $\\chi(p)$.\n        quant_p: `float [n_quant_p]`&lt;/br&gt;\n            Corresponding quantiles to `var_p`.\n        simple: If `True`, `temp_ft_change_theory` will be\n            $\\delta \\chi(p_x) + \\overline{\\eta}\\delta p_x$. If `False`, will also include\n            $\\Delta \\eta(p_x) \\delta p_x + \\delta \\eta(p_x) \\delta p_x$.\n\n    Returns:\n        var_x_change: `float [n_quant_x]`&lt;/br&gt;\n            Simulated $\\delta \\chi[x]$\n        var_x_change_theory: `float [n_quant_x]`&lt;/br&gt;\n            Theoretical $\\delta \\chi[x]$\n        var_x_change_cont: Dictionary recording the five terms in the theory for $\\delta \\Delta \\chi(x)$.\n            The sum of all these terms should match the simulated `var_x_change`.\n\n            * `var_p`: $\\delta \\chi(p_x)$\n            * `p_x`: $\\overline{\\eta}\\delta p_x$\n            * `nl_eta0`: $\\Delta \\eta(p_x) \\delta p_x$\n            * `nl_change`: $\\delta \\eta(p_x) \\delta p_x$\n            * `approx_integral`: Accounts for approximation made during the integral:\n                $\\int_{p_x}^{p_x + \\delta p_x} \\eta(p) + \\delta \\eta(p) dp -\n                (\\eta(p_x) + \\delta \\eta(p_x))\\delta p_x$\n\n    \"\"\"\n    n_exp, n_quant_x = var_x.shape\n\n    # Get FT percentile corresponding to each FT temperature conditioned on near-surface percentile\n    p_av_ind = np.zeros(n_exp, dtype=int)\n    p_x = np.zeros((n_exp, n_quant_x))\n    p_x_ind = np.zeros((n_exp, n_quant_x), dtype=int)\n    for i in range(n_exp):\n        p_av_ind[i] = get_p_x(var_av[i], var_p[i], quant_p)[1]\n        p_x[i], p_x_ind[i] = get_p_x(var_x[i], var_p[i], quant_p)\n\n    # Get eta on corresponding to p_x in the reference (coldest) simulation\n    eta_av0 = np.zeros(n_exp)\n    eta_px0 = np.zeros((n_exp, n_quant_x))\n    for i in range(n_exp):\n        eta_use = np.gradient(var_p[i], quant_p)\n        eta_av0[i] = eta_use[p_av_ind[0]]\n        for j in range(n_quant_x):\n            eta_px0[i, j] = eta_use[p_x_ind[0, j]]\n\n    # Isolate x dependence into different terms\n    eta_px0_anom = eta_px0 - eta_av0[:, np.newaxis]\n    var_x_change = var_x[1] - var_x[0]\n    var_x_change_cont = {'var_p': var_p[1, p_x_ind[0]] - var_p[0, p_x_ind[0]],\n                         'p_x': eta_av0[0] * (p_x[1] - p_x[0]),\n                         'nl_eta0': eta_px0_anom[0] * (p_x[1] - p_x[0]),\n                         'nl_change': (eta_px0[1] - eta_px0[0]) * (p_x[1] - p_x[0])}\n\n    # This residual term is due to error in integral approximation where assume integrand constant\n    var_x_change_cont['approx_integral'] = var_x_change - sum(var_x_change_cont.values())\n\n    # Theory is sum of terms\n    var_x_change_theory = var_x_change_cont['var_p'] + var_x_change_cont['p_x']\n    if not simple:\n        var_x_change_theory = var_x_change_theory + \\\n                                var_x_change_cont['nl_eta0'] + var_x_change_cont['nl_change']\n\n    return var_x_change, var_x_change_theory, var_x_change_cont\n</code></pre>"},{"location":"code/thesis/adiabat_theory2/#isca_tools.thesis.adiabat_theory2.get_approx_terms","title":"<code>get_approx_terms(temp_surf_ref, temp_surf_quant, r_ref, r_quant, temp_ft_quant, epsilon_quant, pressure_surf, pressure_ft, epsilon_ref=None, z_approx_ref=None, simple=False, cape_form=False)</code>","text":"<p>Function which returns terms quantifying the errors associated with various approximations, grouped together in \\(A\\) variables that go into the derivation of the theoretical scaling factor, \\(\\delta \\hat{T}_s(x)/\\delta \\tilde{T}_s\\), returned by <code>get_scale_factor_theory</code>.</p> <p>The exact scaling factor is given by:</p> \\[ \\begin{align} \\frac{\\delta T_s(x)}{\\delta \\tilde{T}_s} &amp;= \\frac{\\delta \\hat{T}_s(x)}{\\delta \\tilde{T}_s} + A_{\\delta \\Delta T_{FT}} - A_{\\delta \\Delta T_s} - A_{\\delta r}[x] - A_{\\delta \\Delta T_s \\delta r}[x] \\\\ &amp;+ A_{\\Delta T_s \\Delta r}[x] + A_{\\Delta}[x] + \\tilde{A}_{\\delta} + A_{NL}[x] \\end{align} \\] <p>For more details on the approximations, there is a Jupyter notebook that goes through each step of the derivation.</p> Terms in equation <ul> <li>\\(h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon =     \\left(c_p + R^{\\dagger}\\right) T_{FT} + L_v q^*_{FT} + A_z\\)     where we used an approximate relation to replace \\(z_{FT}\\) in \\(h^*_{FT}\\).</li> <li>\\(\\epsilon = h_s - h^*_{FT}\\), where \\(h_s\\) is near-surface MSE (at \\(p_s\\)) and     \\(h^*_{FT}\\) is free tropospheric saturated MSE (at \\(p_{FT}\\)).</li> <li>\\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\)</li> <li>\\(\\Delta \\chi[x] = \\chi[x] - \\tilde{\\chi}\\)</li> <li>\\(\\chi[x]\\) is the value of \\(\\chi\\) averaged over all days     where near-surface temperature, \\(T_s\\), is between percentile \\(x-0.5\\) and \\(x+0.5\\).</li> <li>\\(\\tilde{\\chi}\\) is the reference value of \\(\\chi\\), which is free to be chosen.</li> <li>\\(\\beta_{FT1} = \\frac{\\partial h^{\\dagger}}{\\partial T_{FT}} = c_p + R^{\\dagger} + L_v \\alpha_{FT} q_{FT}^*\\)</li> <li>\\(\\beta_{FT2} = T_{FT} \\frac{\\partial^2h^{\\dagger}}{\\partial T_{FT}^2} =     T_{FT}\\frac{d\\beta_{FT1}}{d T_{FT}} = L_v \\alpha_{FT} q_{FT}^*(\\alpha_{FT} T_{FT} - 2)\\)</li> <li>\\(\\beta_{s1} = \\frac{\\partial h^{\\dagger}}{\\partial T_s} = c_p - R^{\\dagger} + L_v \\alpha_s q_s\\)</li> <li>\\(\\beta_{s2} = T_s \\frac{\\partial^2 h^{\\dagger}}{\\partial T_s^2} =     T_s\\frac{\\partial \\beta_{s1}}{\\partial T_s} = L_v \\alpha_s q_s(\\alpha_s T_s - 2)\\)</li> <li>\\(\\mu=\\frac{L_v \\alpha_s q_s}{\\beta_{s1}}\\)</li> <li>\\(q = rq^*\\) where \\(q\\) is the specific humidity, \\(r\\) is relative humidity and \\(q^*(T, p)\\)     is saturation specific humidity which is a function of temperature and pressure.</li> <li>\\(\\alpha(T, p)\\) is the clausius clapeyron parameter which is a function of temperature and pressure,     such that \\(\\partial q^*/\\partial T = \\alpha q^*\\).</li> <li>\\(\\Delta h^{\\dagger}_0\\) is referred to as <code>mse_mod_anom0</code> in the code, and is defined through: \\(\\Delta h^{\\dagger}[x] = \\tilde{\\beta}_{s1}\\left(1+\\tilde{\\mu}\\frac{\\Delta r_s[x]}{\\tilde{r}_s}\\right)     \\Delta T_s[x] + L_v \\tilde{q}_s\\frac{\\Delta r_s[x]}{\\tilde{r}_s} - \\Delta \\epsilon[x] + A_{s\\Delta}[x]=     \\Delta h^{\\dagger}_0[x] + A_{s\\Delta}[x]\\)</li> <li>\\(\\delta \\tilde{h}^{\\dagger}_0\\) is referred to as <code>mse_mod_ref_change0</code> in the code, and is defined through: \\(\\delta \\tilde{h}^{\\dagger} =     \\tilde{\\beta}_{s1}\\left(1+\\tilde{\\mu}\\frac{\\delta \\tilde{r}_s}{\\tilde{r}_s}\\right)\\delta \\tilde{T}_s+     L_v \\tilde{q}_s\\frac{\\delta \\tilde{r}_s}{\\tilde{r}_s} - \\delta \\tilde{\\epsilon} + \\tilde{A}_{s\\delta}=     \\delta \\tilde{h}^{\\dagger}_0 + \\tilde{A}_{s\\delta}\\)</li> <li>\\(\\delta \\Delta T_{FT}'[x]\\) is referred to as <code>temp_ft_anom_change_mod</code> in the code, and is defined through: \\(\\tilde{\\beta}_{FT1}\\delta \\Delta T_{FT}'[x] = \\tilde{\\beta}_{FT1} \\delta T_{FT}[x] - \\delta \\tilde{h}^{\\dagger}_0\\)     The idea being that \\(\\delta \\tilde{T}_{FT} \\approx \\delta \\tilde{h}^{\\dagger}_0 / \\tilde{\\beta}_{FT1}\\).</li> </ul> <p>If <code>cape_form=True</code>, \\(A_{\\Delta}\\) will be modified, and an additional term \\(A_{CAPE}[x]\\) is introduced for the equation to remain exact, given that the theoretical scaling factor \\(\\delta \\hat{T}_s(x)/\\delta \\tilde{T}_s\\) is modified.</p> <p>Parameters:</p> Name Type Description Default <code>temp_surf_ref</code> <code>ndarray</code> <p><code>float [n_exp]</code> \\(\\tilde{T}_s\\) Reference near surface temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K. We assume <code>n_exp=2</code>.</p> required <code>temp_surf_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(T_s(x)\\) <code>temp_surf_quant[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface temperature of experiment <code>i</code>. Units: K. Note that <code>quant_use</code> is not provided as not needed by this function, but is likely to be <code>np.arange(1, 100)</code> - leave out <code>x=0</code> as doesn't really make sense to consider \\(0^{th}\\) percentile of a quantity.</p> required <code>r_ref</code> <code>ndarray</code> <p><code>float [n_exp]</code> \\(\\tilde{r}_s\\) Reference near surface relative humidity of each simulation. Units: dimensionless (from 0 to 1).</p> required <code>r_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(r_s[x]\\) <code>r_quant[i, j]</code> is near-surface relative humidity, averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: dimensionless.</p> required <code>temp_ft_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(T_{FT}[x]\\) <code>temp_ft_quant[i, j]</code> is temperature at <code>pressure_ft</code>, averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: kg/kg.</p> required <code>epsilon_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(\\epsilon[x]\\) <code>epsilon_quant[i, j]</code> is \\(\\epsilon = h_s - h^*_{FT}\\), averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: kJ/kg.</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface, \\(p_s\\), in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level, \\(p_{FT}\\), in Pa.</p> required <code>epsilon_ref</code> <code>Optional[ndarray]</code> <p><code>float [n_exp]</code> \\(\\tilde{\\epsilon}_s\\) Reference value of \\(\\epsilon = h_s - h^*_{FT}\\), where \\(h_s\\) is near-surface MSE and \\(h^*_{FT}\\) is saturated MSE at <code>pressure_ft</code>. If not given, weill set to 0. Units: kJ/kg.</p> <code>None</code> <code>z_approx_ref</code> <code>Optional[ndarray]</code> <p><code>float [n_exp]</code> \\(\\tilde{A}_z\\) The exact equation for modified MSE is given by: \\(h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon = (c_p + R^{\\dagger})T_{FT} + L_vq^*(T_{FT}, p_{FT}) + A_z\\) where \\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\) and \\(A_z\\) quantifies the error due to approximation of geopotential height, as relating to temperature. Here you have the option of specifying the reference \\(A_z\\) for each simulation. If not provided, will set to 0. Units: kJ/kg.</p> <code>None</code> <code>simple</code> <code>bool</code> <p>If <code>True</code>, will return approximate values of \\(A_{\\delta \\Delta T_{FT}}\\), \\(A_{\\delta r}[x]\\) and \\(A_{\\Delta T_s \\Delta r}[x]\\). Idea being that these tend to be the most significant, so these approximate values can then be incorporated into theory for scale factor. The approximate terms correspond to non-linear combinations of different physical mechanisms, e.g. combined effect of relative humidity anomaly in current climate and free tropospheric change with warming: \\(\\Delta r_s[x] \\delta \\Delta T_{FT}[x]\\).</p> <code>False</code> <code>cape_form</code> <code>bool</code> <p>If <code>True</code>, \\(A_{\\Delta}\\) will be modified, and an additional term \\(A_{CAPE}[x]\\) is introduced for the equation to remain exact, given that the theoretical scaling factor \\(\\delta \\hat{T}_s(x)/\\delta \\tilde{T}_s\\) is modified.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>approx_terms</code> <code>dict</code> <p>Dictionary containing approximations associated with final scaling factor, \\(\\delta T_s(x)/\\delta \\tilde{T}_s\\) so units are K/K. Terms have been named based on what causes the variation in \\(x\\). Each value in dictionary is a <code>float [n_quant]</code> array, except <code>ref_change</code> which is a <code>float</code>.</p> <ul> <li><code>temp_ft_anom_change</code>: \\(A_{\\delta \\Delta T_{FT}}\\)     Involves contribution from \\(\\delta \\Delta T_{FT}[x]\\).     If <code>simple=True</code>, will return approximate form: \\(A_{\\delta \\Delta T_{FT}} \\tilde{\\beta}_{s1}\\delta \\tilde{T}_s \\approx     \\frac{\\tilde{\\beta}_{FT2}}{2\\tilde{\\beta}_{FT1}} \\left(\\frac{\\delta T_{FT}[x]}{\\tilde{T}_{FT}} -     \\frac{\\delta \\tilde{h}^{\\dagger}_0}{\\tilde{\\beta}_{FT1}\\tilde{T}_{FT}}\\right)     (\\delta \\tilde{h}^{\\dagger}_0 + 2\\Delta \\tilde{h}^{\\dagger}_0[x] + \\tilde{\\beta}_{FT1}\\delta T_{FT}[x])\\)     Note that \\(\\Delta \\tilde{h}^{\\dagger}_0[x]\\) can be decomposed to give relative contributions     of different anomalies in current climate: \\(\\Delta T_s[x], \\Delta r_s[x], \\Delta \\epsilon[x]\\).</li> <li><code>temp_s_anom_change</code>: \\(-A_{\\delta \\Delta T_s}\\)     Involves contribution from \\(\\delta \\Delta T_s(x)\\). Returned value is multiplied by the \\(-1\\);     negative value is because this approx comes from the surface \\(\\delta \\Delta h^{\\dagger}\\) derivation.</li> <li><code>r_change</code>: \\(A_{\\delta r}[x]\\)     Involves contribution from \\(\\delta r_s[x]\\) and \\(\\delta \\tilde{r}_s\\).     Returned value is multiplied by the \\(-1\\);     negative value is because this approx comes from the surface \\(\\delta \\Delta h^{\\dagger}\\) derivation.     If <code>simple=True</code>, will return approximate form: \\(-A_{\\delta r}[x] \\tilde{\\beta}_{s1}\\delta \\tilde{T}_s \\approx -     \\tilde{\\mu}\\tilde{\\beta}_{s1}\\left(\\delta \\tilde{T}_s + \\Delta T_s[x]\\right)     \\frac{\\delta r_s[x]}{\\tilde{r}_s}\\)</li> <li><code>temp_s_anom_r_change</code>: \\(A_{\\delta \\Delta T_s \\delta r}[x]\\)     Involves contribution from \\(\\delta \\Delta T_s(x) \\delta (r_s[x]/\\tilde{r}_s)\\).     Returned value is multiplied by the \\(-1\\);     negative value is because this approx comes from the surface \\(\\delta \\Delta h^{\\dagger}\\) derivation.</li> <li><code>anom_temp_s_r</code>: \\(A_{\\Delta T_s \\Delta r}[x]\\)     Involves contribution from \\(\\Delta T_s(x) \\Delta r_s[x]\\) in the current climate.     If <code>simple=True</code>, will return approximate form: \\(A_{\\Delta T_s \\Delta r} \\tilde{\\beta}_{s1}\\delta \\tilde{T}_s \\approx     \\left[\\frac{\\tilde{\\beta}_{FT2}}{\\tilde{\\beta}_{FT1}}     \\frac{\\delta \\tilde{h}^{\\dagger}_0}{\\tilde{\\beta}_{FT1}\\tilde{T}_{FT}}     \\tilde{\\mu} \\tilde{\\beta}_{s1} \\tilde{T}_s -     \\tilde{\\beta}_{s2}\\delta \\tilde{T}_s\\right]\\frac{\\Delta r_s[x]}{\\tilde{r}_s}     \\frac{\\Delta T_s[x]}{\\tilde{T}_s}\\)</li> <li><code>anom</code>: \\(A_{\\Delta}[x]\\) Groups together errors due to approximation of anomaly in current climate.     Excludes those in \\(A_{\\Delta T_s \\Delta r}\\).     If <code>cape_form=True</code>, will be modified to exclude any contribution from \\(\\Delta \\epsilon\\).</li> <li><code>ref_change</code>: \\(\\tilde{A}_{\\delta}\\)     Groups together errors due to change with warming of the reference day quantities.</li> <li><code>z_anom_change</code>: \\(\\delta \\Delta A_z[x]\\)     Quantifies how error due to approximation of geopotential height changes with warming.</li> <li><code>nl</code>: \\(A_{NL}[x]\\)     Residual non-linear errors that don't directly correspond to any of the above.</li> <li><code>cape</code>: \\(A_{CAPE}[x]\\)     Includes the error contribution which arises due to the conversion between \\(\\epsilon\\) and \\(CAPE\\). \\(A_{CAPE}[x] = \\frac{\\delta \\tilde{\\beta}_{FT1}}{R^{\\dagger}}(\\widetilde{CAPE} + \\delta CAPE[x]) +     \\delta (A_{FT\\Delta,\\epsilon=0}[x]-A_{FT\\Delta}[x]) -     \\frac{\\delta \\tilde{\\beta}_{FT1}}{\\tilde{\\beta}_{FT1}}     \\left(A_{FT\\Delta,\\epsilon=0}[x]-A_{FT\\Delta}[x] - \\tilde{A}_{\\epsilon}\\right)\\)     Will only be included if <code>cape_form=True</code>.</li> </ul> <code>approx</code> <code>dict</code> <p>Approximation, \\(A\\), terms which arise through the derivation of the theory. All have units of J/kg, except <code>ft_beta</code> and <code>s_beta</code> which are both dimensionless:</p> <ul> <li><code>z_ref</code>: <code>float [n_exp]</code>Same as <code>z_approx_ref</code> input, \\(\\tilde{A}_z\\) (set to 0 if not provided).</li> <li><code>z_quant</code>: <code>float [n_exp, n_quant]</code>Error associated with geopotential height approx on quantile day,     \\(A_z[x]\\), computed from provided variables according to     \\(h^{\\dagger}[x] = (c_p - R^{\\dagger})T_s[x] + L_v q_s[x] - \\epsilon = (c_p + R^{\\dagger})T_{FT}[x] + L_vq^*(T_{FT}[x], p_{FT}) + A_z[x]\\)</li> <li><code>z_anom</code>: <code>float [n_exp, n_quant]</code>\\(\\Delta A_z[x] = A_z[x] - \\tilde{A}_z\\).</li> <li><code>ft_anom</code>: <code>float [n_exp, n_quant]</code> Approximation associated with anomaly of \\(h^{\\dagger}\\) at FT level:     \\(A_{FT\\Delta}[x] = \\sum_{n=2}^{\\infty}\\frac{1}{n!}\\frac{\\partial^n h^{\\dagger}}{\\partial T_{FT}^n}(\\Delta T_{FT})^n\\)</li> <li><code>ft_beta</code>: <code>float</code> \\(\\tilde{A}_{FT\\beta}\\) such that     \\(\\delta \\tilde{\\beta}_{FT1} = \\tilde{\\beta}_{FT2}(1 + \\tilde{A}_{FT\\beta})\\frac{\\delta \\tilde{T}_{FT}}{\\tilde{T}_{FT}}\\)     where \\(\\beta_{FT1} = \\frac{\\partial h^{\\dagger}}{\\partial T_{FT}}\\) and     \\(\\beta_{FT2} = T_{FT} \\frac{\\partial^2 h^{\\dagger}}{\\partial T_{FT}^2}\\).</li> <li><code>ft_ref_change</code>: <code>float</code> Approximation associated with change of \\(h^{\\dagger}\\) with warming at FT level:     \\(\\tilde{A}_{FT\\delta} = \\sum_{n=2}^{\\infty}\\frac{1}{n!}\\frac{\\partial^n h^{\\dagger}}{\\partial T_{FT}^n}(\\delta \\tilde{T}_{FT})^n\\)</li> <li><code>s_anom</code>: <code>float [n_exp, n_quant]</code> Approximation associated with anomaly of \\(h^{\\dagger}\\) at surface:     \\(A_{s\\Delta}[x] = (1 + \\frac{\\Delta r_s[x]}{\\tilde{r}_s})A_{s\\Delta T}[x]\\) where     \\(A_{s\\Delta T}[x] = \\sum_{n=2}^{\\infty}\\frac{1}{n!}\\frac{\\partial^n h^{\\dagger}}{\\partial T_{s}^n}(\\Delta T_{s})^n\\)</li> <li><code>s_anom_temp_cont</code>: <code>float [n_exp, n_quant]</code> Temperature only contribution to <code>s_anom</code>:     \\(A_{s\\Delta T}[x] = \\sum_{n=2}^{\\infty}\\frac{1}{n!}\\frac{\\partial^n h^{\\dagger}}{\\partial T_{s}^n}(\\Delta T_{s})^n\\).</li> <li><code>s_anom_temp_r_cont</code>: <code>float [n_exp, n_quant]</code> Temperature and relative humidity non-linear contribution     to <code>s_anom</code>: \\(A_{s\\Delta T \\Delta r}[x] = \\frac{\\Delta r_s[x]}{\\tilde{r}_s}A_{s\\Delta T}[x]\\).</li> <li><code>s_anom_change_temp_cont</code>: <code>float [n_quant]</code> Temperature only contribution due to change in <code>s_anom</code> with warming:     \\(A_{s\\delta \\Delta T}[x] = (1 + \\frac{\\Delta r_s[x]}{\\tilde{r}_s})\\delta A_{s\\Delta T}[x]\\)</li> <li><code>s_anom_change_r_cont</code>: <code>float [n_quant]</code> Relative humidity only contribution due to change in <code>s_anom</code> with warming:     \\(A_{s\\delta \\Delta r}[x] = \\delta (\\frac{\\Delta r_s[x]}{\\tilde{r}_s}) A_{s\\Delta T}[x]\\)</li> <li><code>s_anom_change_temp_r_cont</code>: <code>float [n_quant]</code> Temperature and relative humidity non-linear contribution     due to change in <code>s_anom</code> with warming:     \\(A_{s\\delta \\Delta T \\delta \\Delta r}[x] = \\delta (\\frac{\\Delta r_s[x]}{\\tilde{r}_s}) \\delta A_{s\\Delta T}[x]\\)</li> <li><code>s_beta</code>: <code>float</code> \\(\\tilde{A}_{s\\beta}\\) such that     \\(\\delta \\tilde{\\beta}_{s1} = \\tilde{\\beta}_{s2}(1 + \\tilde{A}_{FT\\beta})\\left(1 + \\frac{\\delta \\tilde{r}_s}{\\tilde{r}_s}\\right)\\frac{\\delta \\tilde{T}_s}{\\tilde{T}_s} + \\tilde{\\mu}\\tilde{\\beta}_{s1} \\frac{\\delta \\tilde{r}_s}{\\tilde{r}_s}\\)</li> <li><code>s_ref_change</code>: <code>float</code> Approximation associated with change of \\(h^{\\dagger}\\) with warming at surface:     \\(\\tilde{A}_{s\\delta}[x] = (1 + \\frac{\\delta \\tilde{r}_s[x]}{\\tilde{r}_s})\\sum_{n=2}^{\\infty}\\frac{1}{n!}\\frac{\\partial^n h^{\\dagger}}{\\partial T_{s}^n}(\\delta \\tilde{T}_{s})^n\\)</li> <li><code>ft_anom_no_cape</code>: <code>float [n_exp, n_quant]</code> Same as <code>ft_anom</code>, but for anomaly associated with parcel     temperature, \\(T_{FT,\\epsilon=0}\\), rather than environmental temperature:     \\(A_{FT\\Delta, \\epsilon=0}[x] = \\sum_{n=2}^{\\infty}\\frac{1}{n!}\\frac{\\partial^n h^{\\dagger}}{\\partial T_{FT}^n}     (T_{FT,\\epsilon=0}-\\tilde{T}_{FT})^n\\)     Only included if <code>cape_form=True</code>.</li> <li><code>cape_ref</code>: <code>float [n_exp]</code>     Error in relating reference \\(\\epsilon\\) to reference CAPE. Defined according to:     \\(\\tilde{\\epsilon} = \\frac{\\tilde{\\beta}_{FT1}}{R^{\\dagger}}\\widetilde{CAPE} +     \\tilde{A}_{\\epsilon}\\)     Only included if <code>cape_form=True</code>.</li> </ul> Source code in <code>isca_tools/thesis/adiabat_theory2.py</code> <pre><code>def get_approx_terms(temp_surf_ref: np.ndarray, temp_surf_quant: np.ndarray, r_ref: np.ndarray,\n                     r_quant: np.ndarray, temp_ft_quant: np.ndarray,\n                     epsilon_quant: np.ndarray,\n                     pressure_surf: float, pressure_ft: float,\n                     epsilon_ref: Optional[np.ndarray] = None,\n                     z_approx_ref: Optional[np.ndarray] = None,\n                     simple: bool = False, cape_form: bool = False) -&gt; Tuple[dict, dict]:\n    \"\"\"\n    Function which returns terms quantifying the errors associated with various approximations, grouped together in\n    $A$ variables that go into the derivation of the theoretical scaling factor,\n    $\\delta \\hat{T}_s(x)/\\delta \\\\tilde{T}_s$, returned by `get_scale_factor_theory`.\n\n    The exact scaling factor is given by:\n\n    $$\n    \\\\begin{align}\n    \\\\frac{\\delta T_s(x)}{\\delta \\\\tilde{T}_s} &amp;= \\\\frac{\\delta \\hat{T}_s(x)}{\\delta \\\\tilde{T}_s} + A_{\\delta \\Delta T_{FT}}\n    - A_{\\delta \\Delta T_s} - A_{\\delta r}[x] - A_{\\delta \\Delta T_s \\delta r}[x] \\\\\\\\\n    &amp;+ A_{\\Delta T_s \\Delta r}[x] + A_{\\Delta}[x] + \\\\tilde{A}_{\\delta} + A_{NL}[x]\n    \\\\end{align}\n    $$\n\n    For more details on the approximations, there is a\n    [Jupyter notebook](https://github.com/jduffield65/Isca/blob/main/jobs/tau_sweep/land/meridional_band/publish_figures/theory_approximations2.ipynb)\n    that goes through each step of the derivation.\n\n    ??? note \"Terms in equation\"\n        * $h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon =\n            \\\\left(c_p + R^{\\dagger}\\\\right) T_{FT} + L_v q^*_{FT} + A_z$\n            where we used an approximate relation to replace $z_{FT}$ in $h^*_{FT}$.\n        * $\\epsilon = h_s - h^*_{FT}$, where $h_s$ is near-surface MSE (at $p_s$) and\n            $h^*_{FT}$ is free tropospheric saturated MSE (at $p_{FT}$).\n        * $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$\n        * $\\\\Delta \\chi[x] = \\chi[x] - \\\\tilde{\\chi}$\n        * $\\chi[x]$ is the value of $\\chi$ averaged over all days\n            where near-surface temperature, $T_s$, is between percentile $x-0.5$ and $x+0.5$.\n        * $\\\\tilde{\\chi}$ is the reference value of $\\chi$, which is free to be chosen.\n        * $\\\\beta_{FT1} = \\\\frac{\\partial h^{\\\\dagger}}{\\partial T_{FT}} = c_p + R^{\\dagger} + L_v \\\\alpha_{FT} q_{FT}^*$\n        * $\\\\beta_{FT2} = T_{FT} \\\\frac{\\partial^2h^{\\\\dagger}}{\\partial T_{FT}^2} =\n            T_{FT}\\\\frac{d\\\\beta_{FT1}}{d T_{FT}} = L_v \\\\alpha_{FT} q_{FT}^*(\\\\alpha_{FT} T_{FT} - 2)$\n        * $\\\\beta_{s1} = \\\\frac{\\partial h^{\\dagger}}{\\partial T_s} = c_p - R^{\\dagger} + L_v \\\\alpha_s q_s$\n        * $\\\\beta_{s2} = T_s \\\\frac{\\partial^2 h^{\\dagger}}{\\partial T_s^2} =\n            T_s\\\\frac{\\partial \\\\beta_{s1}}{\\partial T_s} = L_v \\\\alpha_s q_s(\\\\alpha_s T_s - 2)$\n        * $\\mu=\\\\frac{L_v \\\\alpha_s q_s}{\\\\beta_{s1}}$\n        * $q = rq^*$ where $q$ is the specific humidity, $r$ is relative humidity and $q^*(T, p)$\n            is saturation specific humidity which is a function of temperature and pressure.\n        * $\\\\alpha(T, p)$ is the clausius clapeyron parameter which is a function of temperature and pressure,\n            such that $\\partial q^*/\\partial T = \\\\alpha q^*$.\n        * $\\Delta h^{\\dagger}_0$ is referred to as `mse_mod_anom0` in the code, and is defined through:&lt;/br&gt;\n            $\\Delta h^{\\dagger}[x] = \\\\tilde{\\\\beta}_{s1}\\\\left(1+\\\\tilde{\\mu}\\\\frac{\\Delta r_s[x]}{\\\\tilde{r}_s}\\\\right)\n            \\Delta T_s[x] + L_v \\\\tilde{q}_s\\\\frac{\\Delta r_s[x]}{\\\\tilde{r}_s} - \\Delta \\epsilon[x] + A_{s\\Delta}[x]=\n            \\Delta h^{\\dagger}_0[x] + A_{s\\Delta}[x]$\n        * $\\delta \\\\tilde{h}^{\\dagger}_0$ is referred to as `mse_mod_ref_change0` in the code, and is defined through:&lt;/br&gt;\n            $\\delta \\\\tilde{h}^{\\dagger} =\n            \\\\tilde{\\\\beta}_{s1}\\\\left(1+\\\\tilde{\\mu}\\\\frac{\\delta \\\\tilde{r}_s}{\\\\tilde{r}_s}\\\\right)\\delta \\\\tilde{T}_s+\n            L_v \\\\tilde{q}_s\\\\frac{\\delta \\\\tilde{r}_s}{\\\\tilde{r}_s} - \\delta \\\\tilde{\\epsilon} + \\\\tilde{A}_{s\\delta}=\n            \\delta \\\\tilde{h}^{\\dagger}_0 + \\\\tilde{A}_{s\\delta}$\n        *  $\\delta \\Delta T_{FT}'[x]$ is referred to as `temp_ft_anom_change_mod` in the code, and is defined through:&lt;/br&gt;\n            $\\\\tilde{\\\\beta}_{FT1}\\delta \\Delta T_{FT}'[x] = \\\\tilde{\\\\beta}_{FT1} \\delta T_{FT}[x] - \\delta \\\\tilde{h}^{\\dagger}_0$&lt;/br&gt;\n            The idea being that $\\delta \\\\tilde{T}_{FT} \\\\approx \\delta \\\\tilde{h}^{\\dagger}_0 / \\\\tilde{\\\\beta}_{FT1}$.\n\n    If `cape_form=True`, $A_{\\Delta}$ will be modified, and an additional term $A_{CAPE}[x]$ is introduced for\n    the equation to remain exact, given that the theoretical scaling factor $\\delta \\hat{T}_s(x)/\\delta \\\\tilde{T}_s$\n    is modified.\n\n    Args:\n        temp_surf_ref: `float [n_exp]` $\\\\tilde{T}_s$&lt;/br&gt;\n            Reference near surface temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*. We assume `n_exp=2`.\n        temp_surf_quant: `float [n_exp, n_quant]` $T_s(x)$ &lt;/br&gt;\n            `temp_surf_quant[i, j]` is the percentile `quant_use[j]` of near surface temperature of\n            experiment `i`. Units: *K*.&lt;/br&gt;\n            Note that `quant_use` is not provided as not needed by this function, but is likely to be\n            `np.arange(1, 100)` - leave out `x=0` as doesn't really make sense to consider $0^{th}$ percentile\n            of a quantity.\n        r_ref: `float [n_exp]` $\\\\tilde{r}_s$&lt;/br&gt;\n            Reference near surface relative humidity of each simulation. Units: dimensionless (from 0 to 1).\n        r_quant: `float [n_exp, n_quant]` $r_s[x]$&lt;/br&gt;\n            `r_quant[i, j]` is near-surface relative humidity, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: dimensionless.\n        temp_ft_quant: `float [n_exp, n_quant]` $T_{FT}[x]$&lt;/br&gt;\n            `temp_ft_quant[i, j]` is temperature at `pressure_ft`, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *kg/kg*.\n        epsilon_quant: `float [n_exp, n_quant]` $\\epsilon[x]$&lt;/br&gt;\n            `epsilon_quant[i, j]` is $\\epsilon = h_s - h^*_{FT}$, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *kJ/kg*.\n        pressure_surf:\n            Pressure at near-surface, $p_s$, in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level, $p_{FT}$, in *Pa*.\n        epsilon_ref: `float [n_exp]` $\\\\tilde{\\epsilon}_s$&lt;/br&gt;\n            Reference value of $\\epsilon = h_s - h^*_{FT}$, where $h_s$ is near-surface MSE and\n            $h^*_{FT}$ is saturated MSE at `pressure_ft`. If not given, weill set to 0. Units: *kJ/kg*.\n        z_approx_ref: `float [n_exp]` $\\\\tilde{A}_z$&lt;/br&gt;\n            The exact equation for modified MSE is given by: $h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v q_s\n            - \\epsilon = (c_p + R^{\\dagger})T_{FT} + L_vq^*(T_{FT}, p_{FT}) + A_z$\n            where $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$ and $A_z$ quantifies the error due to\n            approximation of geopotential height, as relating to temperature.&lt;/br&gt;\n            Here you have the option of specifying the reference $A_z$ for each simulation. If not provided,\n            will set to 0. Units: *kJ/kg*.\n        simple: If `True`, will return approximate values of $A_{\\delta \\Delta T_{FT}}$, $A_{\\delta r}[x]$ and\n            $A_{\\Delta T_s \\Delta r}[x]$. Idea being that these tend to be the most significant, so these approximate\n            values can then be incorporated into theory for scale factor.\n            The approximate terms correspond to non-linear combinations of different physical mechanisms, e.g.\n            combined effect of relative humidity anomaly in current climate and free tropospheric change with warming:\n            $\\Delta r_s[x] \\delta \\Delta T_{FT}[x]$.\n        cape_form: If `True`, $A_{\\Delta}$ will be modified, and an additional term $A_{CAPE}[x]$ is introduced for\n            the equation to remain exact, given that the theoretical scaling factor\n            $\\delta \\hat{T}_s(x)/\\delta \\\\tilde{T}_s$ is modified.\n\n    Returns:\n        approx_terms: Dictionary containing approximations associated with final scaling factor,\n            $\\delta T_s(x)/\\delta \\\\tilde{T}_s$ so units are *K/K*. Terms have been named based on what causes the\n            variation in $x$. Each value in dictionary is a `float [n_quant]` array, except\n            `ref_change` which is a `float`.\n\n            * `temp_ft_anom_change`: $A_{\\delta \\Delta T_{FT}}$&lt;/br&gt;\n                Involves contribution from $\\delta \\Delta T_{FT}[x]$.\n                If `simple=True`, will return approximate form:&lt;/br&gt;\n                $A_{\\delta \\Delta T_{FT}} \\\\tilde{\\\\beta}_{s1}\\delta \\\\tilde{T}_s \\\\approx\n                \\\\frac{\\\\tilde{\\\\beta}_{FT2}}{2\\\\tilde{\\\\beta}_{FT1}} \\\\left(\\\\frac{\\delta T_{FT}[x]}{\\\\tilde{T}_{FT}} -\n                \\\\frac{\\delta \\\\tilde{h}^{\\dagger}_0}{\\\\tilde{\\\\beta}_{FT1}\\\\tilde{T}_{FT}}\\\\right)\n                (\\delta \\\\tilde{h}^{\\dagger}_0 + 2\\Delta \\\\tilde{h}^{\\dagger}_0[x] + \\\\tilde{\\\\beta}_{FT1}\\delta T_{FT}[x])$&lt;/br&gt;\n                Note that $\\Delta \\\\tilde{h}^{\\dagger}_0[x]$ can be decomposed to give relative contributions\n                of different anomalies in current climate: $\\Delta T_s[x], \\Delta r_s[x], \\Delta \\epsilon[x]$.\n            * `temp_s_anom_change`: $-A_{\\delta \\Delta T_s}$&lt;/br&gt;\n                Involves contribution from $\\delta \\Delta T_s(x)$. Returned value is multiplied by the $-1$;\n                negative value is because this approx comes from the surface $\\delta \\Delta h^{\\dagger}$ derivation.\n            * `r_change`: $A_{\\delta r}[x]$&lt;/br&gt;\n                Involves contribution from $\\delta r_s[x]$ and $\\delta \\\\tilde{r}_s$.\n                Returned value is multiplied by the $-1$;\n                negative value is because this approx comes from the surface $\\delta \\Delta h^{\\dagger}$ derivation.\n                If `simple=True`, will return approximate form:&lt;/br&gt;\n                $-A_{\\delta r}[x] \\\\tilde{\\\\beta}_{s1}\\delta \\\\tilde{T}_s \\\\approx -\n                \\\\tilde{\\mu}\\\\tilde{\\\\beta}_{s1}\\\\left(\\delta \\\\tilde{T}_s + \\Delta T_s[x]\\\\right)\n                \\\\frac{\\delta r_s[x]}{\\\\tilde{r}_s}$\n            * `temp_s_anom_r_change`: $A_{\\delta \\Delta T_s \\delta r}[x]$&lt;/br&gt;\n                Involves contribution from $\\delta \\Delta T_s(x) \\delta (r_s[x]/\\\\tilde{r}_s)$.\n                Returned value is multiplied by the $-1$;\n                negative value is because this approx comes from the surface $\\delta \\Delta h^{\\dagger}$ derivation.\n            * `anom_temp_s_r`: $A_{\\Delta T_s \\Delta r}[x]$&lt;/br&gt;\n                Involves contribution from $\\Delta T_s(x) \\Delta r_s[x]$ in the current climate.\n                If `simple=True`, will return approximate form:&lt;/br&gt;\n                $A_{\\Delta T_s \\Delta r} \\\\tilde{\\\\beta}_{s1}\\delta \\\\tilde{T}_s \\\\approx\n                \\\\left[\\\\frac{\\\\tilde{\\\\beta}_{FT2}}{\\\\tilde{\\\\beta}_{FT1}}\n                \\\\frac{\\delta \\\\tilde{h}^{\\dagger}_0}{\\\\tilde{\\\\beta}_{FT1}\\\\tilde{T}_{FT}}\n                \\\\tilde{\\mu} \\\\tilde{\\\\beta}_{s1} \\\\tilde{T}_s -\n                \\\\tilde{\\\\beta}_{s2}\\delta \\\\tilde{T}_s\\\\right]\\\\frac{\\Delta r_s[x]}{\\\\tilde{r}_s}\n                \\\\frac{\\Delta T_s[x]}{\\\\tilde{T}_s}$\n            * `anom`: $A_{\\Delta}[x]$&lt;/br&gt; Groups together errors due to approximation of anomaly in current climate.\n                Excludes those in $A_{\\Delta T_s \\Delta r}$.\n                If `cape_form=True`, will be modified to exclude any contribution from $\\Delta \\epsilon$.\n            * `ref_change`: $\\\\tilde{A}_{\\delta}$&lt;/br&gt;\n                Groups together errors due to change with warming of the reference day quantities.\n            * `z_anom_change`: $\\delta \\Delta A_z[x]$&lt;/br&gt;\n                Quantifies how error due to approximation of geopotential height changes with warming.\n            * `nl`: $A_{NL}[x]$&lt;/br&gt;\n                Residual non-linear errors that don't directly correspond to any of the above.\n            * `cape`: $A_{CAPE}[x]$&lt;/br&gt;\n                Includes the error contribution which arises due to the conversion between $\\epsilon$ and $CAPE$.&lt;/br&gt;\n                $A_{CAPE}[x] = \\\\frac{\\delta \\\\tilde{\\\\beta}_{FT1}}{R^{\\dagger}}(\\\\widetilde{CAPE} + \\delta CAPE[x]) +\n                \\delta (A_{FT\\Delta,\\epsilon=0}[x]-A_{FT\\Delta}[x]) -\n                \\\\frac{\\delta \\\\tilde{\\\\beta}_{FT1}}{\\\\tilde{\\\\beta}_{FT1}}\n                \\\\left(A_{FT\\Delta,\\epsilon=0}[x]-A_{FT\\Delta}[x] - \\\\tilde{A}_{\\epsilon}\\\\right)$&lt;/br&gt;\n                Will only be included if `cape_form=True`.\n        approx: Approximation, $A$, terms which arise through the derivation of the theory.\n            All have units of *J/kg*, except `ft_beta` and `s_beta` which are both dimensionless:\n\n            * `z_ref`: `float [n_exp]`&lt;/br&gt;Same as `z_approx_ref` input, $\\\\tilde{A}_z$ (set to 0 if not provided).\n            * `z_quant`: `float [n_exp, n_quant]`&lt;/br&gt;Error associated with geopotential height approx on quantile day,\n                $A_z[x]$, computed from provided variables according to\n                $h^{\\dagger}[x] = (c_p - R^{\\dagger})T_s[x] + L_v q_s[x] - \\epsilon = (c_p + R^{\\dagger})T_{FT}[x] + L_vq^*(T_{FT}[x], p_{FT}) + A_z[x]$\n            * `z_anom`: `float [n_exp, n_quant]`&lt;/br&gt;$\\Delta A_z[x] = A_z[x] - \\\\tilde{A}_z$.\n            * `ft_anom`: `float [n_exp, n_quant]`&lt;/br&gt; Approximation associated with anomaly of $h^{\\dagger}$ at FT level:\n                $A_{FT\\Delta}[x] = \\sum_{n=2}^{\\infty}\\\\frac{1}{n!}\\\\frac{\\partial^n h^{\\dagger}}{\\partial T_{FT}^n}(\\Delta T_{FT})^n$\n            * `ft_beta`: `float`&lt;/br&gt; $\\\\tilde{A}_{FT\\\\beta}$ such that\n                $\\delta \\\\tilde{\\\\beta}_{FT1} = \\\\tilde{\\\\beta}_{FT2}(1 + \\\\tilde{A}_{FT\\\\beta})\\\\frac{\\delta \\\\tilde{T}_{FT}}{\\\\tilde{T}_{FT}}$\n                where $\\\\beta_{FT1} = \\\\frac{\\partial h^{\\dagger}}{\\partial T_{FT}}$ and\n                $\\\\beta_{FT2} = T_{FT} \\\\frac{\\partial^2 h^{\\dagger}}{\\partial T_{FT}^2}$.\n            * `ft_ref_change`: `float`&lt;/br&gt; Approximation associated with change of $h^{\\dagger}$ with warming at FT level:\n                $\\\\tilde{A}_{FT\\delta} = \\sum_{n=2}^{\\infty}\\\\frac{1}{n!}\\\\frac{\\partial^n h^{\\dagger}}{\\partial T_{FT}^n}(\\delta \\\\tilde{T}_{FT})^n$\n            * `s_anom`: `float [n_exp, n_quant]`&lt;/br&gt; Approximation associated with anomaly of $h^{\\dagger}$ at surface:\n                $A_{s\\Delta}[x] = (1 + \\\\frac{\\Delta r_s[x]}{\\\\tilde{r}_s})A_{s\\Delta T}[x]$ where\n                $A_{s\\Delta T}[x] = \\sum_{n=2}^{\\infty}\\\\frac{1}{n!}\\\\frac{\\partial^n h^{\\dagger}}{\\partial T_{s}^n}(\\Delta T_{s})^n$\n            * `s_anom_temp_cont`: `float [n_exp, n_quant]`&lt;/br&gt; Temperature only contribution to `s_anom`:\n                $A_{s\\Delta T}[x] = \\sum_{n=2}^{\\infty}\\\\frac{1}{n!}\\\\frac{\\partial^n h^{\\dagger}}{\\partial T_{s}^n}(\\Delta T_{s})^n$.\n            * `s_anom_temp_r_cont`: `float [n_exp, n_quant]`&lt;/br&gt; Temperature and relative humidity non-linear contribution\n                to `s_anom`: $A_{s\\Delta T \\Delta r}[x] = \\\\frac{\\Delta r_s[x]}{\\\\tilde{r}_s}A_{s\\Delta T}[x]$.\n            * `s_anom_change_temp_cont`: `float [n_quant]`&lt;/br&gt; Temperature only contribution due to change in `s_anom` with warming:\n                $A_{s\\delta \\Delta T}[x] = (1 + \\\\frac{\\Delta r_s[x]}{\\\\tilde{r}_s})\\delta A_{s\\Delta T}[x]$\n            * `s_anom_change_r_cont`: `float [n_quant]`&lt;/br&gt; Relative humidity only contribution due to change in `s_anom` with warming:\n                $A_{s\\delta \\Delta r}[x] = \\delta (\\\\frac{\\Delta r_s[x]}{\\\\tilde{r}_s}) A_{s\\Delta T}[x]$\n            * `s_anom_change_temp_r_cont`: `float [n_quant]`&lt;/br&gt; Temperature and relative humidity non-linear contribution\n                due to change in `s_anom` with warming:\n                $A_{s\\delta \\Delta T \\delta \\Delta r}[x] = \\delta (\\\\frac{\\Delta r_s[x]}{\\\\tilde{r}_s}) \\delta A_{s\\Delta T}[x]$\n            * `s_beta`: `float`&lt;/br&gt; $\\\\tilde{A}_{s\\\\beta}$ such that\n                $\\delta \\\\tilde{\\\\beta}_{s1} = \\\\tilde{\\\\beta}_{s2}(1 + \\\\tilde{A}_{FT\\\\beta})\\\\left(1 + \\\\frac{\\delta \\\\tilde{r}_s}{\\\\tilde{r}_s}\\\\right)\\\\frac{\\delta \\\\tilde{T}_s}{\\\\tilde{T}_s} + \\\\tilde{\\mu}\\\\tilde{\\\\beta}_{s1} \\\\frac{\\delta \\\\tilde{r}_s}{\\\\tilde{r}_s}$\n            * `s_ref_change`: `float`&lt;/br&gt; Approximation associated with change of $h^{\\dagger}$ with warming at surface:\n                $\\\\tilde{A}_{s\\delta}[x] = (1 + \\\\frac{\\delta \\\\tilde{r}_s[x]}{\\\\tilde{r}_s})\\sum_{n=2}^{\\infty}\\\\frac{1}{n!}\\\\frac{\\partial^n h^{\\dagger}}{\\partial T_{s}^n}(\\delta \\\\tilde{T}_{s})^n$\n            * `ft_anom_no_cape`: `float [n_exp, n_quant]`&lt;/br&gt; Same as `ft_anom`, but for anomaly associated with parcel\n                temperature, $T_{FT,\\epsilon=0}$, rather than environmental temperature:\n                $A_{FT\\Delta, \\epsilon=0}[x] = \\sum_{n=2}^{\\infty}\\\\frac{1}{n!}\\\\frac{\\partial^n h^{\\dagger}}{\\partial T_{FT}^n}\n                (T_{FT,\\epsilon=0}-\\\\tilde{T}_{FT})^n$&lt;/br&gt;\n                Only included if `cape_form=True`.\n            * `cape_ref`: `float [n_exp]`&lt;/br&gt;\n                Error in relating reference $\\epsilon$ to reference CAPE. Defined according to:\n                $\\\\tilde{\\epsilon} = \\\\frac{\\\\tilde{\\\\beta}_{FT1}}{R^{\\dagger}}\\\\widetilde{CAPE} +\n                \\\\tilde{A}_{\\epsilon}$&lt;/br&gt;\n                Only included if `cape_form=True`.\n    \"\"\"\n    n_exp, n_quant = temp_surf_quant.shape\n    if z_approx_ref is None:\n        z_approx_ref = np.zeros(n_exp)\n    if epsilon_ref is None:\n        epsilon_ref = np.zeros(n_exp)\n\n    sphum_ref = r_ref * sphum_sat(temp_surf_ref, pressure_surf)\n    sphum_quant = r_quant * sphum_sat(temp_surf_quant, pressure_surf)\n    temp_ft_ref = np.zeros(n_exp)\n    for i in range(n_exp):\n        temp_ft_ref[i] = get_temp_adiabat(temp_surf_ref[i], sphum_ref[i], pressure_surf, pressure_ft,\n                                          epsilon=epsilon_ref[i] + z_approx_ref[i])\n\n    R_mod, _, _, beta_ft1, beta_ft2, _, _ = get_theory_prefactor_terms(temp_ft_ref, pressure_surf, pressure_ft)\n    _, _, _, beta_s1, beta_s2, _, mu = get_theory_prefactor_terms(temp_surf_ref, pressure_surf, pressure_ft, sphum_ref)\n\n    # For everything below, deal in units of J/kg\n    mse_mod_quant = (moist_static_energy(temp_surf_quant, sphum_quant, height=0,\n                                         c_p_const=c_p - R_mod) - epsilon_quant) * 1000\n    mse_mod_ref = (moist_static_energy(temp_surf_ref, sphum_ref, height=0, c_p_const=c_p - R_mod) - epsilon_ref) * 1000\n    mse_mod_anom = mse_mod_quant - mse_mod_ref[:, np.newaxis]\n    temp_surf_anom = temp_surf_quant - temp_surf_ref[:, np.newaxis]\n    temp_ft_anom = temp_ft_quant - temp_ft_ref[:, np.newaxis]\n    r_anom = r_quant - r_ref[:, np.newaxis]\n    epsilon_anom = (epsilon_quant - epsilon_ref[:, np.newaxis]) * 1000      # units of J/kg\n\n\n    approx = {}\n    # Z error - The starting equation for mse_mod approximates the geopotential height. We quantify that here.\n    approx['z_quant'] = mse_mod_quant - moist_static_energy(temp_ft_quant, sphum_sat(temp_ft_quant, pressure_ft),\n                                                            height=0, c_p_const=c_p+R_mod)*1000\n    approx['z_ref'] = z_approx_ref * 1000\n    approx['z_anom'] = approx['z_quant'] - approx['z_ref'][:, np.newaxis]\n\n    # Expansion of mse_mod about ref at FT. I.e. in approximating mse_mod_anom for each simulation.\n    approx['ft_anom'] = mse_mod_anom - approx['z_anom'] - beta_ft1[:, np.newaxis] * temp_ft_anom\n\n    # Change with warming of mse_mod_anom at FT level has contribution from change in beta_ft1 - dimensionless\n    approx['ft_beta'] = np.diff(beta_ft1, axis=0).squeeze() * temp_ft_ref[0] \\\n                        / np.diff(temp_ft_ref, axis=0).squeeze() / beta_ft2[0] - 1\n\n    # Change in mse_mod_ref with warming at FT level\n    approx['ft_ref_change'] = np.diff(mse_mod_ref, axis=0).squeeze() - beta_ft1[0] * \\\n                              np.diff(temp_ft_ref, axis=0).squeeze() - np.diff(approx['z_ref'], axis=0).squeeze()\n\n    # Expansion of mse_mod about ref at Surface. I.e. in approximating mse_mod_anom for each simulation.\n    approx['s_anom'] = mse_mod_anom - beta_s1[:, np.newaxis] * (\n            1 + mu[:, np.newaxis] * (r_anom / r_ref[:, np.newaxis])) * temp_surf_anom - \\\n                       L_v * sphum_ref[:, np.newaxis] * (r_anom / r_ref[:, np.newaxis]) + epsilon_anom\n    approx['s_anom_temp_cont'] = approx['s_anom'] / (1 + (r_anom / r_ref[:, np.newaxis]))\n    # Decompose change with warming into contributions from temp, RH and NL\n    approx['s_anom_change_temp_cont'] = (1 + (r_anom / r_ref[:, np.newaxis])[0]) * np.diff(\n        approx['s_anom_temp_cont'], axis=0).squeeze()\n    approx['s_anom_change_r_cont'] = np.diff((r_anom / r_ref[:, np.newaxis]), axis=0).squeeze() * \\\n                                     approx['s_anom_temp_cont'][0]\n    approx['s_anom_change_temp_r_cont'] = np.diff((r_anom / r_ref[:, np.newaxis]), axis=0).squeeze() * np.diff(\n        approx['s_anom_temp_cont'], axis=0).squeeze()\n\n    # Change with warming of mse_mod_anom at Surface has contribution from change in beta_s1 - dimensionless\n    approx['s_beta'] = (np.diff(beta_s1, axis=0).squeeze() - mu[0] * beta_s1[0] * np.diff(r_ref, axis=0).squeeze() /\n                        r_ref[0]) * temp_surf_ref[0] / np.diff(temp_surf_ref, axis=0).squeeze() / (\n                                   1 + np.diff(r_ref, axis=0).squeeze() / r_ref[0]) / beta_s2[0] - 1\n\n    # Change in mse_mod_ref with warming at Surface\n    # Remember to convert epsilon to J/kg in this calculation\n    approx['s_ref_change'] = np.diff(mse_mod_ref, axis=0).squeeze() - beta_s1[0] * (\n            1 + mu[0] * (np.diff(r_ref, axis=0).squeeze() / r_ref[0])\n    ) * np.diff(temp_surf_ref, axis=0).squeeze() - L_v * sphum_ref[0] * (np.diff(r_ref, axis=0).squeeze() / r_ref[0]) \\\n                             + np.diff(epsilon_ref*1000, axis=0).squeeze()\n\n    # Combine approximations into terms which contribute to final scaling factor\n    prefactor_mse_ft = beta_ft2[0] / beta_ft1[0] ** 2 / temp_ft_ref[0]\n    mse_mod_ref_change0 = np.diff(mse_mod_ref, axis=0).squeeze() - approx['s_ref_change']\n    mse_mod_anom0 = mse_mod_anom[0] - approx['s_anom'][0]\n    temp_ft_anom_change_mod = np.diff(temp_ft_quant, axis=0).squeeze() - (mse_mod_ref_change0 / beta_ft1[0])\n    var_ref_change = approx['s_ref_change'] - approx['ft_ref_change'] - np.diff(approx['z_ref'], axis=0).squeeze()\n    var_anom0 = approx['s_anom'][0] - approx['ft_anom'][0] - approx['z_anom'][0]\n\n    # For details of different terms, see theory_approximations2 notebook\n    approx_terms = {}\n    # From FT derivation\n    if simple:\n        approx_terms['temp_ft_anom_change'] = prefactor_mse_ft * beta_ft1[0] * (\n                mse_mod_ref_change0 + mse_mod_anom0 + 0.5*beta_ft1[0]*temp_ft_anom_change_mod) * temp_ft_anom_change_mod\n    else:\n        approx_terms['temp_ft_anom_change'] = np.diff(approx['ft_anom'], axis=0).squeeze() + \\\n                                              np.diff(beta_ft1, axis=0).squeeze() * np.diff(temp_ft_anom, axis=0).squeeze()\n    approx_terms['z_anom_change'] = np.diff(approx['z_anom'], axis=0).squeeze()\n    approx_terms['ref_change'] =  -var_ref_change\n    approx_terms['nl'] = prefactor_mse_ft * ((approx['ft_beta'] * mse_mod_ref_change0) +\n                                              (1+approx['ft_beta']) * var_ref_change) * var_anom0\n\n    if cape_form:   # TO CHANGE approx_terms['anom'] IN CAPE FORM\n        # remove epsilon contribution in approx_terms['anom'] if cape_form as add extra approx_terms['cape']\n        mse_mod_anom0 += epsilon_anom[0]\n\n    approx_terms['anom'] = prefactor_mse_ft * mse_mod_ref_change0 * (var_anom0 + approx['ft_beta'] * mse_mod_anom0) \\\n                           + (prefactor_mse_ft * (1+approx['ft_beta']) * var_ref_change) * mse_mod_anom0\n    approx_terms['anom_temp_s_r'] = prefactor_mse_ft * mse_mod_ref_change0 * beta_s1[0] * mu[0] * \\\n                                    r_anom[0]/r_ref[0] * temp_surf_anom[0]\n\n    # From Surface derivation\n    approx_terms['anom'] -= (approx['s_beta'] * beta_s2[0] * (1 + np.diff(r_ref, axis=0).squeeze()/r_ref[0]) *\n                                               np.diff(temp_surf_ref, axis=0).squeeze()/temp_surf_ref[0]\n                                               ) * temp_surf_anom[0] + (approx['s_ref_change']/r_ref[0]) * r_anom[0]\n    if simple:\n        approx_terms['anom_temp_s_r'] -= beta_s2[0] * np.diff(temp_surf_ref, axis=0).squeeze() * r_anom[0]/r_ref[0] * \\\n                                         temp_surf_anom[0]/temp_surf_ref[0]\n    else:\n        approx_terms['anom_temp_s_r'] -= np.diff(beta_s1, axis=0).squeeze() * \\\n                                       r_anom[0]/r_ref[0] * temp_surf_anom[0]\n    approx_terms['temp_s_anom_change'] = - approx['s_anom_change_temp_cont'] - (\n            (mu*beta_s1/r_ref)[0]*r_anom[0] + (1+r_anom[0]/r_ref[0])*np.diff(beta_s1, axis=0).squeeze()\n    )* np.diff(temp_surf_anom, axis=0).squeeze()\n\n    if simple:\n        approx_terms['r_change'] = -mu[0]*beta_s1[0] * (np.diff(temp_surf_ref, axis=0).squeeze()+temp_surf_anom[0]) * \\\n                                   np.diff(r_anom/r_ref[:, np.newaxis], axis=0).squeeze()\n    else:\n        # r_change1 term\n        approx_terms['r_change'] = -approx['s_anom_change_r_cont'] - (L_v * np.diff(sphum_ref, axis=0).squeeze() +\n                                          (mu[0]*beta_s1[0] + np.diff(beta_s1, axis=0).squeeze()) * temp_surf_anom[0]\n                                          ) * np.diff(r_anom/r_ref[:, np.newaxis], axis=0).squeeze()\n        # Add r_change2 term\n        approx_terms['r_change'] -= L_v * sphum_ref[0] * (\n                np.diff(r_anom / r_ref[:, np.newaxis], axis=0).squeeze() + r_anom[0] *\n                (np.diff(r_ref, axis=0).squeeze() / r_ref[0] ** 2)\n                - np.diff(r_anom, axis=0).squeeze() / r_ref[0])\n\n    approx_terms['temp_s_anom_r_change'] = -approx['s_anom_change_temp_r_cont'] - (\n            mu[0]*beta_s1[0] + np.diff(beta_s1, axis=0).squeeze()\n    ) * np.diff(r_anom/r_ref[:, np.newaxis], axis=0).squeeze() * np.diff(temp_surf_anom, axis=0).squeeze()\n\n    if cape_form:\n        # Get cape variables required for approx terms\n        cape_ref = np.zeros(n_exp)\n        cape_quant = np.zeros((n_exp, n_quant))\n        temp_ft_parcel_quant = np.zeros((n_exp, n_quant))\n        for i in range(n_exp):\n            if epsilon_ref[i] != 0: # if epsilon_ref=0, then cape_ref=0 too\n                # For ref, have temp_ft,  z_approx and epsilon so provide all to do a sanity check\n                cape_ref[i] = get_cape_approx(temp_surf_ref[i], r_ref[i], pressure_surf, pressure_ft,\n                                              epsilon=epsilon_ref[i], z_approx=z_approx_ref[i], temp_ft=temp_ft_ref[i])[0]\n            for j in range(n_quant):\n                # For quant, don't have z_approx so compute from temp_ft and epsilon\n                cape_quant[i, j], temp_ft_parcel_quant[i, j] = \\\n                    get_cape_approx(temp_surf_quant[i, j], r_quant[i, j], pressure_surf, pressure_ft,\n                                    epsilon=epsilon_quant[i, j], temp_ft=temp_ft_quant[i, j])\n        cape_ref *= 1000       # convert to units of J/kg\n        cape_quant *= 1000     # convert to units of J/kg\n        mse_mod_quant_no_cape = moist_static_energy(temp_surf_quant, sphum_quant, height=0,\n                                                    c_p_const=c_p - R_mod) * 1000       # units of J/kg\n\n        # Add cape specific approx terms, ensure everything in units of J/kg\n        approx['cape_ref'] = (epsilon_ref*1000) - beta_ft1 / R_mod * cape_ref\n        approx['ft_anom_no_cape'] = mse_mod_quant_no_cape - mse_mod_ref[:, np.newaxis] - \\\n                                    approx['z_anom'] - beta_ft1[:, np.newaxis] * (\n                                            temp_ft_parcel_quant - temp_ft_ref[:, np.newaxis])\n        approx_terms['cape'] = np.diff(beta_ft1, axis=0).squeeze() / R_mod * (cape_ref[0] +\n                                                                              np.diff(cape_quant, axis=0).squeeze())\n        approx_terms['cape'] += np.diff(approx['ft_anom_no_cape'] - approx['ft_anom'], axis=0).squeeze()\n        approx_terms['cape'] -= (np.diff(beta_ft1, axis=0).squeeze() / beta_ft1[0]) * (\n                approx['ft_anom_no_cape'] - approx['ft_anom'] - approx['cape_ref'][:, np.newaxis])[0]\n\n\n    # Convert into scaling factor K/K units\n    for key in approx_terms:\n        approx_terms[key] = approx_terms[key] / (beta_s1[0] * np.diff(temp_surf_ref, axis=0).squeeze())\n    return approx_terms, approx\n</code></pre>"},{"location":"code/thesis/adiabat_theory2/#isca_tools.thesis.adiabat_theory2.get_cape_approx","title":"<code>get_cape_approx(temp_surf, r_surf, pressure_surf, pressure_ft, temp_ft=None, epsilon=None, z_approx=None)</code>","text":"<p>Calculates an approximate value of CAPE using a single pressure level in the free troposphere, \\(p_{FT}\\):</p> \\[CAPE = R^{\\dagger} (T_{FT,\\epsilon=0} - T_{FT})\\] <p>where \\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\) and \\(T_{FT,\\epsilon=0}\\) is the free tropospheric temperature which would occur if \\(\\epsilon=0\\), all else the same. I.e. this is the parcel rather than environmental temperature at \\(p_{FT}\\).</p> Computation of \\(T_{FT,\\epsilon=0}\\) and \\(T_{FT}\\) <p>\\(T_{FT}\\) is exactly related to the surface through the modified MSE relation:</p> \\[h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v r_s q^*(T_s, p_s) - \\epsilon = (c_p + R^{\\dagger})T_{FT} + L_vq^*(T_{FT}, p_{FT}) + A_z\\] <p>So only two of the three variables \\(T_{FT}\\), \\(\\epsilon\\) and \\(A_z\\) are required. The other will be computed from this equation.</p> <p>Similarly, \\(T_{FT,\\epsilon=0}\\) will be computed from the following equation where all variables have the same value as for the \\(T_{FT}\\) equation:</p> \\[h^{\\dagger}_{\\epsilon=0} = (c_p - R^{\\dagger})T_s + L_v r_s q^*(T_s, p_s) = (c_p + R^{\\dagger})T_{FT,\\epsilon=0} + L_vq^*(T_{FT,\\epsilon=0}, p_{FT}) + A_z\\] Terms in equation <ul> <li>\\(h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon =     \\left(c_p + R^{\\dagger}\\right) T_{FT} + L_v q^*_{FT} + A_z\\)     where we used an approximate relation to replace \\(z_{FT}\\) in \\(h^*_{FT}\\),     and \\(A_z\\) quantifies the error in this replacement.</li> <li>\\(\\epsilon = h_s - h^*_{FT}\\), where \\(h_s\\) is near-surface MSE (at \\(p_s\\)) and     \\(h^*_{FT}\\) is free tropospheric saturated MSE (at \\(p_{FT}\\)).</li> <li>\\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\)</li> <li>\\(q = rq^*\\) where \\(q\\) is the specific humidity, \\(r\\) is relative humidity and \\(q^*(T, p)\\)     is saturation specific humidity which is a function of temperature and pressure.</li> <li>\\(A_z\\) quantifies the error due to     approximation of geopotential height, as relating to temperature.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>temp_surf</code> <code>Union[float, ndarray]</code> <p>Temperature at <code>pressure_surf</code> in Kelvin. If array, must be same size as <code>r_surf</code>.</p> required <code>r_surf</code> <code>Union[float, ndarray]</code> <p>Relative humidity at <code>pressure_surf</code> in Kelvin. If array, must be same size as <code>temp_surf</code>.</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface, \\(p_s\\), in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level, \\(p_{FT}\\), in Pa.</p> required <code>temp_ft</code> <code>Optional[Union[float, ndarray]]</code> <p>Temperature at <code>pressure_ft</code> in Kelvin. If array, must be same size as <code>temp_surf</code>. If not provided, will be computed using <code>epsilon</code> and <code>approx_z</code> (see Computation of \\(T_{FT,\\epsilon=0}\\) and \\(T_{FT}\\) box above).</p> <code>None</code> <code>epsilon</code> <code>Optional[Union[float, ndarray]]</code> <p>\\(\\epsilon = h_s - h^*_{FT}\\) If array, must be same size as <code>temp_surf</code>. If not provided, will be computed using <code>temp_ft</code> and <code>approx_z</code> (see Computation of \\(T_{FT,\\epsilon=0}\\) and \\(T_{FT}\\) box above). Units: kJ/kg.</p> <code>None</code> <code>z_approx</code> <code>Optional[Union[float, ndarray]]</code> <p>\\(A_z\\) quantifies the error due to approximation of replacing geopotential height with temperature. If array, must be same size as <code>temp_surf</code>. If not provided, will be computed using <code>temp_ft</code> and <code>epsilon</code> (see Computation of \\(T_{FT,\\epsilon=0}\\) and \\(T_{FT}\\) box above). Units: kJ/kg.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>cape</code> <code>Union[float, ndarray]</code> <p>\\(CAPE = R^{\\dagger} (T_{FT,\\epsilon=0} - T_{FT})\\) in units of kJ/kg If array, will be same size as <code>temp_surf</code>.</p> <code>temp_ft_parcel</code> <code>Union[float, ndarray]</code> <p>\\(T_{FT,\\epsilon=0}\\) in units of Kelvin. If array, will be same size as <code>temp_surf</code>.</p> Source code in <code>isca_tools/thesis/adiabat_theory2.py</code> <pre><code>def get_cape_approx(temp_surf: Union[float, np.ndarray], r_surf: Union[float, np.ndarray],\n                    pressure_surf: float, pressure_ft: float,\n                    temp_ft: Optional[Union[float, np.ndarray]] = None,\n                    epsilon: Optional[Union[float, np.ndarray]] = None,\n                    z_approx: Optional[Union[float, np.ndarray]] = None\n                    ) -&gt; Tuple[Union[float, np.ndarray], Union[float, np.ndarray]]:\n    \"\"\"\n    Calculates an approximate value of CAPE using a single pressure level in the free troposphere, $p_{FT}$:\n\n    $$CAPE = R^{\\dagger} (T_{FT,\\epsilon=0} - T_{FT})$$\n\n    where $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$ and $T_{FT,\\epsilon=0}$ is the free tropospheric temperature which\n    would occur if $\\epsilon=0$, all else the same.\n    I.e. this is the parcel rather than environmental temperature at $p_{FT}$.\n\n    ??? note \"Computation of $T_{FT,\\epsilon=0}$ and $T_{FT}$\"\n        $T_{FT}$ is exactly related to the surface through the modified MSE relation:\n\n        $$h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v r_s q^*(T_s, p_s) - \\epsilon\n        = (c_p + R^{\\dagger})T_{FT} + L_vq^*(T_{FT}, p_{FT}) + A_z$$\n\n        So only two of the three variables $T_{FT}$, $\\epsilon$ and $A_z$ are required. The other will be computed from\n        this equation.\n\n        Similarly, $T_{FT,\\epsilon=0}$ will be computed from the following equation where all variables have the same\n        value as for the $T_{FT}$ equation:\n\n        $$h^{\\dagger}_{\\epsilon=0} = (c_p - R^{\\dagger})T_s + L_v r_s q^*(T_s, p_s)\n        = (c_p + R^{\\dagger})T_{FT,\\epsilon=0} + L_vq^*(T_{FT,\\epsilon=0}, p_{FT}) + A_z$$\n\n    ??? note \"Terms in equation\"\n        * $h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon =\n            \\\\left(c_p + R^{\\dagger}\\\\right) T_{FT} + L_v q^*_{FT} + A_z$\n            where we used an approximate relation to replace $z_{FT}$ in $h^*_{FT}$,\n            and $A_z$ quantifies the error in this replacement.\n        * $\\epsilon = h_s - h^*_{FT}$, where $h_s$ is near-surface MSE (at $p_s$) and\n            $h^*_{FT}$ is free tropospheric saturated MSE (at $p_{FT}$).\n        * $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$\n        * $q = rq^*$ where $q$ is the specific humidity, $r$ is relative humidity and $q^*(T, p)$\n            is saturation specific humidity which is a function of temperature and pressure.\n        * $A_z$ quantifies the error due to\n            approximation of geopotential height, as relating to temperature.\n\n    Args:\n        temp_surf:\n            Temperature at `pressure_surf` in Kelvin. If array, must be same size as `r_surf`.\n        r_surf:\n            Relative humidity at `pressure_surf` in Kelvin. If array, must be same size as `temp_surf`.\n        pressure_surf:\n            Pressure at near-surface, $p_s$, in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level, $p_{FT}$, in *Pa*.\n        temp_ft:\n            Temperature at `pressure_ft` in Kelvin. If array, must be same size as `temp_surf`.&lt;/br&gt;\n            If not provided, will be computed using `epsilon` and `approx_z`\n            (see *Computation of $T_{FT,\\epsilon=0}$ and $T_{FT}$* box above).\n        epsilon:\n            $\\epsilon = h_s - h^*_{FT}$ If array, must be same size as `temp_surf`.&lt;/br&gt;\n            If not provided, will be computed using `temp_ft` and `approx_z`\n            (see *Computation of $T_{FT,\\epsilon=0}$ and $T_{FT}$* box above).&lt;/br&gt;\n            Units: *kJ/kg*.\n        z_approx:\n            $A_z$ quantifies the error due to approximation of replacing geopotential height with temperature.\n            If array, must be same size as `temp_surf`.&lt;/br&gt;\n            If not provided, will be computed using `temp_ft` and `epsilon`\n            (see *Computation of $T_{FT,\\epsilon=0}$ and $T_{FT}$* box above).&lt;/br&gt;\n            Units: *kJ/kg*.&lt;/br&gt;\n\n    Returns:\n        cape: $CAPE = R^{\\dagger} (T_{FT,\\epsilon=0} - T_{FT})$ in units of *kJ/kg*\n            If array, will be same size as `temp_surf`.\n        temp_ft_parcel: $T_{FT,\\epsilon=0}$ in units of Kelvin.\n            If array, will be same size as `temp_surf`.\n    \"\"\"\n    R_mod = R * np.log(pressure_surf/pressure_ft)/2\n    sphum_surf = r_surf * sphum_sat(temp_surf, pressure_surf)\n    if z_approx is None:\n        if (temp_ft is None) or (epsilon is None):\n            raise ValueError(\"If approx_z not provided, must provide both temp_ft and epsilon\")\n        z_approx = moist_static_energy(temp_surf, sphum_surf, height=0, c_p_const=c_p - R_mod) - epsilon \\\n                   - moist_static_energy(temp_ft, sphum_sat(temp_ft, pressure_ft), height=0, c_p_const=c_p+R_mod)\n    elif temp_ft is None:\n        if (z_approx is None) or (epsilon is None):\n            raise ValueError(\"If temp_ft not provided, must provide both epsilon and approx_z\")\n        temp_ft = get_temp_adiabat(temp_surf, sphum_surf, pressure_surf,\n                                   pressure_ft, epsilon=epsilon + z_approx)\n    elif epsilon is None:\n        if (temp_ft is None) or (z_approx is None):\n            raise ValueError(\"If epsilon not provided, must provide both temp_ft and approx_z\")\n    else:\n        # If provide all values, do sanity check that temp_ft is same as computed from epsilon and approx_z\n        temp_ft_calc = get_temp_adiabat(temp_surf, sphum_surf,\n                                        pressure_surf, pressure_ft, epsilon=epsilon + z_approx)\n        if not np.isclose(temp_ft, temp_ft_calc):\n            raise ValueError(f\"temp_ft={temp_ft}K provided does not match temp_ft_calc={temp_ft_calc}K \"\n                             f\"computed using epsilon and approx_z.\")\n    if np.isnan(temp_surf).any():\n        if temp_surf.size == 1:\n            temp_ft_parcel = np.nan     # if single number, set to nan\n        else:\n            # If nan values, only compute temp_ft_parcel for non-nan values, otherwise does weird things\n            temp_ft_parcel = np.full_like(temp_surf, np.nan)\n            temp_ft_parcel[~np.isnan(temp_surf)] = get_temp_adiabat(temp_surf[~np.isnan(temp_surf)],\n                                                                    sphum_surf[~np.isnan(temp_surf)], pressure_surf,\n                                                                    pressure_ft, epsilon=z_approx[~np.isnan(temp_surf)])\n    else:\n        temp_ft_parcel = get_temp_adiabat(temp_surf, sphum_surf, pressure_surf,\n                                          pressure_ft, epsilon=z_approx)\n    return R_mod*(temp_ft_parcel - temp_ft) / 1000, temp_ft_parcel\n</code></pre>"},{"location":"code/thesis/adiabat_theory2/#isca_tools.thesis.adiabat_theory2.get_scale_factor_theory","title":"<code>get_scale_factor_theory(temp_surf_ref, temp_surf_quant, r_ref, r_quant, temp_ft_quant, epsilon_quant, pressure_surf_ref, pressure_ft_ref, epsilon_ref=None, z_approx_ref=None, include_non_linear=False, cape_form=False, pressure_surf_quant=None, pressure_ft_quant=None)</code>","text":"<p>Calculates the theoretical near-surface temperature change for percentile \\(x\\), \\(\\delta \\hat{T}_s(x)\\), relative to the reference temperature change, \\(\\delta \\tilde{T}_s\\). The theoretical scale factor is given by:</p> \\[ \\begin{align} \\frac{\\delta \\hat{T}_s(x)}{\\delta \\tilde{T}_s} &amp;= \\gamma_{\\delta T_{FT}}\\frac{\\delta T_{FT}[x]}{\\delta \\tilde{T}_s} - \\gamma_{\\delta r}\\frac{\\tilde{T}_s}{\\tilde{r}_s} \\frac{\\delta r_s[x]}{\\delta \\tilde{T}_s} + \\gamma_{\\delta \\epsilon} \\frac{\\delta \\epsilon[x]}{c_p \\delta \\tilde{T}_s} \\\\ &amp;+ \\gamma_{\\Delta T_s} \\frac{\\Delta T_s(x)}{\\tilde{T}_s} - \\gamma_{\\Delta r} \\frac{\\Delta r[x]}{\\tilde{r}_s} - \\gamma_{\\Delta \\epsilon} \\frac{\\Delta \\epsilon[x]}{c_p \\tilde{T}_s} - \\gamma_{\\delta \\tilde{r}}\\frac{\\delta \\tilde{r}_s}{\\tilde{r}_s} \\end{align} \\] <p>where the dimensionless \\(\\gamma\\) parameters quantify the significance of different physical mechanisms in causing a change in the near-surface temperature distribution. These are given by the <code>get_sensitivity_factors</code> function.</p> <p>The approximations which cause \\(\\frac{\\delta \\hat{T}_s(x)}{\\delta \\tilde{T}_s}\\) to differ from the exact scale factor are given in <code>get_approx_terms</code>.</p> Reference Quantities <p>The reference quantities, \\(\\tilde{\\chi}\\) are free to be chosen by the user. For ease of interpretation, I propose the following, where \\(\\overline{\\chi}\\) is the mean value of \\(\\chi\\) across all days:</p> <ul> <li>\\(\\tilde{T}_s = \\overline{T_s}; \\delta \\tilde{T}_s = \\delta \\overline{T_s}\\)</li> <li>\\(\\tilde{r}_s = \\overline{r_s}; \\delta \\tilde{r}_s = 0\\)</li> <li>\\(\\tilde{\\epsilon} = 0; \\delta \\tilde{\\epsilon} = 0\\)</li> <li>\\(\\tilde{A}_z = \\overline{A}_z; \\delta \\tilde{A}_z = 0\\)</li> </ul> <p>Given the choice of these four reference variables and their changes with warming, the reference free troposphere temperature, \\(\\tilde{T}_{FT}\\), can be computed according to the definition of \\(\\tilde{h}^{\\dagger}\\):</p> <p>\\(\\tilde{h}^{\\dagger} = (c_p - R^{\\dagger})\\tilde{T}_s + L_v \\tilde{q}_s - \\tilde{\\epsilon} =     (c_p + R^{\\dagger}) \\tilde{T}_{FT} + L_v q^*(\\tilde{T}_{FT}, p_{FT}) + \\tilde{A}_z\\)</p> <p>If <code>cape_form=True</code>, the reference CAPE, \\(\\widetilde{CAPE}\\), will also be computed from these four variables using <code>get_cape_approx</code>. This will be 0 if \\(\\tilde{\\epsilon}=0\\).</p> <p>Poor choice of reference quantities may cause the theoretical scale factor to be a bad approximation. If this is the case, <code>get_approx_terms</code> can be used to investigate what is causing the theory to break down.</p> Terms in equation <ul> <li>\\(h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon =     (c_p + R^{\\dagger}) T_{FT} + L_v q^*_{FT} + A_z\\)     where we used an approximate relation to replace \\(z_{FT}\\) in \\(h^*_{FT}\\).</li> <li>\\(\\epsilon = h_s - h^*_{FT}\\), where \\(h_s\\) is near-surface MSE (at \\(p_s\\)) and     \\(h^*_{FT}\\) is free tropospheric saturated MSE (at \\(p_{FT}\\)).</li> <li>\\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\)</li> <li>\\(\\Delta \\chi[x] = \\chi[x] - \\tilde{\\chi}\\)</li> <li>\\(\\chi[x]\\) is the value of \\(\\chi\\) averaged over all days     where near-surface temperature, \\(T_s\\), is between percentile \\(x-0.5\\) and \\(x+0.5\\).</li> <li>\\(\\tilde{\\chi}\\) is the reference value of \\(\\chi\\), which is free to be chosen.</li> <li>\\(\\beta_{FT1} = \\frac{\\partial h^{\\dagger}}{\\partial T_{FT}} = c_p + R^{\\dagger} + L_v \\alpha_{FT} q_{FT}^*\\)</li> <li>\\(\\beta_{FT2} = T_{FT} \\frac{\\partial^2h^{\\dagger}}{\\partial T_{FT}^2} =     T_{FT}\\frac{d\\beta_{FT1}}{d T_{FT}} = L_v \\alpha_{FT} q_{FT}^*(\\alpha_{FT} T_{FT} - 2)\\)</li> <li>\\(\\beta_{s1} = \\frac{\\partial h^{\\dagger}}{\\partial T_s} = c_p - R^{\\dagger} + L_v \\alpha_s q_s\\)</li> <li>\\(\\beta_{s2} = T_s \\frac{\\partial^2 h^{\\dagger}}{\\partial T_s^2} =     T_s\\frac{\\partial \\beta_{s1}}{\\partial T_s} = L_v \\alpha_s q_s(\\alpha_s T_s - 2)\\)</li> <li>\\(\\mu=\\frac{L_v \\alpha_s q_s}{\\beta_{s1}}\\)</li> <li>\\(q = rq^*\\) where \\(q\\) is the specific humidity, \\(r\\) is relative humidity and \\(q^*(T, p)\\)     is saturation specific humidity which is a function of temperature and pressure.</li> <li>\\(\\alpha(T, p)\\) is the clausius clapeyron parameter which is a function of temperature and pressure,     such that \\(\\partial q^*/\\partial T = \\alpha q^*\\).</li> </ul> <p>If <code>cape_form=True</code>, will replace both \\(\\epsilon\\) terms with a single \\(CAPE\\) anomaly change: \\(\\gamma_{\\delta T_{FT}}\\frac{\\delta CAPE[x]}{R^{\\dagger}\\delta \\overline{T}_s}\\).</p> Definition of CAPE <p>\\(CAPE = R^{\\dagger} (T_{FT,\\epsilon=0} - T_{FT})\\) where \\(T_{FT,\\epsilon=0}\\) is the free tropospheric temperature which would occur if \\(\\epsilon=0\\), all else the same. I.e. this is the parcel rather than environmental temperature at \\(p_{FT}\\).</p> <p>Parameters:</p> Name Type Description Default <code>temp_surf_ref</code> <code>ndarray</code> <p><code>float [n_exp]</code> \\(\\tilde{T}_s\\) Reference near surface temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K. We assume <code>n_exp=2</code>.</p> required <code>temp_surf_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(T_s(x)\\) <code>temp_surf_quant[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface temperature of experiment <code>i</code>. Units: K. Note that <code>quant_use</code> is not provided as not needed by this function, but is likely to be <code>np.arange(1, 100)</code> - leave out <code>x=0</code> as doesn't really make sense to consider \\(0^{th}\\) percentile of a quantity.</p> required <code>r_ref</code> <code>ndarray</code> <p><code>float [n_exp]</code> \\(\\tilde{r}_s\\) Reference near surface relative humidity of each simulation. Units: dimensionless (from 0 to 1).</p> required <code>r_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(r_s[x]\\) <code>r_quant[i, j]</code> is near-surface relative humidity, averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: dimensionless.</p> required <code>temp_ft_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(T_{FT}[x]\\) <code>temp_ft_quant[i, j]</code> is temperature at <code>pressure_ft</code>, averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: kg/kg.</p> required <code>epsilon_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(\\epsilon[x]\\) <code>epsilon_quant[i, j]</code> is \\(\\epsilon = h_s - h^*_{FT}\\), averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: kJ/kg.</p> required <code>pressure_surf_ref</code> <code>float</code> <p>Pressure at near-surface for reference day, \\(p_s\\), in Pa.</p> required <code>pressure_ft_ref</code> <code>float</code> <p>Pressure at free troposphere level for reference day, \\(p_{FT}\\), in Pa.</p> required <code>epsilon_ref</code> <code>Optional[ndarray]</code> <p><code>float [n_exp]</code> \\(\\tilde{\\epsilon}_s\\) Reference value of \\(\\epsilon = h_s - h^*_{FT}\\), where \\(h_s\\) is near-surface MSE and \\(h^*_{FT}\\) is saturated MSE at <code>pressure_ft</code>. If not given, weill set to 0. Units: kJ/kg.</p> <code>None</code> <code>z_approx_ref</code> <code>Optional[ndarray]</code> <p><code>float [n_exp]</code> \\(\\tilde{A}_z\\) The exact equation for modified MSE is given by: \\(h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon = (c_p + R^{\\dagger})T_{FT} + L_vq^*(T_{FT}, p_{FT}) + A_z\\) where \\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\) and \\(A_z\\) quantifies the error due to approximation of geopotential height, as relating to temperature. Here you have the option of specifying the reference \\(A_z\\) for each simulation. If not provided, will set to 0. Units: kJ/kg.</p> <code>None</code> <code>include_non_linear</code> <code>bool</code> <p>If <code>True</code>, will include the approximate values of \\(A_{\\delta \\Delta T_{FT}}\\), \\(A_{\\delta r}[x]\\) and \\(A_{\\Delta T_s \\Delta r}[x]\\) in <code>info_cont</code> with the names <code>nl_temp_ft_anom_change</code>, <code>nl_r_change</code> and <code>nl_anom_temp_s_r</code> respectively. These are obtained using <code>get_approx_terms</code> with <code>simple=True</code>. These will also be included in the <code>scale_factor</code> theory.</p> <code>False</code> <code>cape_form</code> <code>bool</code> <p>If <code>True</code>, scaling factor is in \\(CAPE\\) rather than \\(\\epsilon\\) form, so a single <code>cape_anom_change</code> value returned in all dictionaries, instead of the <code>epsilon_change</code> and <code>epsilon_anom</code> values. <code>scale_factor</code> will also be adjusted to reflect this form.</p> <code>False</code> <code>pressure_surf_quant</code> <code>Optional[ndarray]</code> <p><code>float [n_exp, n_quant]</code> \\(p_s[x]\\) To compute \\(CAPE\\), require surface pressure conditioned on \\(x\\) days. If not given, will assume same as <code>pressure_surf_ref</code>. Only required if <code>cape_form=True</code>.</p> <code>None</code> <code>pressure_ft_quant</code> <code>Optional[ndarray]</code> <p><code>float [n_exp, n_quant]</code> \\(p_s[x]\\) To compute \\(CAPE\\), require FT pressure conditioned on \\(x\\) days. If not given, will assume same as <code>pressure_ft_ref</code>. Only required if <code>cape_form=True</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scale_factor</code> <code>ndarray</code> <p><code>float [n_quant]</code> <code>scale_factor[i]</code> refers to the theoretical temperature difference between experiments for percentile <code>quant_use[i]</code>, relative to the reference temperature change, \\(\\delta \\tilde{T_s}\\).</p> <code>gamma</code> <code>dict</code> <p>This is the dictionary output by <code>get_sensitivity_factors</code></p> <code>info_var</code> <code>dict</code> <p>For each <code>key</code> in <code>gamma</code>, this dictionary has an entry for the same <code>key</code> which equals the dimensionless variable which multiplies <code>gamma[key]</code> in the equation for \\(\\frac{\\delta \\hat{T}_s(x)}{\\delta \\tilde{T}_s}\\):</p> <ul> <li><code>temp_ft_change</code>: \\(\\frac{\\delta T_{FT}[x]}{\\delta \\tilde{T}_s}\\)</li> <li><code>r_change</code>: \\(\\frac{\\tilde{T}_s}{\\tilde{r}_s} \\frac{\\delta r_s[x]}{\\delta \\tilde{T}_s}\\)</li> <li><code>epsilon_change</code>: \\(\\frac{\\delta \\epsilon[x]}{\\tilde{\\beta}_{s1} \\delta \\tilde{T}_s}\\)     Only returned if <code>cape_form=False</code>.</li> <li><code>temp_anom</code>: \\(\\frac{\\Delta T_s(x)}{\\tilde{T}_s}\\)</li> <li><code>r_anom</code>: \\(\\frac{\\Delta r[x]}{\\tilde{r}_s}\\)</li> <li><code>epsilon_anom</code>: \\(\\frac{\\Delta \\epsilon[x]}{c_p \\tilde{T}_s}\\)     Only returned if <code>cape_form=False</code>.</li> <li><code>r_ref_change</code>: \\(\\frac{\\delta \\tilde{r}_s}{\\tilde{r}_s}\\)</li> <li><code>cape_anom_change</code>: \\(\\frac{\\delta \\Delta CAPE[x]}{R^{\\dagger} \\delta \\tilde{T}_s}\\)</li> </ul> <p>All are arrays of size <code>float [n_quant]</code>, except <code>r_ref_change</code> which is just a single <code>float</code>.</p> <code>info_cont</code> <code>dict</code> <p>Dictionary containing <code>gamma[key] x info_var[key]</code> for each <code>key</code> in <code>gamma</code>. This gives the contribution from each physical mechanism to the overall scale factor. If <code>include_non_linear=True</code>, will also include <code>nl_temp_ft_anom_change</code>, <code>nl_r_change</code> and <code>nl_anom_temp_s_r</code> which arise from including the most significant approximations.</p> Source code in <code>isca_tools/thesis/adiabat_theory2.py</code> <pre><code>def get_scale_factor_theory(temp_surf_ref: np.ndarray, temp_surf_quant: np.ndarray, r_ref: np.ndarray,\n                            r_quant: np.ndarray, temp_ft_quant: np.ndarray,\n                            epsilon_quant: np.ndarray,\n                            pressure_surf_ref: float, pressure_ft_ref: float,\n                            epsilon_ref: Optional[np.ndarray] = None,\n                            z_approx_ref: Optional[np.ndarray] = None,\n                            include_non_linear: bool = False,\n                            cape_form: bool = False,\n                            pressure_surf_quant: Optional[np.ndarray] = None,\n                            pressure_ft_quant: Optional[np.ndarray] = None) -&gt; Tuple[np.ndarray, dict, dict, dict]:\n    \"\"\"\n    Calculates the theoretical near-surface temperature change for percentile $x$, $\\delta \\hat{T}_s(x)$, relative\n    to the reference temperature change, $\\delta \\\\tilde{T}_s$. The theoretical scale factor is given by:\n\n    $$\n    \\\\begin{align}\n    \\\\frac{\\delta \\hat{T}_s(x)}{\\delta \\\\tilde{T}_s} &amp;= \\gamma_{\\delta T_{FT}}\\\\frac{\\delta T_{FT}[x]}{\\delta \\\\tilde{T}_s}\n    - \\gamma_{\\delta r}\\\\frac{\\\\tilde{T}_s}{\\\\tilde{r}_s} \\\\frac{\\delta r_s[x]}{\\delta \\\\tilde{T}_s}\n    + \\gamma_{\\delta \\epsilon} \\\\frac{\\delta \\epsilon[x]}{c_p \\delta \\\\tilde{T}_s} \\\\\\\\\n    &amp;+ \\gamma_{\\Delta T_s} \\\\frac{\\Delta T_s(x)}{\\\\tilde{T}_s}\n    - \\gamma_{\\Delta r} \\\\frac{\\Delta r[x]}{\\\\tilde{r}_s}\n    - \\gamma_{\\Delta \\epsilon} \\\\frac{\\Delta \\epsilon[x]}{c_p \\\\tilde{T}_s}\n    - \\gamma_{\\delta \\\\tilde{r}}\\\\frac{\\delta \\\\tilde{r}_s}{\\\\tilde{r}_s}\n    \\\\end{align}\n    $$\n\n    where the dimensionless $\\gamma$ parameters quantify the significance of different physical mechanisms in causing\n    a change in the near-surface temperature distribution. These are given by the `get_sensitivity_factors` function.\n\n    The approximations which cause $\\\\frac{\\delta \\hat{T}_s(x)}{\\delta \\\\tilde{T}_s}$ to differ from the exact\n    scale factor are given in `get_approx_terms`.\n\n    ??? note \"Reference Quantities\"\n        The reference quantities, $\\\\tilde{\\chi}$ are free to be chosen by the user. For ease of interpretation,\n        I propose the following, where $\\overline{\\chi}$ is the mean value of $\\chi$ across all days:\n\n        * $\\\\tilde{T}_s = \\overline{T_s}; \\delta \\\\tilde{T}_s = \\delta \\overline{T_s}$\n        * $\\\\tilde{r}_s = \\overline{r_s}; \\delta \\\\tilde{r}_s = 0$\n        * $\\\\tilde{\\epsilon} = 0; \\delta \\\\tilde{\\epsilon} = 0$\n        * $\\\\tilde{A}_z = \\overline{A}_z; \\delta \\\\tilde{A}_z = 0$\n\n        Given the choice of these four reference variables and their changes with warming, the reference free\n        troposphere temperature, $\\\\tilde{T}_{FT}$, can be computed according to the definition of $\\\\tilde{h}^{\\dagger}$:\n\n        $\\\\tilde{h}^{\\dagger} = (c_p - R^{\\dagger})\\\\tilde{T}_s + L_v \\\\tilde{q}_s - \\\\tilde{\\epsilon} =\n            (c_p + R^{\\dagger}) \\\\tilde{T}_{FT} + L_v q^*(\\\\tilde{T}_{FT}, p_{FT}) + \\\\tilde{A}_z$\n\n        If `cape_form=True`, the reference CAPE, $\\\\widetilde{CAPE}$, will also be computed from these four variables\n        using `get_cape_approx`. This will be 0 if $\\\\tilde{\\epsilon}=0$.\n\n        Poor choice of reference quantities may cause the theoretical scale factor to be a bad approximation. If this\n        is the case, `get_approx_terms` can be used to investigate what is causing the theory to break down.\n\n    ??? note \"Terms in equation\"\n        * $h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon =\n            (c_p + R^{\\dagger}) T_{FT} + L_v q^*_{FT} + A_z$\n            where we used an approximate relation to replace $z_{FT}$ in $h^*_{FT}$.\n        * $\\epsilon = h_s - h^*_{FT}$, where $h_s$ is near-surface MSE (at $p_s$) and\n            $h^*_{FT}$ is free tropospheric saturated MSE (at $p_{FT}$).\n        * $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$\n        * $\\\\Delta \\chi[x] = \\chi[x] - \\\\tilde{\\chi}$\n        * $\\chi[x]$ is the value of $\\chi$ averaged over all days\n            where near-surface temperature, $T_s$, is between percentile $x-0.5$ and $x+0.5$.\n        * $\\\\tilde{\\chi}$ is the reference value of $\\chi$, which is free to be chosen.\n        * $\\\\beta_{FT1} = \\\\frac{\\partial h^{\\\\dagger}}{\\partial T_{FT}} = c_p + R^{\\dagger} + L_v \\\\alpha_{FT} q_{FT}^*$\n        * $\\\\beta_{FT2} = T_{FT} \\\\frac{\\partial^2h^{\\\\dagger}}{\\partial T_{FT}^2} =\n            T_{FT}\\\\frac{d\\\\beta_{FT1}}{d T_{FT}} = L_v \\\\alpha_{FT} q_{FT}^*(\\\\alpha_{FT} T_{FT} - 2)$\n        * $\\\\beta_{s1} = \\\\frac{\\partial h^{\\dagger}}{\\partial T_s} = c_p - R^{\\dagger} + L_v \\\\alpha_s q_s$\n        * $\\\\beta_{s2} = T_s \\\\frac{\\partial^2 h^{\\dagger}}{\\partial T_s^2} =\n            T_s\\\\frac{\\partial \\\\beta_{s1}}{\\partial T_s} = L_v \\\\alpha_s q_s(\\\\alpha_s T_s - 2)$\n        * $\\mu=\\\\frac{L_v \\\\alpha_s q_s}{\\\\beta_{s1}}$\n        * $q = rq^*$ where $q$ is the specific humidity, $r$ is relative humidity and $q^*(T, p)$\n            is saturation specific humidity which is a function of temperature and pressure.\n        * $\\\\alpha(T, p)$ is the clausius clapeyron parameter which is a function of temperature and pressure,\n            such that $\\partial q^*/\\partial T = \\\\alpha q^*$.\n\n    If `cape_form=True`, will replace both $\\epsilon$ terms with a single $CAPE$ anomaly change:\n    $\\gamma_{\\delta T_{FT}}\\\\frac{\\delta CAPE[x]}{R^{\\dagger}\\delta \\overline{T}_s}$.\n\n    ??? note \"Definition of CAPE\"\n        $CAPE = R^{\\dagger} (T_{FT,\\epsilon=0} - T_{FT})$ where $T_{FT,\\epsilon=0}$ is the free tropospheric\n        temperature which would occur if $\\epsilon=0$, all else the same. I.e. this is the parcel rather\n        than environmental temperature at $p_{FT}$.\n\n    Args:\n        temp_surf_ref: `float [n_exp]` $\\\\tilde{T}_s$&lt;/br&gt;\n            Reference near surface temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*. We assume `n_exp=2`.\n        temp_surf_quant: `float [n_exp, n_quant]` $T_s(x)$ &lt;/br&gt;\n            `temp_surf_quant[i, j]` is the percentile `quant_use[j]` of near surface temperature of\n            experiment `i`. Units: *K*.&lt;/br&gt;\n            Note that `quant_use` is not provided as not needed by this function, but is likely to be\n            `np.arange(1, 100)` - leave out `x=0` as doesn't really make sense to consider $0^{th}$ percentile\n            of a quantity.\n        r_ref: `float [n_exp]` $\\\\tilde{r}_s$&lt;/br&gt;\n            Reference near surface relative humidity of each simulation. Units: dimensionless (from 0 to 1).\n        r_quant: `float [n_exp, n_quant]` $r_s[x]$&lt;/br&gt;\n            `r_quant[i, j]` is near-surface relative humidity, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: dimensionless.\n        temp_ft_quant: `float [n_exp, n_quant]` $T_{FT}[x]$&lt;/br&gt;\n            `temp_ft_quant[i, j]` is temperature at `pressure_ft`, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *kg/kg*.\n        epsilon_quant: `float [n_exp, n_quant]` $\\epsilon[x]$&lt;/br&gt;\n            `epsilon_quant[i, j]` is $\\epsilon = h_s - h^*_{FT}$, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *kJ/kg*.\n        pressure_surf_ref:\n            Pressure at near-surface for reference day, $p_s$, in *Pa*.\n        pressure_ft_ref:\n            Pressure at free troposphere level for reference day, $p_{FT}$, in *Pa*.\n        epsilon_ref: `float [n_exp]` $\\\\tilde{\\epsilon}_s$&lt;/br&gt;\n            Reference value of $\\epsilon = h_s - h^*_{FT}$, where $h_s$ is near-surface MSE and\n            $h^*_{FT}$ is saturated MSE at `pressure_ft`. If not given, weill set to 0. Units: *kJ/kg*.\n        z_approx_ref: `float [n_exp]` $\\\\tilde{A}_z$&lt;/br&gt;\n            The exact equation for modified MSE is given by: $h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v q_s\n            - \\epsilon = (c_p + R^{\\dagger})T_{FT} + L_vq^*(T_{FT}, p_{FT}) + A_z$\n            where $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$ and $A_z$ quantifies the error due to\n            approximation of geopotential height, as relating to temperature.&lt;/br&gt;\n            Here you have the option of specifying the reference $A_z$ for each simulation. If not provided,\n            will set to 0. Units: *kJ/kg*.\n        include_non_linear: If `True`, will include the approximate values of\n            $A_{\\delta \\Delta T_{FT}}$, $A_{\\delta r}[x]$ and $A_{\\Delta T_s \\Delta r}[x]$ in `info_cont` with the\n            names `nl_temp_ft_anom_change`, `nl_r_change` and `nl_anom_temp_s_r` respectively.\n            These are obtained using `get_approx_terms` with `simple=True`.\n            These will also be included in the `scale_factor` theory.\n        cape_form: If `True`, scaling factor is in $CAPE$ rather than $\\epsilon$ form, so a single `cape_anom_change`\n            value returned in all dictionaries, instead of the `epsilon_change` and `epsilon_anom` values.\n            `scale_factor` will also be adjusted to reflect this form.\n        pressure_surf_quant: `float [n_exp, n_quant]` $p_s[x]$&lt;/br&gt;\n            To compute $CAPE$, require surface pressure conditioned on $x$ days. If not given, will assume same\n            as `pressure_surf_ref`. Only required if `cape_form=True`.\n        pressure_ft_quant: `float [n_exp, n_quant]` $p_s[x]$&lt;/br&gt;\n            To compute $CAPE$, require FT pressure conditioned on $x$ days. If not given, will assume same\n            as `pressure_ft_ref`. Only required if `cape_form=True`.\n\n    Returns:\n        scale_factor: `float [n_quant]`&lt;/br&gt;\n            `scale_factor[i]` refers to the theoretical temperature difference between experiments\n            for percentile `quant_use[i]`, relative to the reference temperature change, $\\delta \\\\tilde{T_s}$.\n        gamma: This is the dictionary output by `get_sensitivity_factors`\n        info_var: For each `key` in `gamma`, this dictionary has an entry for the same `key` which equals the dimensionless\n            variable which multiplies `gamma[key]` in the equation for $\\\\frac{\\delta \\hat{T}_s(x)}{\\delta \\\\tilde{T}_s}$:\n\n            * `temp_ft_change`: $\\\\frac{\\delta T_{FT}[x]}{\\delta \\\\tilde{T}_s}$\n            * `r_change`: $\\\\frac{\\\\tilde{T}_s}{\\\\tilde{r}_s} \\\\frac{\\delta r_s[x]}{\\delta \\\\tilde{T}_s}$\n            * `epsilon_change`: $\\\\frac{\\delta \\epsilon[x]}{\\\\tilde{\\\\beta}_{s1} \\delta \\\\tilde{T}_s}$\n                Only returned if `cape_form=False`.\n            * `temp_anom`: $\\\\frac{\\Delta T_s(x)}{\\\\tilde{T}_s}$\n            * `r_anom`: $\\\\frac{\\Delta r[x]}{\\\\tilde{r}_s}$\n            * `epsilon_anom`: $\\\\frac{\\Delta \\epsilon[x]}{c_p \\\\tilde{T}_s}$\n                Only returned if `cape_form=False`.\n            * `r_ref_change`: $\\\\frac{\\delta \\\\tilde{r}_s}{\\\\tilde{r}_s}$\n            * `cape_anom_change`: $\\\\frac{\\delta \\Delta CAPE[x]}{R^{\\dagger} \\delta \\\\tilde{T}_s}$\n\n            All are arrays of size `float [n_quant]`, except `r_ref_change` which is just a single `float`.\n        info_cont: Dictionary containing `gamma[key] x info_var[key]` for each `key` in `gamma`. This gives\n            the contribution from each physical mechanism to the overall scale factor.&lt;/br&gt;\n            If `include_non_linear=True`, will also include `nl_temp_ft_anom_change`, `nl_r_change`\n            and `nl_anom_temp_s_r` which arise from including the most significant approximations.\n\n    \"\"\"\n    n_exp, n_quant = temp_surf_quant.shape\n    if epsilon_ref is None:\n        epsilon_ref = np.zeros(n_exp)\n    gamma = get_sensitivity_factors(temp_surf_ref, r_ref, pressure_surf_ref, pressure_ft_ref, epsilon_ref, z_approx_ref,\n                                    cape_form)\n    sphum_ref = r_ref * sphum_sat(temp_surf_ref, pressure_surf_ref)\n    R_mod, _, _, beta_s1, beta_s2, _, mu = get_theory_prefactor_terms(temp_surf_ref, pressure_surf_ref, pressure_ft_ref,\n                                                                      sphum_ref)\n    temp_surf_ref_change = np.diff(temp_surf_ref, axis=0).squeeze()\n    # Get non-dimensional variables which multiply gamma\n    # Multiply epsilon by 1000 to get in correct units of J/kg\n    info_var = {'r_ref_change': np.diff(r_ref, axis=0).squeeze()/r_ref[0],\n                'temp_ft_change': np.diff(temp_ft_quant, axis=0).squeeze()/temp_surf_ref_change,\n                'r_change': np.diff(r_quant, axis=0).squeeze()/r_ref[0, np.newaxis] * temp_surf_ref[0]/temp_surf_ref_change,\n                'temp_anom': (temp_surf_quant[0]-temp_surf_ref[0]) / temp_surf_ref[0],\n                'r_anom': (r_quant[0]-r_ref[0]) / r_ref[0]}\n    if cape_form:\n        cape_ref = np.zeros(n_exp)\n        cape_quant = np.zeros((n_exp, n_quant))\n        for i in range(n_exp):\n            if epsilon_ref[i] != 0: # if epsilon_ref=0, then cape_ref=0 too\n                # For ref, don't have temp_ft so compute from z_approx and epsilon\n                cape_ref[i] = get_cape_approx(temp_surf_ref[i], r_ref[i], pressure_surf_ref, pressure_ft_ref,\n                                              epsilon=epsilon_ref[i], z_approx=z_approx_ref[i])[0]\n            # For quant, don't have z_approx so compute from temp_ft and epsilon\n            cape_quant[i] = get_cape_approx(temp_surf_quant[i], r_quant[i],\n                                            pressure_surf_ref if pressure_surf_quant is None else pressure_surf_quant[i],\n                                            pressure_ft_ref if pressure_ft_quant is None else pressure_ft_quant[i],\n                                            epsilon=epsilon_quant[i], temp_ft=temp_ft_quant[i])[0]\n        info_var['cape_change'] = np.diff(cape_quant, axis=0).squeeze()*1000 / R_mod / temp_surf_ref_change\n    else:\n        info_var['epsilon_change'] = np.diff(epsilon_quant, axis=0).squeeze()*1000/c_p/temp_surf_ref_change\n        info_var['epsilon_anom'] = (epsilon_quant[0]-epsilon_ref[0])*1000 / c_p / temp_surf_ref[0]\n    # All gamma are positive, so sign below is to multiply gamma in equation\n    coef_sign = {'r_ref_change': -1, 'temp_ft_change': 1, 'r_change': -1, 'epsilon_change': 1,\n                 'temp_anom': 1, 'r_anom': -1, 'epsilon_anom': -1, 'cape_change': 1}\n\n    # Get contribution from each term\n    info_cont = {}\n    for key in gamma:\n        info_cont[key] = coef_sign[key] * gamma[key] * info_var[key]\n\n    if include_non_linear:\n        # Add non-linear terms\n        approx_var = get_approx_terms(temp_surf_ref, temp_surf_quant, r_ref, r_quant, temp_ft_quant,\n                                      epsilon_quant, pressure_surf_ref, pressure_ft_ref, epsilon_ref,\n                                      z_approx_ref, simple=True, cape_form=cape_form)[0]\n        for key in ['temp_ft_anom_change', 'r_change', 'anom_temp_s_r']:\n            info_cont['nl_'+key] = approx_var[key]\n\n    final_answer = np.asarray(sum([info_cont[key] for key in info_cont]))\n    return final_answer, gamma, info_var, info_cont\n</code></pre>"},{"location":"code/thesis/adiabat_theory2/#isca_tools.thesis.adiabat_theory2.get_scale_factor_theory_numerical","title":"<code>get_scale_factor_theory_numerical(temp_surf_ref, temp_surf_quant, r_ref, r_quant, temp_ft_quant, epsilon_quant, pressure_surf_ref, pressure_ft_ref, epsilon_ref=None, z_approx_ref=None)</code>","text":"<p>Calculates the theoretical near-surface temperature change for percentile \\(x\\), \\(\\delta \\hat{T}_s(x)\\), relative to the reference temperature change, \\(\\delta \\tilde{T}_s\\). The theoretical scale factor is given by the linear sum of mechanisms assumed independent: either anomalous values in current climate, \\(\\Delta\\), or due to the variation in that parameter with warming, \\(\\delta\\).</p> Reference Quantities <p>The reference quantities, \\(\\tilde{\\chi}\\) are free to be chosen by the user. For ease of interpretation, I propose the following, where \\(\\overline{\\chi}\\) is the mean value of \\(\\chi\\) across all days:</p> <ul> <li>\\(\\tilde{T}_s = \\overline{T_s}; \\delta \\tilde{T}_s = \\delta \\overline{T_s}\\)</li> <li>\\(\\tilde{r}_s = \\overline{r_s}; \\delta \\tilde{r}_s = 0\\)</li> <li>\\(\\tilde{\\epsilon} = 0; \\delta \\tilde{\\epsilon} = 0\\)</li> <li>\\(\\tilde{A}_z = \\overline{A}_z; \\delta \\tilde{A}_z = 0\\)</li> </ul> <p>Given the choice of these four reference variables and their changes with warming, the reference free troposphere temperature, \\(\\tilde{T}_{FT}\\), can be computed according to the definition of \\(\\tilde{h}^{\\dagger}\\):</p> <p>\\(\\tilde{h}^{\\dagger} = (c_p - R^{\\dagger})\\tilde{T}_s + L_v \\tilde{q}_s - \\tilde{\\epsilon} =     (c_p + R^{\\dagger}) \\tilde{T}_{FT} + L_v q^*(\\tilde{T}_{FT}, p_{FT}) + \\tilde{A}_z\\)</p> <p>If <code>cape_form=True</code>, the reference CAPE, \\(\\widetilde{CAPE}\\), will also be computed from these four variables using <code>get_cape_approx</code>. This will be 0 if \\(\\tilde{\\epsilon}=0\\).</p> <p>Poor choice of reference quantities may cause the theoretical scale factor to be a bad approximation. If this is the case, <code>get_approx_terms</code> can be used to investigate what is causing the theory to break down.</p> Terms in equation <ul> <li>\\(h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon =     (c_p + R^{\\dagger}) T_{FT} + L_v q^*_{FT} + A_z\\)     where we used an approximate relation to replace \\(z_{FT}\\) in \\(h^*_{FT}\\).</li> <li>\\(\\epsilon = h_s - h^*_{FT}\\), where \\(h_s\\) is near-surface MSE (at \\(p_s\\)) and     \\(h^*_{FT}\\) is free tropospheric saturated MSE (at \\(p_{FT}\\)).</li> <li>\\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\)</li> <li>\\(\\Delta \\chi[x] = \\chi[x] - \\tilde{\\chi}\\)</li> <li>\\(\\chi[x]\\) is the value of \\(\\chi\\) averaged over all days     where near-surface temperature, \\(T_s\\), is between percentile \\(x-0.5\\) and \\(x+0.5\\).</li> <li>\\(\\tilde{\\chi}\\) is the reference value of \\(\\chi\\), which is free to be chosen.</li> <li>\\(\\beta_{FT1} = \\frac{\\partial h^{\\dagger}}{\\partial T_{FT}} = c_p + R^{\\dagger} + L_v \\alpha_{FT} q_{FT}^*\\)</li> <li>\\(\\beta_{FT2} = T_{FT} \\frac{\\partial^2h^{\\dagger}}{\\partial T_{FT}^2} =     T_{FT}\\frac{d\\beta_{FT1}}{d T_{FT}} = L_v \\alpha_{FT} q_{FT}^*(\\alpha_{FT} T_{FT} - 2)\\)</li> <li>\\(\\beta_{s1} = \\frac{\\partial h^{\\dagger}}{\\partial T_s} = c_p - R^{\\dagger} + L_v \\alpha_s q_s\\)</li> <li>\\(\\beta_{s2} = T_s \\frac{\\partial^2 h^{\\dagger}}{\\partial T_s^2} =     T_s\\frac{\\partial \\beta_{s1}}{\\partial T_s} = L_v \\alpha_s q_s(\\alpha_s T_s - 2)\\)</li> <li>\\(\\mu=\\frac{L_v \\alpha_s q_s}{\\beta_{s1}}\\)</li> <li>\\(q = rq^*\\) where \\(q\\) is the specific humidity, \\(r\\) is relative humidity and \\(q^*(T, p)\\)     is saturation specific humidity which is a function of temperature and pressure.</li> <li>\\(\\alpha(T, p)\\) is the clausius clapeyron parameter which is a function of temperature and pressure,     such that \\(\\partial q^*/\\partial T = \\alpha q^*\\).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>temp_surf_ref</code> <code>ndarray</code> <p><code>float [n_exp]</code> \\(\\tilde{T}_s\\) Reference near surface temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K. We assume <code>n_exp=2</code>.</p> required <code>temp_surf_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(T_s(x)\\) <code>temp_surf_quant[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface temperature of experiment <code>i</code>. Units: K. Note that <code>quant_use</code> is not provided as not needed by this function, but is likely to be <code>np.arange(1, 100)</code> - leave out <code>x=0</code> as doesn't really make sense to consider \\(0^{th}\\) percentile of a quantity.</p> required <code>r_ref</code> <code>ndarray</code> <p><code>float [n_exp]</code> \\(\\tilde{r}_s\\) Reference near surface relative humidity of each simulation. Units: dimensionless (from 0 to 1).</p> required <code>r_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(r_s[x]\\) <code>r_quant[i, j]</code> is near-surface relative humidity, averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: dimensionless.</p> required <code>temp_ft_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(T_{FT}[x]\\) <code>temp_ft_quant[i, j]</code> is temperature at <code>pressure_ft</code>, averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: kg/kg.</p> required <code>epsilon_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> \\(\\epsilon[x]\\) <code>epsilon_quant[i, j]</code> is \\(\\epsilon = h_s - h^*_{FT}\\), averaged over all days with near-surface temperature  corresponding to the quantile <code>quant_use[j]</code>, for experiment <code>i</code>. Units: kJ/kg.</p> required <code>pressure_surf_ref</code> <code>float</code> <p>Pressure at near-surface for reference day, \\(p_s\\), in Pa.</p> required <code>pressure_ft_ref</code> <code>float</code> <p>Pressure at free troposphere level for reference day, \\(p_{FT}\\), in Pa.</p> required <code>epsilon_ref</code> <code>Optional[ndarray]</code> <p><code>float [n_exp]</code> \\(\\tilde{\\epsilon}_s\\) Reference value of \\(\\epsilon = h_s - h^*_{FT}\\), where \\(h_s\\) is near-surface MSE and \\(h^*_{FT}\\) is saturated MSE at <code>pressure_ft</code>. If not given, weill set to 0. Units: kJ/kg.</p> <code>None</code> <code>z_approx_ref</code> <code>Optional[ndarray]</code> <p><code>float [n_exp]</code> \\(\\tilde{A}_z\\) The exact equation for modified MSE is given by: \\(h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon = (c_p + R^{\\dagger})T_{FT} + L_vq^*(T_{FT}, p_{FT}) + A_z\\) where \\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\) and \\(A_z\\) quantifies the error due to approximation of geopotential height, as relating to temperature. Here you have the option of specifying the reference \\(A_z\\) for each simulation. If not provided, will set to 0. Units: kJ/kg.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scale_factor</code> <code>ndarray</code> <p><code>float [n_quant]</code> <code>scale_factor[i]</code> refers to the theoretical temperature difference between experiments for percentile <code>quant_use[i]</code>, relative to the reference temperature change, \\(\\delta \\tilde{T_s}\\).</p> <code>info_cont</code> <code>dict</code> <p>Dictionary containing contribution from each mechanism. This gives the contribution from each physical mechanism to the overall scale factor.</p> Source code in <code>isca_tools/thesis/adiabat_theory2.py</code> <pre><code>def get_scale_factor_theory_numerical(temp_surf_ref: np.ndarray, temp_surf_quant: np.ndarray, r_ref: np.ndarray,\n                                      r_quant: np.ndarray, temp_ft_quant: np.ndarray,\n                                      epsilon_quant: np.ndarray,\n                                      pressure_surf_ref: float, pressure_ft_ref: float,\n                                      epsilon_ref: Optional[np.ndarray] = None,\n                                      z_approx_ref: Optional[np.ndarray] = None) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"\n    Calculates the theoretical near-surface temperature change for percentile $x$, $\\delta \\hat{T}_s(x)$, relative\n    to the reference temperature change, $\\delta \\\\tilde{T}_s$. The theoretical scale factor is given by the linear\n    sum of mechanisms assumed independent: either anomalous values in current climate, $\\Delta$, or due to the\n    variation in that parameter with warming, $\\delta$.\n\n    ??? note \"Reference Quantities\"\n        The reference quantities, $\\\\tilde{\\chi}$ are free to be chosen by the user. For ease of interpretation,\n        I propose the following, where $\\overline{\\chi}$ is the mean value of $\\chi$ across all days:\n\n        * $\\\\tilde{T}_s = \\overline{T_s}; \\delta \\\\tilde{T}_s = \\delta \\overline{T_s}$\n        * $\\\\tilde{r}_s = \\overline{r_s}; \\delta \\\\tilde{r}_s = 0$\n        * $\\\\tilde{\\epsilon} = 0; \\delta \\\\tilde{\\epsilon} = 0$\n        * $\\\\tilde{A}_z = \\overline{A}_z; \\delta \\\\tilde{A}_z = 0$\n\n        Given the choice of these four reference variables and their changes with warming, the reference free\n        troposphere temperature, $\\\\tilde{T}_{FT}$, can be computed according to the definition of $\\\\tilde{h}^{\\dagger}$:\n\n        $\\\\tilde{h}^{\\dagger} = (c_p - R^{\\dagger})\\\\tilde{T}_s + L_v \\\\tilde{q}_s - \\\\tilde{\\epsilon} =\n            (c_p + R^{\\dagger}) \\\\tilde{T}_{FT} + L_v q^*(\\\\tilde{T}_{FT}, p_{FT}) + \\\\tilde{A}_z$\n\n        If `cape_form=True`, the reference CAPE, $\\\\widetilde{CAPE}$, will also be computed from these four variables\n        using `get_cape_approx`. This will be 0 if $\\\\tilde{\\epsilon}=0$.\n\n        Poor choice of reference quantities may cause the theoretical scale factor to be a bad approximation. If this\n        is the case, `get_approx_terms` can be used to investigate what is causing the theory to break down.\n\n    ??? note \"Terms in equation\"\n        * $h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon =\n            (c_p + R^{\\dagger}) T_{FT} + L_v q^*_{FT} + A_z$\n            where we used an approximate relation to replace $z_{FT}$ in $h^*_{FT}$.\n        * $\\epsilon = h_s - h^*_{FT}$, where $h_s$ is near-surface MSE (at $p_s$) and\n            $h^*_{FT}$ is free tropospheric saturated MSE (at $p_{FT}$).\n        * $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$\n        * $\\\\Delta \\chi[x] = \\chi[x] - \\\\tilde{\\chi}$\n        * $\\chi[x]$ is the value of $\\chi$ averaged over all days\n            where near-surface temperature, $T_s$, is between percentile $x-0.5$ and $x+0.5$.\n        * $\\\\tilde{\\chi}$ is the reference value of $\\chi$, which is free to be chosen.\n        * $\\\\beta_{FT1} = \\\\frac{\\partial h^{\\\\dagger}}{\\partial T_{FT}} = c_p + R^{\\dagger} + L_v \\\\alpha_{FT} q_{FT}^*$\n        * $\\\\beta_{FT2} = T_{FT} \\\\frac{\\partial^2h^{\\\\dagger}}{\\partial T_{FT}^2} =\n            T_{FT}\\\\frac{d\\\\beta_{FT1}}{d T_{FT}} = L_v \\\\alpha_{FT} q_{FT}^*(\\\\alpha_{FT} T_{FT} - 2)$\n        * $\\\\beta_{s1} = \\\\frac{\\partial h^{\\dagger}}{\\partial T_s} = c_p - R^{\\dagger} + L_v \\\\alpha_s q_s$\n        * $\\\\beta_{s2} = T_s \\\\frac{\\partial^2 h^{\\dagger}}{\\partial T_s^2} =\n            T_s\\\\frac{\\partial \\\\beta_{s1}}{\\partial T_s} = L_v \\\\alpha_s q_s(\\\\alpha_s T_s - 2)$\n        * $\\mu=\\\\frac{L_v \\\\alpha_s q_s}{\\\\beta_{s1}}$\n        * $q = rq^*$ where $q$ is the specific humidity, $r$ is relative humidity and $q^*(T, p)$\n            is saturation specific humidity which is a function of temperature and pressure.\n        * $\\\\alpha(T, p)$ is the clausius clapeyron parameter which is a function of temperature and pressure,\n            such that $\\partial q^*/\\partial T = \\\\alpha q^*$.\n\n    Args:\n        temp_surf_ref: `float [n_exp]` $\\\\tilde{T}_s$&lt;/br&gt;\n            Reference near surface temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*. We assume `n_exp=2`.\n        temp_surf_quant: `float [n_exp, n_quant]` $T_s(x)$ &lt;/br&gt;\n            `temp_surf_quant[i, j]` is the percentile `quant_use[j]` of near surface temperature of\n            experiment `i`. Units: *K*.&lt;/br&gt;\n            Note that `quant_use` is not provided as not needed by this function, but is likely to be\n            `np.arange(1, 100)` - leave out `x=0` as doesn't really make sense to consider $0^{th}$ percentile\n            of a quantity.\n        r_ref: `float [n_exp]` $\\\\tilde{r}_s$&lt;/br&gt;\n            Reference near surface relative humidity of each simulation. Units: dimensionless (from 0 to 1).\n        r_quant: `float [n_exp, n_quant]` $r_s[x]$&lt;/br&gt;\n            `r_quant[i, j]` is near-surface relative humidity, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: dimensionless.\n        temp_ft_quant: `float [n_exp, n_quant]` $T_{FT}[x]$&lt;/br&gt;\n            `temp_ft_quant[i, j]` is temperature at `pressure_ft`, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *kg/kg*.\n        epsilon_quant: `float [n_exp, n_quant]` $\\epsilon[x]$&lt;/br&gt;\n            `epsilon_quant[i, j]` is $\\epsilon = h_s - h^*_{FT}$, averaged over all days with near-surface temperature\n             corresponding to the quantile `quant_use[j]`, for experiment `i`. Units: *kJ/kg*.\n        pressure_surf_ref:\n            Pressure at near-surface for reference day, $p_s$, in *Pa*.\n        pressure_ft_ref:\n            Pressure at free troposphere level for reference day, $p_{FT}$, in *Pa*.\n        epsilon_ref: `float [n_exp]` $\\\\tilde{\\epsilon}_s$&lt;/br&gt;\n            Reference value of $\\epsilon = h_s - h^*_{FT}$, where $h_s$ is near-surface MSE and\n            $h^*_{FT}$ is saturated MSE at `pressure_ft`. If not given, weill set to 0. Units: *kJ/kg*.\n        z_approx_ref: `float [n_exp]` $\\\\tilde{A}_z$&lt;/br&gt;\n            The exact equation for modified MSE is given by: $h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v q_s\n            - \\epsilon = (c_p + R^{\\dagger})T_{FT} + L_vq^*(T_{FT}, p_{FT}) + A_z$\n            where $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$ and $A_z$ quantifies the error due to\n            approximation of geopotential height, as relating to temperature.&lt;/br&gt;\n            Here you have the option of specifying the reference $A_z$ for each simulation. If not provided,\n            will set to 0. Units: *kJ/kg*.\n\n    Returns:\n        scale_factor: `float [n_quant]`&lt;/br&gt;\n            `scale_factor[i]` refers to the theoretical temperature difference between experiments\n            for percentile `quant_use[i]`, relative to the reference temperature change, $\\delta \\\\tilde{T_s}$.\n        info_cont: Dictionary containing contribution from each mechanism. This gives\n            the contribution from each physical mechanism to the overall scale factor.&lt;/br&gt;\n\n    \"\"\"\n    n_exp, n_quant = temp_surf_quant.shape\n    if epsilon_ref is None:\n        epsilon_ref = np.zeros(n_exp)\n    if z_approx_ref is None:\n        z_approx_ref = np.zeros(n_exp)\n\n    # Compute temp_ft_ref using base climate reference rh, epsilon and z_approx\n    temp_ft_ref = get_temp_adiabat(temp_surf_ref, r_ref[0] * sphum_sat(temp_surf_ref, pressure_surf_ref),\n                                   pressure_surf_ref, pressure_ft_ref, epsilon=epsilon_ref[0] + z_approx_ref[0])\n\n    # Get error due to approximation of z\n    R_mod = R * np.log(pressure_surf_ref / pressure_ft_ref) / 2\n    mse_mod_quant = moist_static_energy(temp_surf_quant, r_quant * sphum_sat(temp_surf_quant, pressure_surf_ref),\n                                        height=0, c_p_const=c_p - R_mod) - epsilon_quant\n    z_approx_quant = mse_mod_quant - moist_static_energy(temp_ft_quant, sphum_sat(temp_ft_quant, pressure_ft_ref),\n                                                         height=0, c_p_const=c_p + R_mod)\n\n    # Temp_ft change is different if account for ref value changes or not\n    temp_ft_ref_change = {'base': temp_ft_ref[1] - temp_ft_ref[0],\n                          'r_ref': get_temp_adiabat(temp_surf_ref, r_ref * sphum_sat(temp_surf_ref, pressure_surf_ref),\n                                                    pressure_surf_ref, pressure_ft_ref,\n                                                    epsilon=epsilon_ref[0] + z_approx_ref[0])[1] - temp_ft_ref[0],\n                          'epsilon_ref': get_temp_adiabat(temp_surf_ref, r_ref[0] * sphum_sat(temp_surf_ref, pressure_surf_ref),\n                                                          pressure_surf_ref, pressure_ft_ref,\n                                                          epsilon=epsilon_ref + z_approx_ref[0])[1] - temp_ft_ref[0],\n                          'z_approx_ref': get_temp_adiabat(temp_surf_ref, r_ref[0] * sphum_sat(temp_surf_ref, pressure_surf_ref),\n                                                          pressure_surf_ref, pressure_ft_ref,\n                                                          epsilon=epsilon_ref[0] + z_approx_ref)[1] - temp_ft_ref[0],\n                          }\n\n    # Base FT temperature to use depends on if considering anomalies in current climate or not\n    temp_ft0 = {'base': temp_ft_ref[0],\n                'r_anom': get_temp_adiabat(np.full_like(temp_surf_quant[0], temp_surf_ref[0]), r_quant[0] * sphum_sat(temp_surf_ref[0], pressure_surf_ref),\n                                           pressure_surf_ref, pressure_ft_ref, epsilon=epsilon_ref[0] + z_approx_ref[0]),\n                'temp_anom': get_temp_adiabat(temp_surf_quant[0], r_ref[0] * sphum_sat(temp_surf_quant[0], pressure_surf_ref),\n                                              pressure_surf_ref, pressure_ft_ref, epsilon=epsilon_ref[0] + z_approx_ref[0]),\n                'epsilon_anom': get_temp_adiabat(np.full_like(temp_surf_quant[0], temp_surf_ref[0]),\n                                                 np.full_like(temp_surf_quant[0], r_ref[0]) * sphum_sat(temp_surf_ref[0], pressure_surf_ref),\n                                                 pressure_surf_ref, pressure_ft_ref, epsilon=epsilon_quant[0] + z_approx_ref[0]),\n                'z_approx_anom': get_temp_adiabat(np.full_like(temp_surf_quant[0], temp_surf_ref[0]),\n                                                   np.full_like(temp_surf_quant[0], r_ref[0]) * sphum_sat(temp_surf_ref[0], pressure_surf_ref),\n                                                   pressure_surf_ref, pressure_ft_ref, epsilon=epsilon_ref[0] + z_approx_quant[0])}\n\n\n    info_cont = {key: np.full(n_quant, temp_surf_ref[1]-temp_surf_ref[0]) for key in ['r_ref_change', 'epsilon_ref_change', 'z_approx_ref_change',\n                                                    'temp_ft_change', 'r_change', 'epsilon_change', 'z_approx_change',\n                                                    'temp_anom', 'r_anom', 'epsilon_anom', 'z_approx_anom']}\n    for i in range(n_quant):\n        for key in ['r_ref', 'epsilon_ref', 'z_approx_ref']:\n            # Reference quantities change with warming but nothing else, and all ref quantities in current climate\n            if temp_ft_ref_change[key] == temp_ft_ref_change['base']:\n                continue\n            info_cont[f'{key}_change'][i] = get_temp_adiabat_surf(r_ref[0] + (r_ref[1] - r_ref[0] if key=='r_ref' else 0),\n                                                                  temp_ft0['base'] + temp_ft_ref_change[key],\n                                                                  z_ft=None, pressure_surf=pressure_surf_ref, pressure_ft=pressure_ft_ref,\n                                                                  epsilon=epsilon_ref[0] + (epsilon_ref[1] - epsilon_ref[0] if key=='epsilon_ref' else 0)+\n                                                                          z_approx_ref[0] + (z_approx_ref[1] - z_approx_ref[0] if key=='z_approx_ref' else 0))  - temp_surf_ref[0]\n        # temp_ft changes with warming, but all ref quantities in current climate\n        info_cont['temp_ft_change'][i] = get_temp_adiabat_surf(r_ref[0], temp_ft0['base'] + temp_ft_quant[1, i]-temp_ft_quant[0, i], z_ft=None,\n                                                               pressure_surf=pressure_surf_ref, pressure_ft=pressure_ft_ref,\n                                                               epsilon=epsilon_ref[0] + z_approx_ref[0]) - temp_surf_ref[0]\n        # RH changes with warming, but all ref quantities in current climate\n        info_cont['r_change'][i] = get_temp_adiabat_surf(r_ref[0] + r_quant[1, i] - r_quant[0, i], temp_ft0['base'] + temp_ft_ref_change['base'],\n                                                         z_ft=None, pressure_surf=pressure_surf_ref, pressure_ft=pressure_ft_ref,\n                                                         epsilon=epsilon_ref[0] + z_approx_ref[0]) - temp_surf_ref[0]\n        # epsilon changes with warming, but all ref quantities in current climate\n        info_cont['epsilon_change'][i] = get_temp_adiabat_surf(r_ref[0], temp_ft0['base'] + temp_ft_ref_change['base'],\n                                                               z_ft=None, pressure_surf=pressure_surf_ref, pressure_ft=pressure_ft_ref,\n                                                               epsilon=epsilon_ref[0] + epsilon_quant[1,i] - epsilon_quant[0,i]\n                                                                       + z_approx_ref[0]) - temp_surf_ref[0]\n        # z_approx changes with warming, but all ref quantities in current climate\n        info_cont['z_approx_change'][i] = get_temp_adiabat_surf(r_ref[0], temp_ft0['base'] + temp_ft_ref_change['base'],\n                                                               z_ft=None, pressure_surf=pressure_surf_ref, pressure_ft=pressure_ft_ref,\n                                                               epsilon=epsilon_ref[0] + z_approx_ref[0] + z_approx_quant[1,i] - z_approx_quant[0, i]) - temp_surf_ref[0]\n        # Only temp_ft changes with warming according to ref_change, all ref quantities in current climate except temp_surf\n        info_cont['temp_anom'][i] = get_temp_adiabat_surf(r_ref[0], temp_ft0['temp_anom'][i] + temp_ft_ref_change['base'],\n                                                          z_ft=None, pressure_surf=pressure_surf_ref, pressure_ft=pressure_ft_ref,\n                                                          epsilon=epsilon_ref[0] + z_approx_ref[0]) - temp_surf_quant[0, i]\n        # Only temp_ft changes with warming according to ref_change, all ref quantities in current climate except RH\n        info_cont['r_anom'][i] = get_temp_adiabat_surf(r_quant[0, i], temp_ft0['r_anom'][i] + temp_ft_ref_change['base'],\n                                                       z_ft=None, pressure_surf=pressure_surf_ref, pressure_ft=pressure_ft_ref,\n                                                       epsilon=epsilon_ref[0] + z_approx_ref[0]) - \\\n                                 temp_surf_ref[0]\n        # Only temp_ft changes with warming according to ref_change, all ref quantities in current climate except epsilon\n        info_cont['epsilon_anom'][i] = get_temp_adiabat_surf(r_ref[0], temp_ft0['epsilon_anom'][i] + temp_ft_ref_change['base'],\n                                                             z_ft=None, pressure_surf=pressure_surf_ref, pressure_ft=pressure_ft_ref,\n                                                             epsilon=epsilon_quant[0, i] + z_approx_ref[0]) - \\\n                                       temp_surf_ref[0]\n        # Only temp_ft changes with warming according to ref_change, all ref quantities in current climate except z_approx\n        info_cont['z_approx_anom'][i] = get_temp_adiabat_surf(r_ref[0], temp_ft0['z_approx_anom'][i] + temp_ft_ref_change['base'],\n                                                             z_ft=None, pressure_surf=pressure_surf_ref, pressure_ft=pressure_ft_ref,\n                                                             epsilon=epsilon_ref[0] + z_approx_quant[0, i]) - \\\n                                        temp_surf_ref[0]\n    for key in info_cont:\n        info_cont[key] /= (temp_surf_ref[1]-temp_surf_ref[0])       # Make it so it gives scale factor contribution\n\n    final_answer = np.asarray(sum([info_cont[key]-1 for key in info_cont])) + 1\n    return final_answer, info_cont\n</code></pre>"},{"location":"code/thesis/adiabat_theory2/#isca_tools.thesis.adiabat_theory2.get_sensitivity_factors","title":"<code>get_sensitivity_factors(temp_surf_ref, r_ref, pressure_surf, pressure_ft, epsilon_ref=None, z_approx_ref=None, cape_form=False)</code>","text":"<p>Calculates the dimensionless sensitivity \\(\\gamma\\) parameters such that the theoretical scaling factor is given by:</p> \\[ \\begin{align} \\frac{\\delta \\hat{T}_s(x)}{\\delta \\tilde{T}_s} &amp;= \\gamma_{\\delta T_{FT}}\\frac{\\delta T_{FT}[x]}{\\delta \\tilde{T}_s} - \\gamma_{\\delta r}\\frac{\\tilde{T}_s}{\\tilde{r}_s} \\frac{\\delta r_s[x]}{\\delta \\tilde{T}_s} + \\gamma_{\\delta \\epsilon} \\frac{\\delta \\epsilon[x]}{c_p \\delta \\tilde{T}_s} \\\\ &amp;+ \\gamma_{\\Delta T_s} \\frac{\\Delta T_s(x)}{\\tilde{T}_s} - \\gamma_{\\Delta r} \\frac{\\Delta r[x]}{\\tilde{r}_s} - \\gamma_{\\Delta \\epsilon} \\frac{\\Delta \\epsilon[x]}{c_p \\tilde{T}_s} - \\gamma_{\\delta \\tilde{r}}\\frac{\\delta \\tilde{r}_s}{\\tilde{r}_s} \\end{align} \\] <p>These \\(\\gamma\\) parameters quantify the significance of different physical mechanisms in causing a change in the near-surface temperature distribution.</p> Terms in equation <ul> <li>\\(h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon =     \\left(c_p + R^{\\dagger}\\right) T_{FT} + L_v q^*_{FT} + A_z\\)     where we used an approximate relation to replace \\(z_{FT}\\) in \\(h^*_{FT}\\).</li> <li>\\(\\epsilon = h_s - h^*_{FT}\\), where \\(h_s\\) is near-surface MSE (at \\(p_s\\)) and     \\(h^*_{FT}\\) is free tropospheric saturated MSE (at \\(p_{FT}\\)).</li> <li>\\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\)</li> <li>\\(\\Delta \\chi[x] = \\chi[x] - \\tilde{\\chi}\\)</li> <li>\\(\\chi[x]\\) is the value of \\(\\chi\\) averaged over all days     where near-surface temperature, \\(T_s\\), is between percentile \\(x-0.5\\) and \\(x+0.5\\).</li> <li>\\(\\tilde{\\chi}\\) is the reference value of \\(\\chi\\), which is free to be chosen.</li> <li>\\(\\beta_{FT1} = \\frac{\\partial h^{\\dagger}}{\\partial T_{FT}} = c_p + R^{\\dagger} + L_v \\alpha_{FT} q_{FT}^*\\)</li> <li>\\(\\beta_{FT2} = T_{FT} \\frac{\\partial^2h^{\\dagger}}{\\partial T_{FT}^2} =     T_{FT}\\frac{d\\beta_{FT1}}{d T_{FT}} = L_v \\alpha_{FT} q_{FT}^*(\\alpha_{FT} T_{FT} - 2)\\)</li> <li>\\(\\beta_{s1} = \\frac{\\partial h^{\\dagger}}{\\partial T_s} = c_p - R^{\\dagger} + L_v \\alpha_s q_s\\)</li> <li>\\(\\beta_{s2} = T_s \\frac{\\partial^2 h^{\\dagger}}{\\partial T_s^2} =     T_s\\frac{\\partial \\beta_{s1}}{\\partial T_s} = L_v \\alpha_s q_s(\\alpha_s T_s - 2)\\)</li> <li>\\(\\mu=\\frac{L_v \\alpha_s q_s}{\\beta_{s1}}\\)</li> <li>\\(q = rq^*\\) where \\(q\\) is the specific humidity, \\(r\\) is relative humidity and \\(q^*(T, p)\\)     is saturation specific humidity which is a function of temperature and pressure.</li> <li>\\(\\alpha(T, p)\\) is the clausius clapeyron parameter which is a function of temperature and pressure,     such that \\(\\partial q^*/\\partial T = \\alpha q^*\\).</li> </ul> <p>If <code>cape_form=True</code>, will replace both \\(\\epsilon\\) terms with a single \\(CAPE\\) change: \\(\\gamma_{\\delta T_{FT}}\\frac{\\delta CAPE[x]}{R^{\\dagger}\\delta \\overline{T}_s}\\).</p> Definition of CAPE <p>\\(CAPE = R^{\\dagger} (T_{FT,\\epsilon=0} - T_{FT})\\) where \\(T_{FT,\\epsilon=0}\\) is the free tropospheric temperature which would occur if \\(\\epsilon=0\\), all else the same. I.e. this is the parcel rather than environmental temperature at \\(p_{FT}\\).</p> <p>Parameters:</p> Name Type Description Default <code>temp_surf_ref</code> <code>Union[ndarray, float]</code> <p><code>float [n_exp]</code> \\(\\tilde{T}_s\\) Reference near surface temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K. It is assumed that <code>n_exp=2</code>. If provide just one value, will assume <code>r_ref</code> for both experiments is the same, and thus the value of <code>temp_surf_ref</code> in the second (warmer) experiment does not make a difference.</p> required <code>r_ref</code> <code>Union[ndarray, float]</code> <p><code>float [n_exp]</code> \\(\\tilde{r}_s\\) Reference near surface relative humidity of each simulation. Units: dimensionless (from 0 to 1). If provide just one value, will assume it is the same for both experiments.</p> required <code>pressure_surf</code> <code>float</code> <p>Pressure at near-surface, \\(p_s\\), in Pa.</p> required <code>pressure_ft</code> <code>float</code> <p>Pressure at free troposphere level, \\(p_{FT}\\), in Pa.</p> required <code>epsilon_ref</code> <code>Optional[Union[ndarray, float]]</code> <p><code>float [n_exp]</code> \\(\\tilde{\\epsilon}_s\\) Reference value of \\(\\epsilon = h_s - h^*_{FT}\\), where \\(h_s\\) is near-surface MSE and \\(h^*_{FT}\\) is saturated MSE at <code>pressure_ft</code>. If not given, weill set to 0. Units: kJ/kg. If provide just one value, will assume it is the same for both experiments.</p> <code>None</code> <code>z_approx_ref</code> <code>Optional[Union[ndarray, float]]</code> <p><code>float [n_exp]</code> \\(\\tilde{A}_z\\) The exact equation for modified MSE is given by: \\(h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon = (c_p + R^{\\dagger})T_{FT} + L_vq^*(T_{FT}, p_{FT}) + A_z\\) where \\(R^{\\dagger} = R\\ln(p_s/p_{FT})/2\\) and \\(A_z\\) quantifies the error due to approximation of geopotential height, as relating to temperature. Here you have the option of specifying the reference \\(A_z\\) for each simulation. If not provided, will set to 0. Units: kJ/kg. If provide just one value, will assume it is the same for both experiments.</p> <code>None</code> <code>cape_form</code> <code>bool</code> <p>If <code>True</code>, scaling factor is in \\(CAPE\\) rather than \\(\\epsilon\\) form, so a single <code>cape_change</code> value returned, instead of the <code>epsilon_change</code> and <code>epsilon_anom</code> values.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>gamma</code> <code>dict</code> <p>Dictionary containing sensitivity parameters. All are a single dimensionless <code>float</code>. Below, I give the equation for each parameter if \\(\\delta \\tilde{r}_s = \\delta \\tilde{\\epsilon} = 0\\).</p> <ul> <li><code>temp_ft_change</code>: \\(\\gamma_{\\delta T_{FT}} = \\frac{\\tilde{\\beta}_{FT1}}{\\tilde{\\beta}_{s1}}\\)</li> <li><code>r_change</code>: \\(\\gamma_{\\delta r} = \\frac{L_v\\tilde{q}_s}{\\tilde{\\beta}_{s1} \\tilde{T}_s}\\)</li> <li><code>epsilon_change</code>: \\(\\gamma_{\\delta \\epsilon} = \\frac{c_p}{\\tilde{\\beta}_{s1}}\\).     Only returned if <code>cape_form=False</code>.</li> <li><code>temp_anom</code>: \\(\\gamma_{\\Delta T_s} = \\frac{\\tilde{\\beta}_{FT2}}{\\tilde{\\beta}_{FT1}}     \\frac{\\tilde{\\beta}_{s1} \\tilde{T}_s}{\\tilde{\\beta}_{FT1}\\tilde{T}_{FT}} -     \\frac{\\tilde{\\beta}_{s2}}{\\tilde{\\beta}_{s1}}\\)</li> <li><code>r_anom</code>: \\(\\gamma_{\\Delta r} = \\tilde{\\mu} - \\frac{\\tilde{\\beta}_{FT2}}{\\tilde{\\beta}_{FT1}}     \\frac{L_v \\tilde{q}_s}{\\tilde{\\beta}_{FT1}\\tilde{T}_{FT}}\\)</li> <li><code>epsilon_anom</code>: \\(\\gamma_{\\Delta \\epsilon} = \\frac{\\tilde{\\beta}_{FT2}}{\\tilde{\\beta}_{FT1}}     \\frac{c_p \\tilde{T}_s}{\\tilde{\\beta}_{FT1}\\tilde{T}_{FT}}\\).     Only returned if <code>cape_form=False</code>.</li> <li><code>r_ref_change</code>: \\(\\gamma_{\\delta \\tilde{r}} = \\tilde{\\mu}\\)</li> <li><code>cape_change</code>: The same as \\(\\gamma_{\\delta T_{FT}}\\). Only returned if <code>cape_form=True</code>.</li> </ul> Source code in <code>isca_tools/thesis/adiabat_theory2.py</code> <pre><code>def get_sensitivity_factors(temp_surf_ref: Union[np.ndarray, float], r_ref: Union[np.ndarray, float],\n                            pressure_surf: float, pressure_ft: float,\n                            epsilon_ref: Optional[Union[np.ndarray, float]] = None,\n                            z_approx_ref: Optional[Union[np.ndarray, float]] = None,\n                            cape_form: bool = False) -&gt; dict:\n    \"\"\"\n    Calculates the dimensionless sensitivity $\\gamma$ parameters such that the theoretical scaling factor is given by:\n\n    $$\n    \\\\begin{align}\n    \\\\frac{\\delta \\hat{T}_s(x)}{\\delta \\\\tilde{T}_s} &amp;= \\gamma_{\\delta T_{FT}}\\\\frac{\\delta T_{FT}[x]}{\\delta \\\\tilde{T}_s}\n    - \\gamma_{\\delta r}\\\\frac{\\\\tilde{T}_s}{\\\\tilde{r}_s} \\\\frac{\\delta r_s[x]}{\\delta \\\\tilde{T}_s}\n    + \\gamma_{\\delta \\epsilon} \\\\frac{\\delta \\epsilon[x]}{c_p \\delta \\\\tilde{T}_s} \\\\\\\\\n    &amp;+ \\gamma_{\\Delta T_s} \\\\frac{\\Delta T_s(x)}{\\\\tilde{T}_s}\n    - \\gamma_{\\Delta r} \\\\frac{\\Delta r[x]}{\\\\tilde{r}_s}\n    - \\gamma_{\\Delta \\epsilon} \\\\frac{\\Delta \\epsilon[x]}{c_p \\\\tilde{T}_s}\n    - \\gamma_{\\delta \\\\tilde{r}}\\\\frac{\\delta \\\\tilde{r}_s}{\\\\tilde{r}_s}\n    \\\\end{align}\n    $$\n\n    These $\\gamma$ parameters quantify the significance of different physical mechanisms in causing a change\n    in the near-surface temperature distribution.\n\n    ??? note \"Terms in equation\"\n        * $h^{\\dagger} = h^*_{FT} - R^{\\dagger}T_s - gz_s = (c_p - R^{\\dagger})T_s + L_v q_s - \\epsilon =\n            \\\\left(c_p + R^{\\dagger}\\\\right) T_{FT} + L_v q^*_{FT} + A_z$\n            where we used an approximate relation to replace $z_{FT}$ in $h^*_{FT}$.\n        * $\\epsilon = h_s - h^*_{FT}$, where $h_s$ is near-surface MSE (at $p_s$) and\n            $h^*_{FT}$ is free tropospheric saturated MSE (at $p_{FT}$).\n        * $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$\n        * $\\\\Delta \\chi[x] = \\chi[x] - \\\\tilde{\\chi}$\n        * $\\chi[x]$ is the value of $\\chi$ averaged over all days\n            where near-surface temperature, $T_s$, is between percentile $x-0.5$ and $x+0.5$.\n        * $\\\\tilde{\\chi}$ is the reference value of $\\chi$, which is free to be chosen.\n        * $\\\\beta_{FT1} = \\\\frac{\\partial h^{\\\\dagger}}{\\partial T_{FT}} = c_p + R^{\\dagger} + L_v \\\\alpha_{FT} q_{FT}^*$\n        * $\\\\beta_{FT2} = T_{FT} \\\\frac{\\partial^2h^{\\\\dagger}}{\\partial T_{FT}^2} =\n            T_{FT}\\\\frac{d\\\\beta_{FT1}}{d T_{FT}} = L_v \\\\alpha_{FT} q_{FT}^*(\\\\alpha_{FT} T_{FT} - 2)$\n        * $\\\\beta_{s1} = \\\\frac{\\partial h^{\\dagger}}{\\partial T_s} = c_p - R^{\\dagger} + L_v \\\\alpha_s q_s$\n        * $\\\\beta_{s2} = T_s \\\\frac{\\partial^2 h^{\\dagger}}{\\partial T_s^2} =\n            T_s\\\\frac{\\partial \\\\beta_{s1}}{\\partial T_s} = L_v \\\\alpha_s q_s(\\\\alpha_s T_s - 2)$\n        * $\\mu=\\\\frac{L_v \\\\alpha_s q_s}{\\\\beta_{s1}}$\n        * $q = rq^*$ where $q$ is the specific humidity, $r$ is relative humidity and $q^*(T, p)$\n            is saturation specific humidity which is a function of temperature and pressure.\n        * $\\\\alpha(T, p)$ is the clausius clapeyron parameter which is a function of temperature and pressure,\n            such that $\\partial q^*/\\partial T = \\\\alpha q^*$.\n\n    If `cape_form=True`, will replace both $\\epsilon$ terms with a single $CAPE$ change:\n    $\\gamma_{\\delta T_{FT}}\\\\frac{\\delta CAPE[x]}{R^{\\dagger}\\delta \\overline{T}_s}$.\n\n    ??? note \"Definition of CAPE\"\n        $CAPE = R^{\\dagger} (T_{FT,\\epsilon=0} - T_{FT})$ where $T_{FT,\\epsilon=0}$ is the free tropospheric\n        temperature which would occur if $\\epsilon=0$, all else the same. I.e. this is the parcel rather\n        than environmental temperature at $p_{FT}$.\n\n    Args:\n        temp_surf_ref: `float [n_exp]` $\\\\tilde{T}_s$&lt;/br&gt;\n            Reference near surface temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*. It is assumed that `n_exp=2`.&lt;/br&gt;\n            If provide just one value, will assume `r_ref` for both experiments is the same, and thus\n            the value of `temp_surf_ref` in the second (warmer) experiment does not make a difference.\n        r_ref: `float [n_exp]` $\\\\tilde{r}_s$&lt;/br&gt;\n            Reference near surface relative humidity of each simulation. Units: dimensionless (from 0 to 1).&lt;/br&gt;\n            If provide just one value, will assume it is the same for both experiments.\n        pressure_surf:\n            Pressure at near-surface, $p_s$, in *Pa*.\n        pressure_ft:\n            Pressure at free troposphere level, $p_{FT}$, in *Pa*.\n        epsilon_ref: `float [n_exp]` $\\\\tilde{\\epsilon}_s$&lt;/br&gt;\n            Reference value of $\\epsilon = h_s - h^*_{FT}$, where $h_s$ is near-surface MSE and\n            $h^*_{FT}$ is saturated MSE at `pressure_ft`. If not given, weill set to 0. Units: *kJ/kg*.&lt;/br&gt;\n            If provide just one value, will assume it is the same for both experiments.\n        z_approx_ref: `float [n_exp]` $\\\\tilde{A}_z$&lt;/br&gt;\n            The exact equation for modified MSE is given by: $h^{\\dagger} = (c_p - R^{\\dagger})T_s + L_v q_s\n            - \\epsilon = (c_p + R^{\\dagger})T_{FT} + L_vq^*(T_{FT}, p_{FT}) + A_z$\n            where $R^{\\dagger} = R\\\\ln(p_s/p_{FT})/2$ and $A_z$ quantifies the error due to\n            approximation of geopotential height, as relating to temperature.&lt;/br&gt;\n            Here you have the option of specifying the reference $A_z$ for each simulation. If not provided,\n            will set to 0. Units: *kJ/kg*.&lt;/br&gt;\n            If provide just one value, will assume it is the same for both experiments.\n        cape_form: If `True`, scaling factor is in $CAPE$ rather than $\\epsilon$ form, so a single `cape_change`\n            value returned, instead of the `epsilon_change` and `epsilon_anom` values.\n\n    Returns:\n        gamma: Dictionary containing sensitivity parameters. All are a single dimensionless `float`.\n            Below, I give the equation for each parameter if\n            $\\\\delta \\\\tilde{r}_s = \\delta \\\\tilde{\\epsilon} = 0$.\n\n            * `temp_ft_change`: $\\gamma_{\\delta T_{FT}} = \\\\frac{\\\\tilde{\\\\beta}_{FT1}}{\\\\tilde{\\\\beta}_{s1}}$\n            * `r_change`: $\\gamma_{\\delta r} = \\\\frac{L_v\\\\tilde{q}_s}{\\\\tilde{\\\\beta}_{s1} \\\\tilde{T}_s}$\n            * `epsilon_change`: $\\gamma_{\\delta \\epsilon} = \\\\frac{c_p}{\\\\tilde{\\\\beta}_{s1}}$.\n                Only returned if `cape_form=False`.\n            * `temp_anom`: $\\gamma_{\\Delta T_s} = \\\\frac{\\\\tilde{\\\\beta}_{FT2}}{\\\\tilde{\\\\beta}_{FT1}}\n                \\\\frac{\\\\tilde{\\\\beta}_{s1} \\\\tilde{T}_s}{\\\\tilde{\\\\beta}_{FT1}\\\\tilde{T}_{FT}} -\n                \\\\frac{\\\\tilde{\\\\beta}_{s2}}{\\\\tilde{\\\\beta}_{s1}}$\n            * `r_anom`: $\\gamma_{\\Delta r} = \\\\tilde{\\mu} - \\\\frac{\\\\tilde{\\\\beta}_{FT2}}{\\\\tilde{\\\\beta}_{FT1}}\n                \\\\frac{L_v \\\\tilde{q}_s}{\\\\tilde{\\\\beta}_{FT1}\\\\tilde{T}_{FT}}$\n            * `epsilon_anom`: $\\gamma_{\\Delta \\epsilon} = \\\\frac{\\\\tilde{\\\\beta}_{FT2}}{\\\\tilde{\\\\beta}_{FT1}}\n                \\\\frac{c_p \\\\tilde{T}_s}{\\\\tilde{\\\\beta}_{FT1}\\\\tilde{T}_{FT}}$.\n                Only returned if `cape_form=False`.\n            * `r_ref_change`: $\\gamma_{\\delta \\\\tilde{r}} = \\\\tilde{\\mu}$\n            * `cape_change`: The same as $\\gamma_{\\delta T_{FT}}$. Only returned if `cape_form=True`.\n    \"\"\"\n    if isinstance(temp_surf_ref, (float, int)) and isinstance(r_ref, (float, int)):\n        # If give numbers, then set r_ref change to be zero, and\n        # Cannot set temp_surf_ref_change to 0, as divide by zero in gamma['epsilon_anom'] equation\n        temp_surf_ref_change = 1        # arbitrarily have temp_diff=1K, could be anything, as does not contribute if r_ref_change=0\n        temp_surf_ref = np.asarray([temp_surf_ref, temp_surf_ref+temp_surf_ref_change])\n        r_ref_change = 0\n        r_ref = np.asarray([r_ref, r_ref+r_ref_change])\n        if isinstance(epsilon_ref, (list, np.ndarray)):\n            # If epsilon_ref_change non-zero, then must have non-zero temp_surf_ref_change, and its value does matter.\n            if epsilon_ref[1] != epsilon_ref[0]:\n                raise ValueError('Cannot have epsilon_ref different for each experiment if only one temp_surf_ref provided')\n    elif not isinstance(temp_surf_ref, (list, np.ndarray)) and isinstance(r_ref, (list, np.ndarray)):\n        raise ValueError('`temp_surf_ref` and `r_ref` must be of same type: either both float or both number')\n    n_exp = temp_surf_ref.size\n\n    if z_approx_ref is None:\n        z_approx_ref = np.zeros(n_exp)\n    elif isinstance(z_approx_ref, (float, int)):\n        # If give float, set same for both experiments (doesn't actually influence anything as temp_ft_ref[1] not used)\n        z_approx_ref = np.full(n_exp, z_approx_ref)\n\n    if epsilon_ref is None:\n        epsilon_ref = np.zeros(n_exp)\n    elif isinstance(epsilon_ref, (float, int)):\n        # If give float, set same for both experiments (use epsilon_ref_change in mse_mod_ref_change0).\n        epsilon_ref = np.full(n_exp, epsilon_ref)\n\n    sphum_ref = r_ref * sphum_sat(temp_surf_ref, pressure_surf)\n    temp_ft_ref = np.zeros(n_exp)\n    for i in range(n_exp):\n        temp_ft_ref[i] = get_temp_adiabat(temp_surf_ref[i], sphum_ref[i], pressure_surf, pressure_ft,\n                                          epsilon=epsilon_ref[i] + z_approx_ref[i])\n\n    # Get parameters required for prefactors in the theory\n    _, _, _, beta_ft1, beta_ft2, _, _ = get_theory_prefactor_terms(temp_ft_ref, pressure_surf, pressure_ft)\n    _, _, _, beta_s1, beta_s2, _, mu = get_theory_prefactor_terms(temp_surf_ref, pressure_surf, pressure_ft,\n                                                                  sphum_ref)\n    # Change in mse_mod, taking linear taylor expansion\n    temp_surf_ref_change = np.diff(temp_surf_ref, axis=0).squeeze()\n    r_ref_change = np.diff(r_ref, axis=0).squeeze()\n    mse_mod_ref_change0 = beta_s1[0] * (1 + mu[0] * (r_ref_change/r_ref[0])\n                                        ) * temp_surf_ref_change \\\n                          + L_v * sphum_ref[0] * (r_ref_change/r_ref[0]) \\\n                          - np.diff(epsilon_ref*1000, axis=0).squeeze()\n\n\n    gamma = {}\n    gamma['temp_ft_change'] = beta_ft1[0]/beta_s1[0]\n    gamma['r_change'] = L_v * sphum_ref[0]/(beta_s1[0] * temp_surf_ref[0])\n    gamma['epsilon_change'] = c_p / beta_s1[0]\n    # gamma['epsilon_anom'] just becomes (beta_ft2/beta_ft1) ((c_p*T_s) / (beta_ft1*T_ft))\n    # if r_ref_change = epsilon_ref_change = 0\n    gamma['epsilon_anom'] = beta_ft2[0] / beta_ft1[0] ** 2 / temp_ft_ref[0] \\\n                            * temp_surf_ref[0] * mse_mod_ref_change0 / temp_surf_ref_change\n    gamma['temp_anom'] = gamma['epsilon_anom'] - beta_s2[0]/beta_s1[0] * (1 + r_ref_change/r_ref[0]) \\\n                         - mu[0] * temp_surf_ref[0]/r_ref[0] * r_ref_change/temp_surf_ref_change\n    gamma['r_anom'] = mu[0] * (1 + r_ref_change/r_ref[0]) \\\n                      - gamma['epsilon_anom'] * L_v * sphum_ref[0]/(beta_s1[0] * temp_surf_ref[0])\n    gamma['r_ref_change'] = mu[0]\n    if cape_form:\n        # delete epsilon contributions, and replace with single cape change\n        del gamma['epsilon_change'], gamma['epsilon_anom']\n        gamma['cape_change'] = gamma['temp_ft_change'] * 1\n    else:\n        # account for new non-dimensional form of gamma so divide epsilon by c_p not beta_s1 in sf equation\n        gamma['epsilon_anom'] *= c_p / beta_s1[0]\n    return gamma\n</code></pre>"},{"location":"code/thesis/aquaplanet_theory/","title":"Aquaplanet Theory","text":""},{"location":"code/thesis/aquaplanet_theory/#isca_tools.thesis.aquaplanet_theory.get_delta_temp_quant_theory","title":"<code>get_delta_temp_quant_theory(temp_mean, sphum_mean, temp_quant, sphum_quant, pressure_surface, const_rh=False, delta_mse_ratio=None, taylor_level='linear_rh_diff')</code>","text":"<p>Computes the theoretical temperature difference between simulations of neighbouring optical depth values for each percentile, \\(\\delta T(x)\\), according to the assumption that changes in MSE are equal to the change in mean MSE, \\(\\delta h(x) = \\delta \\overline{h}\\):</p> \\[\\delta T(x) = \\gamma^T \\delta \\overline{T} + \\gamma^{\\Delta r} \\delta (\\overline{r} - r(x))\\] <p>This above equation is for the default settings, but a more accurate equation can be used with the <code>delta_mse_ratio</code> and <code>taylor_level</code> arguments.</p> <p>If data from <code>n_exp</code> optical depth values provided, <code>n_exp-1</code> theoretical temperature differences will be returned for each percentile.</p> <p>Parameters:</p> Name Type Description Default <code>temp_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average near surface temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K.</p> required <code>sphum_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average near surface specific humidity of each simulation. Units: kg/kg.</p> required <code>temp_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_quant[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface temperature of experiment <code>i</code>. Units: K. Note that <code>quant_use</code> is not provided as not needed by this function, but is likely to be <code>np.arange(1, 100)</code> - leave out <code>x=0</code> as doesn't really make sense to consider \\(0^{th}\\) percentile of a quantity.</p> required <code>sphum_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface specific humidity of experiment <code>i</code>. Units: kg/kg.</p> required <code>pressure_surface</code> <code>float</code> <p>Near surface pressure level. Units: Pa.</p> required <code>const_rh</code> <code>bool</code> <p>If <code>True</code>, will return the constant relative humidity version of the theory, i.e. \\(\\gamma^T \\delta \\overline{T}\\). Otherwise, will return the full theory.</p> <code>False</code> <code>delta_mse_ratio</code> <code>Optional[ndarray]</code> <p><code>float [n_exp-1, n_quant]</code> <code>delta_mse_ratio[i]</code> is the change in \\(x\\) percentile of MSE divided by the change in the mean MSE between experiment <code>i</code> and <code>i+1</code>: \\(\\delta h(x)/\\delta \\overline{h}\\). If not given, it is assumed to be equal to 1 for all \\(x\\).</p> <code>None</code> <code>taylor_level</code> <code>str</code> <p>This specifies the level of approximation that goes into the taylor series for \\(\\delta q(x)\\) and \\(\\delta \\overline{q}\\):</p> <ul> <li><code>squared</code>: Includes squared, \\(\\delta T^2\\), nonlinear, \\(\\delta T \\delta r\\), and linear terms.</li> <li><code>nonlinear</code>: Includes nonlinear, \\(\\delta T \\delta r\\), and linear terms.</li> <li><code>linear</code>: Includes just linear terms so \\(\\delta T(x) = \\gamma^T \\delta \\overline{T} +     \\gamma^{\\bar{r}} \\delta \\overline{r} +\\gamma^{r} \\delta r(x)\\)</li> <li><code>linear_rh_diff</code>: Same as <code>linear</code>, but does another approximation to combine relative humidity     contributions so: \\(\\delta T(x) = \\gamma^T \\delta \\overline{T} +     \\gamma^{\\Delta r} \\delta (\\overline{r} - r(x))\\)</li> </ul> <code>'linear_rh_diff'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_exp-1, n_quant]</code>. <code>delta_temp_quant_theory[i, j]</code> refers to the theoretical temperature difference between experiment <code>i</code> and <code>i+1</code> for percentile <code>quant_use[j]</code>.</p> Source code in <code>isca_tools/thesis/aquaplanet_theory.py</code> <pre><code>def get_delta_temp_quant_theory(temp_mean: np.ndarray, sphum_mean: np.ndarray, temp_quant: np.ndarray,\n                                sphum_quant: np.ndarray, pressure_surface: float, const_rh: bool = False,\n                                delta_mse_ratio: Optional[np.ndarray] = None,\n                                taylor_level: str = 'linear_rh_diff') -&gt; np.ndarray:\n    \"\"\"\n    Computes the theoretical temperature difference between simulations of neighbouring optical depth values for each\n    percentile, $\\delta T(x)$, according to the assumption that changes in MSE are equal to the change in mean MSE,\n    $\\delta h(x) = \\delta \\overline{h}$:\n\n    $$\\delta T(x) = \\gamma^T \\delta \\overline{T} + \\gamma^{\\Delta r} \\delta (\\overline{r} - r(x))$$\n\n    This above equation is for the default settings, but a more accurate equation can be used with the `delta_mse_ratio`\n    and `taylor_level` arguments.\n\n    If data from `n_exp` optical depth values provided, `n_exp-1` theoretical temperature differences will be returned\n    for each percentile.\n\n    Args:\n        temp_mean: `float [n_exp]`&lt;/br&gt;\n            Average near surface temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*.\n        sphum_mean: `float [n_exp]`&lt;/br&gt;\n            Average near surface specific humidity of each simulation. Units: *kg/kg*.\n        temp_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_quant[i, j]` is the percentile `quant_use[j]` of near surface temperature of\n            experiment `i`. Units: *K*.&lt;/br&gt;\n            Note that `quant_use` is not provided as not needed by this function, but is likely to be\n            `np.arange(1, 100)` - leave out `x=0` as doesn't really make sense to consider $0^{th}$ percentile\n            of a quantity.\n        sphum_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant[i, j]` is the percentile `quant_use[j]` of near surface specific humidity of\n            experiment `i`. Units: *kg/kg*.\n        pressure_surface: Near surface pressure level. Units: *Pa*.\n        const_rh: If `True`, will return the constant relative humidity version of the theory, i.e.\n            $\\gamma^T \\delta \\overline{T}$. Otherwise, will return the full theory.\n        delta_mse_ratio: `float [n_exp-1, n_quant]`&lt;/br&gt;\n            `delta_mse_ratio[i]` is the change in $x$ percentile of MSE divided by the change in the mean MSE\n            between experiment `i` and `i+1`: $\\delta h(x)/\\delta \\overline{h}$.\n            If not given, it is assumed to be equal to 1 for all $x$.\n        taylor_level: This specifies the level of approximation that goes into the taylor series for $\\delta q(x)$\n            and $\\delta \\overline{q}$:\n\n            - `squared`: Includes squared, $\\delta T^2$, nonlinear, $\\delta T \\delta r$, and linear terms.\n            - `nonlinear`: Includes nonlinear, $\\delta T \\delta r$, and linear terms.\n            - `linear`: Includes just linear terms so $\\delta T(x) = \\\\gamma^T \\delta \\\\overline{T} +\n                \\\\gamma^{\\\\bar{r}} \\delta \\\\overline{r} +\\\\gamma^{r} \\\\delta r(x)$\n            - `linear_rh_diff`: Same as `linear`, but does another approximation to combine relative humidity\n                contributions so: $\\delta T(x) = \\gamma^T \\delta \\overline{T} +\n                \\gamma^{\\Delta r} \\delta (\\overline{r} - r(x))$\n\n    Returns:\n        `float [n_exp-1, n_quant]`.&lt;/br&gt;\n            `delta_temp_quant_theory[i, j]` refers to the theoretical temperature difference between experiment `i` and\n            `i+1` for percentile `quant_use[j]`.\n    \"\"\"\n    n_exp, n_quant = temp_quant.shape\n    alpha_quant = clausius_clapeyron_factor(temp_quant, pressure_surface)\n    alpha_mean = clausius_clapeyron_factor(temp_mean, pressure_surface)\n    sphum_quant_sat = sphum_sat(temp_quant, pressure_surface)\n    sphum_mean_sat = sphum_sat(temp_mean, pressure_surface)\n    r_quant = sphum_quant / sphum_quant_sat\n    r_mean = sphum_mean / sphum_mean_sat\n    delta_temp_mean = np.diff(temp_mean)\n\n    delta_r_mean = np.diff(r_mean)\n    delta_r_quant = np.diff(r_quant, axis=0)\n    if const_rh:\n        # get rid of relative humidity contribution if constant rh\n        delta_r_mean = 0 * delta_r_mean\n        delta_r_quant = 0 * delta_r_quant\n\n    # Pad all delta variables so same size as temp_quant - will not use this in calculation but just makes it easier\n    pad_array = ((0, 1), (0, 0))\n    if delta_mse_ratio is None:\n        delta_mse_ratio = np.ones_like(temp_quant)\n    else:\n        # make delta_mse_ratio the same size as all other quant variables\n        delta_mse_ratio = np.pad(delta_mse_ratio, pad_width=pad_array)\n    delta_temp_mean = np.pad(delta_temp_mean, pad_width=pad_array[0])\n    delta_r_mean = np.pad(delta_r_mean, pad_width=pad_array[0])\n    delta_r_quant = np.pad(delta_r_quant, pad_width=pad_array)\n\n    if taylor_level == 'squared':\n        # Keep squared, linear and non-linear terms in taylor expansion of delta_sphum_quant\n        coef_a = 0.5 * L_v * alpha_quant * sphum_quant * (alpha_quant - 2 / temp_quant[0])\n        coef_b = c_p + L_v * alpha_quant * (sphum_quant + sphum_quant_sat * delta_r_quant)\n        coef_c = L_v * sphum_quant_sat * delta_r_quant - delta_mse_ratio * np.expand_dims(\n                0.5 * L_v * alpha_mean * sphum_mean * (alpha_mean - 2 / temp_mean) * delta_temp_mean ** 2 +\n                (c_p + L_v * alpha_mean * (sphum_mean + sphum_mean_sat * delta_r_mean)) * delta_temp_mean +\n                L_v * sphum_mean_sat * delta_r_mean, axis=-1)\n        delta_temp_quant_theory = np.asarray([[np.roots([coef_a[i, j], coef_b[i, j], coef_c[i, j]])[1]\n                                               for j in range(n_quant)] for i in range(n_exp - 1)])\n    elif taylor_level in ['nonlinear', 'non-linear']:\n        # Keep linear and non-linear terms in taylor expansion of delta_sphum_quant\n        coef_b = c_p + L_v * alpha_quant * (sphum_quant + sphum_quant_sat * delta_r_quant)\n        coef_c = L_v * sphum_quant_sat * delta_r_quant - delta_mse_ratio * np.expand_dims(\n                (c_p + L_v * alpha_mean * (sphum_mean + sphum_mean_sat * delta_r_mean)) * delta_temp_mean +\n                L_v * sphum_mean_sat * delta_r_mean, axis=-1)\n        delta_temp_quant_theory = -coef_c[:-1] / coef_b[:-1]\n    elif taylor_level == 'linear':\n        # Only keep linear terms in taylor expansion of delta_sphum_quant\n        coef_b = c_p + L_v * alpha_quant * sphum_quant\n        coef_c = L_v * sphum_quant_sat * delta_r_quant - delta_mse_ratio * np.expand_dims(\n                (c_p + L_v * alpha_mean * sphum_mean) * delta_temp_mean + L_v * sphum_mean_sat * delta_r_mean, axis=-1)\n        delta_temp_quant_theory = -coef_c[:-1] / coef_b[:-1]\n    elif taylor_level == 'linear_rh_diff':\n        # combine mean and quantile RH changes with same prefactor\n        # This is a further taylor expansion of sphum_quant_sat around sphum_quant_mean\n        gamma_t, gamma_rdiff = get_gamma(temp_mean, sphum_mean, temp_quant, sphum_quant, pressure_surface)\n        delta_temp_quant_theory = (gamma_t * delta_mse_ratio * np.expand_dims(delta_temp_mean, axis=-1) +\n                                   gamma_rdiff * (delta_mse_ratio * np.expand_dims(delta_r_mean, axis=-1) -\n                                                  delta_r_quant))[:-1]\n    else:\n        raise ValueError(f\"taylor_level given is {taylor_level}. This is not valid, it must be either: 'squared', \"\n                         f\"'nonlinear', 'linear' or 'linear_rh_diff'\")\n    return delta_temp_quant_theory\n</code></pre>"},{"location":"code/thesis/aquaplanet_theory/#isca_tools.thesis.aquaplanet_theory.get_gamma","title":"<code>get_gamma(temp_mean, sphum_mean, temp_quant, sphum_quant, pressure_surface)</code>","text":"<p>This function returns the sensitivity parameters in the theory. One for changes in mean temperature, \\(\\delta \\overline{T}\\), and  one for difference to the mean relative humidity, \\(\\delta (\\overline{r} - r(x))\\):</p> \\[\\gamma^T = \\frac{c_p + L_v \\bar{\\alpha} \\bar{q}}{c_p + L_v \\alpha q};\\quad \\gamma^{\\Delta r} = \\frac{L_v \\overline{q_{sat}}}{c_p + L_v \\alpha q}\\] <p>Parameters:</p> Name Type Description Default <code>temp_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average near surface temperature of each simulation, corresponding to a different optical depth, \\(\\kappa\\). Units: K.</p> required <code>sphum_mean</code> <code>ndarray</code> <p><code>float [n_exp]</code> Average near surface specific humidity of each simulation. Units: kg/kg.</p> required <code>temp_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_quant[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface temperature of experiment <code>i</code>. Units: K. Note that <code>quant_use</code> is not provided as not needed by this function, but is likely to be <code>np.arange(1, 100)</code> - leave out <code>x=0</code> as doesn't really make sense to consider \\(0^{th}\\) percentile of a quantity.</p> required <code>sphum_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface specific humidity of experiment <code>i</code>. Units: kg/kg.</p> required <code>pressure_surface</code> <code>float</code> <p>Near surface pressure level. Units: Pa.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p><code>gamma_t</code>: <code>float [n_exp, n_quant]</code> The sensitivity to change in mean temperature for each experiment and quantile.</p> <code>ndarray</code> <p><code>gamma_rdiff</code>: <code>float [n_exp, n_quant]</code> The sensitivity to change in relative humidity difference from the mean for each experiment and quantile.</p> Source code in <code>isca_tools/thesis/aquaplanet_theory.py</code> <pre><code>def get_gamma(temp_mean: np.ndarray, sphum_mean: np.ndarray, temp_quant: np.ndarray,\n              sphum_quant: np.ndarray, pressure_surface: float) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    This function returns the sensitivity parameters in the theory.\n    One for changes in mean temperature, $\\\\delta \\\\overline{T}$, and  one for difference to the mean relative humidity,\n    $\\delta (\\overline{r} - r(x))$:\n\n    $$\\gamma^T = \\\\frac{c_p + L_v \\\\bar{\\\\alpha} \\\\bar{q}}{c_p + L_v \\\\alpha q};\\quad\n    \\gamma^{\\Delta r} = \\\\frac{L_v \\\\overline{q_{sat}}}{c_p + L_v \\\\alpha q}$$\n\n    Args:\n        temp_mean: `float [n_exp]`&lt;/br&gt;\n            Average near surface temperature of each simulation, corresponding to a different\n            optical depth, $\\kappa$. Units: *K*.\n        sphum_mean: `float [n_exp]`&lt;/br&gt;\n            Average near surface specific humidity of each simulation. Units: *kg/kg*.\n        temp_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_quant[i, j]` is the percentile `quant_use[j]` of near surface temperature of\n            experiment `i`. Units: *K*.&lt;/br&gt;\n            Note that `quant_use` is not provided as not needed by this function, but is likely to be\n            `np.arange(1, 100)` - leave out `x=0` as doesn't really make sense to consider $0^{th}$ percentile\n            of a quantity.\n        sphum_quant: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant[i, j]` is the percentile `quant_use[j]` of near surface specific humidity of\n            experiment `i`. Units: *kg/kg*.\n        pressure_surface: Near surface pressure level. Units: *Pa*.\n\n    Returns:\n        `gamma_t`: `float [n_exp, n_quant]`&lt;/br&gt;\n            The sensitivity to change in mean temperature for each experiment and quantile.\n        `gamma_rdiff`: `float [n_exp, n_quant]`&lt;/br&gt;\n            The sensitivity to change in relative humidity difference from the mean for each experiment and quantile.\n    \"\"\"\n    alpha_quant = clausius_clapeyron_factor(temp_quant, pressure_surface)\n    alpha_mean = clausius_clapeyron_factor(temp_mean, pressure_surface)\n    sphum_mean_sat = sphum_sat(temp_mean, pressure_surface)\n    denom = c_p + L_v * alpha_quant * sphum_quant\n    gamma_t = np.expand_dims(c_p + L_v * alpha_mean * sphum_mean, axis=-1) / denom\n    gamma_rdiff = L_v / denom * np.expand_dims(sphum_mean_sat, axis=-1)\n    return gamma_t, gamma_rdiff\n</code></pre>"},{"location":"code/thesis/aquaplanet_theory/#isca_tools.thesis.aquaplanet_theory.get_lambda_2_theory","title":"<code>get_lambda_2_theory(temp_ft_quant, temp_ft_mean, z_quant, z_mean, pressure_ft)</code>","text":"<p>Compute the approximation for \\(\\lambda_2 = \\delta h^*_{FT}(x)/\\delta \\overline{h^*_{FT}}\\) used for the extratropical part of the theory between two simulations (<code>n_exp</code> should be 2):</p> \\[\\lambda_2 \\approx (1+\\frac{\\overline{T}\\delta \\overline{\\kappa}}{\\delta \\overline{z}}) \\frac{c_p + L_v\\alpha(x) q^*(x)}{c_p + L_v\\overline{\\alpha} \\overline{q^*}} \\frac{\\delta z(x)}{\\delta \\overline{z}} - \\frac{c_p + L_v\\alpha(x) q^*(x)}{c_p + L_v\\overline{\\alpha} \\overline{q^*}} \\frac{\\overline{T} \\delta \\kappa(x)}{\\delta \\overline{z}}\\] <p>where \\(\\kappa=z/T\\) and \\(z\\) and \\(T\\) are the free-troposphere geopotential height and temperature.</p> <p>Parameters:</p> Name Type Description Default <code>temp_ft_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_lat, n_quant]</code> Free troposphere temperature for each experiment, latitude and quantile.</p> required <code>temp_ft_mean</code> <code>ndarray</code> <p><code>float [n_exp, n_lat]</code> Mean free troposphere temperature for each experiment and latitude.</p> required <code>z_quant</code> <code>ndarray</code> <p><code>float [n_exp, n_lat, n_quant]</code> Free troposphere geopotential height for each experiment, latitude and quantile.</p> required <code>z_mean</code> <code>ndarray</code> <p><code>float [n_exp, n_lat]</code> Mean free troposphere geopotential height for each experiment and latitude.</p> required <code>pressure_ft</code> <code>float</code> <p>Free troposphere pressure level. Units: Pa.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>\\(\\lambda_2\\) Approximation: <code>float [n_lat, n_quant]</code> Approximation of \\(\\lambda_2\\) at each latitude and quantile.</p> <code>dict</code> <p><code>prefactors</code>: Dictionary containing the prefactors that go into the approximation. All variables are evaluated at the colder simulation.</p> <ul> <li><code>z1</code>: <code>float [n_exp, n_lat, 1]</code> \\(1+\\frac{\\overline{T}\\delta \\overline{\\kappa}}{\\delta \\overline{z}}\\)</li> <li><code>z2</code>: <code>float [n_exp, n_lat, n_quant]</code> \\(\\frac{c_p + L_v\\alpha(x) q^*(x)}{c_p + L_v\\overline{\\alpha} \\overline{q^*}}\\)</li> <li><code>kappa</code>: <code>float [n_exp, n_lat, n_quant]</code> \\(-\\frac{c_p + L_v\\alpha(x) q^*(x)}{c_p + L_v\\overline{\\alpha} \\overline{q^*}} \\times \\overline{T}\\)</li> </ul> <code>dict</code> <p><code>delta_var</code>: Dictionary containing the following changes between simulations.</p> <ul> <li><code>z_quant</code>: <code>float [n_exp, n_lat, n_quant]</code> \\(\\delta z(x)\\)</li> <li><code>z_mean</code>: <code>float [n_exp, n_lat, 1]</code> \\(\\delta \\overline{z}\\)</li> <li><code>kappa_quant</code>: <code>float [n_exp, n_lat, n_quant]</code> \\(\\delta \\kappa(x)\\)</li> <li><code>kappa_mean</code>: <code>float [n_exp, n_lat, 1]</code> \\(\\delta \\overline{\\kappa}\\)</li> </ul> Source code in <code>isca_tools/thesis/aquaplanet_theory.py</code> <pre><code>def get_lambda_2_theory(temp_ft_quant: np.ndarray, temp_ft_mean: np.ndarray, z_quant: np.ndarray, z_mean: np.ndarray,\n                        pressure_ft: float) -&gt; Tuple[np.ndarray, dict, dict]:\n    \"\"\"\n    Compute the approximation for $\\lambda_2 = \\delta h^*_{FT}(x)/\\delta \\overline{h^*_{FT}}$ used\n    for the extratropical part of the theory between two simulations (`n_exp` should be 2):\n\n    $$\\\\lambda_2 \\\\approx (1+\\\\frac{\\\\overline{T}\\\\delta \\\\overline{\\\\kappa}}{\\\\delta \\\\overline{z}})\n    \\\\frac{c_p + L_v\\\\alpha(x) q^*(x)}{c_p + L_v\\\\overline{\\\\alpha} \\\\overline{q^*}}\n    \\\\frac{\\\\delta z(x)}{\\\\delta \\\\overline{z}} -\n    \\\\frac{c_p + L_v\\\\alpha(x) q^*(x)}{c_p + L_v\\\\overline{\\\\alpha} \\\\overline{q^*}}\n    \\\\frac{\\\\overline{T} \\\\delta \\\\kappa(x)}{\\\\delta \\\\overline{z}}$$\n\n    where $\\kappa=z/T$ and $z$ and $T$ are the free-troposphere geopotential height and temperature.\n\n    Args:\n        temp_ft_quant: `float [n_exp, n_lat, n_quant]`&lt;/br&gt;\n            Free troposphere temperature for each experiment, latitude and quantile.\n        temp_ft_mean: `float [n_exp, n_lat]`&lt;/br&gt;\n            Mean free troposphere temperature for each experiment and latitude.\n        z_quant: `float [n_exp, n_lat, n_quant]`&lt;/br&gt;\n            Free troposphere geopotential height for each experiment, latitude and quantile.\n        z_mean: `float [n_exp, n_lat]`&lt;/br&gt;\n            Mean free troposphere geopotential height for each experiment and latitude.\n        pressure_ft: Free troposphere pressure level. Units: *Pa*.\n\n    Returns:\n        $\\lambda_2$ Approximation: `float [n_lat, n_quant]`&lt;/br&gt;\n            Approximation of $\\lambda_2$ at each latitude and quantile.\n        `prefactors`: Dictionary containing the prefactors that go into the approximation. All variables are\n            evaluated at the colder simulation.\n\n            - `z1`: `float [n_exp, n_lat, 1]`&lt;/br&gt;\n                $1+\\\\frac{\\\\overline{T}\\\\delta \\\\overline{\\\\kappa}}{\\\\delta \\\\overline{z}}$\n            - `z2`: `float [n_exp, n_lat, n_quant]`&lt;/br&gt;\n                $\\\\frac{c_p + L_v\\\\alpha(x) q^*(x)}{c_p + L_v\\\\overline{\\\\alpha} \\\\overline{q^*}}$\n            - `kappa`: `float [n_exp, n_lat, n_quant]`&lt;/br&gt;\n                $-\\\\frac{c_p + L_v\\\\alpha(x) q^*(x)}{c_p + L_v\\\\overline{\\\\alpha} \\\\overline{q^*}} \\\\times \\\\overline{T}$\n        `delta_var`: Dictionary containing the following changes between simulations.\n\n            - `z_quant`: `float [n_exp, n_lat, n_quant]`&lt;/br&gt;\n                    $\\delta z(x)$&lt;/br&gt;\n            - `z_mean`: `float [n_exp, n_lat, 1]`&lt;/br&gt;\n                    $\\delta \\overline{z}$&lt;/br&gt;\n            - `kappa_quant`: `float [n_exp, n_lat, n_quant]`&lt;/br&gt;\n                    $\\delta \\kappa(x)$&lt;/br&gt;\n            - `kappa_mean`: `float [n_exp, n_lat, 1]`&lt;/br&gt;\n                    $\\delta \\overline{\\kappa}$\n    \"\"\"\n    kappa_quant = z_quant / temp_ft_quant\n    kappa_mean = z_mean / temp_ft_mean\n    # Need to expand dims of mean variables so can multiply quant arrays\n    temp_ft_mean = np.expand_dims(temp_ft_mean, axis=-1)\n\n    delta_var = {'z_quant': z_quant[1] - z_quant[0], 'z_mean': np.expand_dims(z_mean[1] - z_mean[0], axis=-1),\n                 'kappa_quant': kappa_quant[1] - kappa_quant[0],\n                 'kappa_mean': np.expand_dims(kappa_mean[1] - kappa_mean[0], axis=-1)}\n\n    alpha_quant = clausius_clapeyron_factor(temp_ft_quant[0], pressure_ft)\n    alpha_mean = clausius_clapeyron_factor(temp_ft_mean[0], pressure_ft)\n    q_quant = sphum_sat(temp_ft_quant[0], pressure_ft)\n    q_mean = sphum_sat(temp_ft_mean[0], pressure_ft)\n\n    prefactors = {'z1': 1+temp_ft_mean[0] * delta_var['kappa_mean'] / delta_var['z_mean'],\n                  'z2': (c_p + L_v * alpha_quant * q_quant) / (c_p + L_v * alpha_mean * q_mean)}\n    prefactors['kappa'] = -prefactors['z2'] * temp_ft_mean[0]\n\n    lambda_2_approx = prefactors['z1'] * prefactors['z2'] * delta_var['z_quant'] / delta_var['z_mean'] + \\\n        prefactors['kappa'] * delta_var['kappa_quant'] / delta_var['z_mean']\n    return lambda_2_approx, prefactors, delta_var\n</code></pre>"},{"location":"code/thesis/extrop_land_theory/","title":"Extratropics Land Theory","text":""},{"location":"code/thesis/extrop_land_theory/#isca_tools.thesis.extrop_land_theory.get_delta_temp_quant_theory","title":"<code>get_delta_temp_quant_theory(temp_quant_ocean, sphum_quant_ocean, temp_quant_land, sphum_quant_land, pressure_surface, const_rh=False, delta_mse_ratio=None, taylor_level='linear_rh_diff')</code>","text":"<p>Computes the theoretical temperature difference between simulations of neighbouring optical depth values for each percentile, \\(\\delta T(x)\\), according to the assumption that changes in MSE are equal to the change in mean MSE, \\(\\delta h(x) = \\delta \\overline{h}\\):</p> \\[\\delta T_L(x) = \\gamma^T \\delta T_O(x) + \\gamma^{\\Delta r} \\delta (r_O(x) - r_L(x))\\] <p>This above equation is for the default settings, but a more accurate equation can be used with the <code>delta_mse_ratio</code> and <code>taylor_level</code> arguments.</p> <p>If data from <code>n_exp</code> optical depth values provided, <code>n_exp-1</code> theoretical temperature differences will be returned for each percentile.</p> <p>Parameters:</p> Name Type Description Default <code>temp_quant_ocean</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_quant_ocean[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface ocean temperature of experiment <code>i</code>. Units: K. Note that <code>quant_use</code> is not provided as not needed by this function, but is likely to be <code>np.arange(1, 100)</code> - leave out <code>x=0</code> as doesn't really make sense to consider \\(0^{th}\\) percentile of a quantity.</p> required <code>sphum_quant_ocean</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant_ocean[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface ocean specific humidity of experiment <code>i</code>. Units: kg/kg.</p> required <code>temp_quant_land</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_quant_land[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface land temperature of experiment <code>i</code>. Units: K.</p> required <code>sphum_quant_land</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant_land[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface land specific humidity of experiment <code>i</code>. Units: kg/kg.</p> required <code>pressure_surface</code> <code>float</code> <p>Near surface pressure level. Units: Pa.</p> required <code>const_rh</code> <code>bool</code> <p>If <code>True</code>, will return the constant relative humidity version of the theory, i.e. \\(\\gamma^T \\delta \\overline{T}\\). Otherwise, will return the full theory.</p> <code>False</code> <code>delta_mse_ratio</code> <code>Optional[ndarray]</code> <p><code>float [n_exp-1, n_quant]</code> <code>delta_mse_ratio[i]</code> is the change in \\(x\\) percentile of MSE divided by the change in the mean MSE between experiment <code>i</code> and <code>i+1</code>: \\(\\delta h(x)/\\delta \\overline{h}\\). If not given, it is assumed to be equal to 1 for all \\(x\\).</p> <code>None</code> <code>taylor_level</code> <code>str</code> <p>This specifies the level of approximation that goes into the taylor series for \\(\\delta q(x)\\) and \\(\\delta \\overline{q}\\):</p> <ul> <li><code>squared</code>: Includes squared, \\(\\delta T^2\\), nonlinear, \\(\\delta T \\delta r\\), and linear terms.</li> <li><code>nonlinear</code>: Includes nonlinear, \\(\\delta T \\delta r\\), and linear terms.</li> <li><code>linear</code>: Includes just linear terms so \\(\\delta T_L(x) = \\gamma^T \\delta T_O(x) +     \\gamma^{r_O} \\delta r_O(x) +\\gamma^{r_L} \\delta r_L(x)\\)</li> <li><code>linear_rh_diff</code>: Same as <code>linear</code>, but does another approximation to combine relative humidity     contributions so: \\(\\delta T_L(x) = \\gamma^T \\delta T_O(x) +     \\gamma^{\\Delta r} \\delta (r_O(x) - r_L(x))\\)</li> </ul> <code>'linear_rh_diff'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_exp-1, n_quant]</code>. <code>delta_temp_quant_theory[i, j]</code> refers to the theoretical temperature difference between experiment <code>i</code> and <code>i+1</code> for percentile <code>quant_use[j]</code>.</p> Source code in <code>isca_tools/thesis/extrop_land_theory.py</code> <pre><code>def get_delta_temp_quant_theory(temp_quant_ocean: np.ndarray, sphum_quant_ocean: np.ndarray, temp_quant_land: np.ndarray,\n                                sphum_quant_land: np.ndarray, pressure_surface: float, const_rh: bool = False,\n                                delta_mse_ratio: Optional[np.ndarray] = None,\n                                taylor_level: str = 'linear_rh_diff') -&gt; np.ndarray:\n    \"\"\"\n    Computes the theoretical temperature difference between simulations of neighbouring optical depth values for each\n    percentile, $\\delta T(x)$, according to the assumption that changes in MSE are equal to the change in mean MSE,\n    $\\delta h(x) = \\delta \\overline{h}$:\n\n    $$\\delta T_L(x) = \\gamma^T \\delta T_O(x) + \\gamma^{\\Delta r} \\delta (r_O(x) - r_L(x))$$\n\n    This above equation is for the default settings, but a more accurate equation can be used with the `delta_mse_ratio`\n    and `taylor_level` arguments.\n\n    If data from `n_exp` optical depth values provided, `n_exp-1` theoretical temperature differences will be returned\n    for each percentile.\n\n    Args:\n        temp_quant_ocean: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_quant_ocean[i, j]` is the percentile `quant_use[j]` of near surface ocean temperature of\n            experiment `i`. Units: *K*.&lt;/br&gt;\n            Note that `quant_use` is not provided as not needed by this function, but is likely to be\n            `np.arange(1, 100)` - leave out `x=0` as doesn't really make sense to consider $0^{th}$ percentile\n            of a quantity.\n        sphum_quant_ocean: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant_ocean[i, j]` is the percentile `quant_use[j]` of near surface ocean specific humidity of\n            experiment `i`. Units: *kg/kg*.\n        temp_quant_land: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_quant_land[i, j]` is the percentile `quant_use[j]` of near surface land temperature of\n            experiment `i`. Units: *K*.&lt;/br&gt;\n        sphum_quant_land: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant_land[i, j]` is the percentile `quant_use[j]` of near surface land specific humidity of\n            experiment `i`. Units: *kg/kg*.\n        pressure_surface: Near surface pressure level. Units: *Pa*.\n        const_rh: If `True`, will return the constant relative humidity version of the theory, i.e.\n            $\\gamma^T \\delta \\overline{T}$. Otherwise, will return the full theory.\n        delta_mse_ratio: `float [n_exp-1, n_quant]`&lt;/br&gt;\n            `delta_mse_ratio[i]` is the change in $x$ percentile of MSE divided by the change in the mean MSE\n            between experiment `i` and `i+1`: $\\delta h(x)/\\delta \\overline{h}$.\n            If not given, it is assumed to be equal to 1 for all $x$.\n        taylor_level: This specifies the level of approximation that goes into the taylor series for $\\delta q(x)$\n            and $\\delta \\overline{q}$:\n\n            - `squared`: Includes squared, $\\delta T^2$, nonlinear, $\\delta T \\delta r$, and linear terms.\n            - `nonlinear`: Includes nonlinear, $\\delta T \\delta r$, and linear terms.\n            - `linear`: Includes just linear terms so $\\delta T_L(x) = \\\\gamma^T \\delta T_O(x) +\n                \\\\gamma^{r_O} \\delta r_O(x) +\\\\gamma^{r_L} \\\\delta r_L(x)$\n            - `linear_rh_diff`: Same as `linear`, but does another approximation to combine relative humidity\n                contributions so: $\\delta T_L(x) = \\gamma^T \\delta T_O(x) +\n                \\gamma^{\\Delta r} \\delta (r_O(x) - r_L(x))$\n\n    Returns:\n        `float [n_exp-1, n_quant]`.&lt;/br&gt;\n            `delta_temp_quant_theory[i, j]` refers to the theoretical temperature difference between experiment `i` and\n            `i+1` for percentile `quant_use[j]`.\n    \"\"\"\n    n_exp, n_quant = temp_quant_land.shape\n    alpha_quant_l = clausius_clapeyron_factor(temp_quant_land, pressure_surface)\n    alpha_quant_o = clausius_clapeyron_factor(temp_quant_ocean, pressure_surface)\n    sphum_quant_sat_l = sphum_sat(temp_quant_land, pressure_surface)\n    sphum_quant_sat_o = sphum_sat(temp_quant_ocean, pressure_surface)\n    r_quant_l = sphum_quant_land / sphum_quant_sat_l\n    r_quant_o = sphum_quant_ocean / sphum_quant_sat_o\n    delta_temp_quant_o = np.diff(temp_quant_ocean, axis=0)\n\n    delta_r_quant_o = np.diff(r_quant_o, axis=0)\n    delta_r_quant_l = np.diff(r_quant_l, axis=0)\n    if const_rh:\n        # get rid of relative humidity contribution if constant rh\n        delta_r_quant_o = 0 * delta_r_quant_o\n        delta_r_quant_l = 0 * delta_r_quant_l\n\n    # Pad all delta variables so same size as temp_quant - will not use this in calculation but just makes it easier\n    pad_array = ((0, 1), (0, 0))\n    if delta_mse_ratio is None:\n        delta_mse_ratio = np.ones_like(temp_quant_land)\n    else:\n        # make delta_mse_ratio the same size as all other quant variables\n        delta_mse_ratio = np.pad(delta_mse_ratio, pad_width=pad_array)\n    delta_temp_quant_o = np.pad(delta_temp_quant_o, pad_width=pad_array)\n    delta_r_quant_o = np.pad(delta_r_quant_o, pad_width=pad_array)\n    delta_r_quant_l = np.pad(delta_r_quant_l, pad_width=pad_array)\n\n    if taylor_level == 'squared':\n        # Keep squared, linear and non-linear terms in taylor expansion of delta_sphum_quant\n        coef_a = 0.5 * L_v * alpha_quant_l * sphum_quant_land * (alpha_quant_l - 2 / temp_quant_land[0])\n        coef_b = c_p + L_v * alpha_quant_l * (sphum_quant_land + sphum_quant_sat_l * delta_r_quant_l)\n        coef_c = L_v * sphum_quant_sat_l * delta_r_quant_l - delta_mse_ratio * (\n            0.5 * L_v * alpha_quant_o * sphum_quant_ocean * (alpha_quant_o - 2 / temp_quant_ocean) * delta_temp_quant_o ** 2 +\n            (c_p + L_v * alpha_quant_o * (sphum_quant_ocean + sphum_quant_sat_o * delta_r_quant_o)) * delta_temp_quant_o +\n            L_v * sphum_quant_sat_o * delta_r_quant_o)\n        delta_temp_quant_theory = np.asarray([[np.roots([coef_a[i, j], coef_b[i, j], coef_c[i, j]])[1]\n                                               for j in range(n_quant)] for i in range(n_exp - 1)])\n    elif taylor_level in ['nonlinear', 'non-linear']:\n        # Keep linear and non-linear terms in taylor expansion of delta_sphum_quant\n        coef_b = c_p + L_v * alpha_quant_l * (sphum_quant_land + sphum_quant_sat_l * delta_r_quant_l)\n        coef_c = L_v * sphum_quant_sat_l * delta_r_quant_l - delta_mse_ratio * (\n            (c_p + L_v * alpha_quant_o * (sphum_quant_ocean + sphum_quant_sat_o * delta_r_quant_o)) * delta_temp_quant_o +\n            L_v * sphum_quant_sat_o * delta_r_quant_o)\n        delta_temp_quant_theory = -coef_c[:-1] / coef_b[:-1]\n    elif taylor_level == 'linear':\n        # Only keep linear terms in taylor expansion of delta_sphum_quant\n        coef_b = c_p + L_v * alpha_quant_l * sphum_quant_land\n        coef_c = L_v * sphum_quant_sat_l * delta_r_quant_l - delta_mse_ratio * (\n            (c_p + L_v * alpha_quant_o * sphum_quant_ocean) * delta_temp_quant_o +\n            L_v * sphum_quant_sat_o * delta_r_quant_o)\n        delta_temp_quant_theory = -coef_c[:-1] / coef_b[:-1]\n    elif taylor_level == 'linear_rh_diff':\n        # combine mean and quantile RH changes with same prefactor\n        # This is a further taylor expansion of sphum_quant_sat around sphum_quant_mean\n        gamma_t, gamma_rdiff = get_gamma(temp_quant_ocean, sphum_quant_ocean, temp_quant_land, sphum_quant_land, pressure_surface)\n        delta_temp_quant_theory = (gamma_t * delta_mse_ratio * delta_temp_quant_o +\n                                   gamma_rdiff * (delta_mse_ratio * delta_r_quant_o -\n                                                  delta_r_quant_l))[:-1]\n    else:\n        raise ValueError(f\"taylor_level given is {taylor_level}. This is not valid, it must be either: 'squared', \"\n                         f\"'nonlinear', 'linear' or 'linear_rh_diff'\")\n    return delta_temp_quant_theory\n</code></pre>"},{"location":"code/thesis/extrop_land_theory/#isca_tools.thesis.extrop_land_theory.get_gamma","title":"<code>get_gamma(temp_quant_ocean, sphum_quant_ocean, temp_quant_land, sphum_quant_land, pressure_surface)</code>","text":"<p>This function returns the sensitivity parameters in the theory. One for changes in ocean temperature, \\(\\delta T_O(x)\\), and  one for difference between land and ocean relative humidity, \\(\\delta (r_O(x) - r_L(x))\\):</p> \\[\\gamma^T = \\frac{c_p + L_v \\alpha_O q_O}{c_p + L_v \\alpha_L q_L};\\quad \\gamma^{\\Delta r} = \\frac{L_v q_{O, sat}}{c_p + L_v \\alpha_L q_L}\\] <p>Parameters:</p> Name Type Description Default <code>temp_quant_ocean</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_quant_ocean[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface ocean temperature of experiment <code>i</code>. Units: K. Note that <code>quant_use</code> is not provided as not needed by this function, but is likely to be <code>np.arange(1, 100)</code> - leave out <code>x=0</code> as doesn't really make sense to consider \\(0^{th}\\) percentile of a quantity.</p> required <code>sphum_quant_ocean</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant_ocean[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface ocean specific humidity of experiment <code>i</code>. Units: kg/kg.</p> required <code>temp_quant_land</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>temp_quant_land[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface land temperature of experiment <code>i</code>. Units: K.</p> required <code>sphum_quant_land</code> <code>ndarray</code> <p><code>float [n_exp, n_quant]</code> <code>sphum_quant_land[i, j]</code> is the percentile <code>quant_use[j]</code> of near surface land specific humidity of experiment <code>i</code>. Units: kg/kg.</p> required <code>pressure_surface</code> <code>float</code> <p>Near surface pressure level. Units: Pa.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p><code>gamma_t</code>: <code>float [n_exp, n_quant]</code> The sensitivity to change in ocean temperature for each experiment and quantile.</p> <code>ndarray</code> <p><code>gamma_rdiff</code>: <code>float [n_exp, n_quant]</code> The sensitivity to change in relative humidity difference from land to ocean for each experiment and quantile.</p> Source code in <code>isca_tools/thesis/extrop_land_theory.py</code> <pre><code>def get_gamma(temp_quant_ocean: np.ndarray, sphum_quant_ocean: np.ndarray, temp_quant_land: np.ndarray,\n              sphum_quant_land: np.ndarray, pressure_surface: float) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    This function returns the sensitivity parameters in the theory.\n    One for changes in ocean temperature, $\\\\delta T_O(x)$, and  one for difference between land and ocean\n    relative humidity, $\\delta (r_O(x) - r_L(x))$:\n\n    $$\\gamma^T = \\\\frac{c_p + L_v \\\\alpha_O q_O}{c_p + L_v \\\\alpha_L q_L};\\quad\n    \\gamma^{\\Delta r} = \\\\frac{L_v q_{O, sat}}{c_p + L_v \\\\alpha_L q_L}$$\n\n    Args:\n        temp_quant_ocean: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_quant_ocean[i, j]` is the percentile `quant_use[j]` of near surface ocean temperature of\n            experiment `i`. Units: *K*.&lt;/br&gt;\n            Note that `quant_use` is not provided as not needed by this function, but is likely to be\n            `np.arange(1, 100)` - leave out `x=0` as doesn't really make sense to consider $0^{th}$ percentile\n            of a quantity.\n        sphum_quant_ocean: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant_ocean[i, j]` is the percentile `quant_use[j]` of near surface ocean specific humidity of\n            experiment `i`. Units: *kg/kg*.\n        temp_quant_land: `float [n_exp, n_quant]`&lt;/br&gt;\n            `temp_quant_land[i, j]` is the percentile `quant_use[j]` of near surface land temperature of\n            experiment `i`. Units: *K*.&lt;/br&gt;\n        sphum_quant_land: `float [n_exp, n_quant]`&lt;/br&gt;\n            `sphum_quant_land[i, j]` is the percentile `quant_use[j]` of near surface land specific humidity of\n            experiment `i`. Units: *kg/kg*.\n        pressure_surface: Near surface pressure level. Units: *Pa*.\n\n    Returns:\n        `gamma_t`: `float [n_exp, n_quant]`&lt;/br&gt;\n            The sensitivity to change in ocean temperature for each experiment and quantile.\n        `gamma_rdiff`: `float [n_exp, n_quant]`&lt;/br&gt;\n            The sensitivity to change in relative humidity difference from land to ocean for each experiment and\n            quantile.\n    \"\"\"\n    alpha_quant_l = clausius_clapeyron_factor(temp_quant_land, pressure_surface)\n    alpha_quant_o = clausius_clapeyron_factor(temp_quant_ocean, pressure_surface)\n    sphum_quant_sat_o = sphum_sat(temp_quant_ocean, pressure_surface)\n    denom = c_p + L_v * alpha_quant_l * sphum_quant_land\n    gamma_t = (c_p + L_v * alpha_quant_o * sphum_quant_ocean) / denom\n    gamma_rdiff = L_v / denom * sphum_quant_sat_o\n    return gamma_t, gamma_rdiff\n</code></pre>"},{"location":"code/thesis/lapse_integral/","title":"Lapse Integration","text":""},{"location":"code/thesis/lapse_integral/#isca_tools.thesis.lapse_integral.const_lapse_fitting","title":"<code>const_lapse_fitting(temp_env_lev, p_lev, temp_env_lower, p_lower, temp_env_upper, p_upper, sanity_check=False)</code>","text":"<p>Find the bulk lapse rate such that \\(\\int_{p_1}^{p_2} \\Gamma_{env}(p) d\\ln p = \\Gamma_{bulk} \\ln (p_2/p_1)\\). Then computes the error in this approximation: \\(\\int_{p_1}^{p_2} |\\Gamma_{env}(p) - \\Gamma_{bulk}| d\\ln p\\).</p> <p>Parameters:</p> Name Type Description Default <code>temp_env_lev</code> <code>Union[DataArray, ndarray]</code> <p><code>[n_lev]</code> Environmental temperature at pressure <code>p_lev</code>.</p> required <code>p_lev</code> <code>Union[DataArray, ndarray]</code> <p><code>[n_lev]</code> Model pressure levels in Pa.</p> required <code>temp_env_lower</code> <code>float</code> <p>Environmental temperature at lower pressure level (nearer surface) <code>p_lower</code>.</p> required <code>p_lower</code> <code>float</code> <p>Pressure level to start profile (near surface).</p> required <code>temp_env_upper</code> <code>float</code> <p>Environmental temperature at upper pressure level (further from surface) <code>p_upper</code>.</p> required <code>p_upper</code> <code>float</code> <p>Pressure level to end profile (further from surface).</p> required <code>sanity_check</code> <code>bool</code> <p>If <code>True</code> will print a sanity check to ensure the calculation is correct.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>lapse_bulk</code> <code>float</code> <p>Bulk lapse rate. Units are K/km.</p> <code>integral</code> <code>float</code> <p>Result of integral \\(\\int_{p_1}^{p_2} \\Gamma_{env}(p) d\\ln p\\). Units are K/km.</p> <code>integral_error</code> <code>float</code> <p>Result of integral \\(\\int_{p_1}^{p_2} |\\Gamma_{env}(p) - \\Gamma_{bulk}| d\\ln p\\). Units are K/km.</p> <code>temp_env_approx_lev</code> <code>Union[DataArray, ndarray]</code> <p><code>[n_lev]</code> Estimate of environmental temperature at pressure <code>p_lev</code>.</p> Source code in <code>isca_tools/thesis/lapse_integral.py</code> <pre><code>def const_lapse_fitting(temp_env_lev: Union[xr.DataArray, np.ndarray], p_lev: Union[xr.DataArray, np.ndarray],\n                        temp_env_lower: float, p_lower: float,\n                        temp_env_upper: float, p_upper: float,\n                        sanity_check: bool = False) -&gt; Tuple[\n    float, float, float, Union[xr.DataArray, np.ndarray]]:\n    \"\"\"\n    Find the bulk lapse rate such that $\\int_{p_1}^{p_2} \\Gamma_{env}(p) d\\ln p = \\Gamma_{bulk} \\ln (p_2/p_1)$.\n    Then computes the error in this approximation: $\\int_{p_1}^{p_2} |\\Gamma_{env}(p) - \\Gamma_{bulk}| d\\ln p$.\n\n    Args:\n        temp_env_lev: `[n_lev]` Environmental temperature at pressure `p_lev`.\n        p_lev: `[n_lev]` Model pressure levels in Pa.\n        temp_env_lower: Environmental temperature at lower pressure level (nearer surface) `p_lower`.\n        p_lower: Pressure level to start profile (near surface).\n        temp_env_upper: Environmental temperature at upper pressure level (further from surface) `p_upper`.\n        p_upper: Pressure level to end profile (further from surface).\n        sanity_check: If `True` will print a sanity check to ensure the calculation is correct.\n\n    Returns:\n        lapse_bulk: Bulk lapse rate. Units are *K/km*.\n        integral: Result of integral $\\int_{p_1}^{p_2} \\Gamma_{env}(p) d\\ln p$. Units are *K/km*.\n        integral_error: Result of integral $\\int_{p_1}^{p_2} |\\Gamma_{env}(p) - \\Gamma_{bulk}| d\\ln p$.\n            Units are *K/km*.\n        temp_env_approx_lev: `[n_lev]` Estimate of environmental temperature at pressure `p_lev`.\n    \"\"\"\n    # Compute integral of actual environmental lapse rate between p_lower and p_upper\n    # lapse_integral = integral_lapse_dlnp_hydrostatic(temp_env_lev, p_lev, p_lower, p_upper,\n    #                                                  temp_env_lower, temp_env_upper)\n    lapse_integral = g/R * np.log(temp_env_upper / temp_env_lower)\n    # Define bulk lapse rate such that a profile following constant lapse rate between p_lower and p_upper\n    # would have same value of above integral as actual profile\n    lapse_bulk = lapse_integral / np.log(p_upper / p_lower)\n    temp_env_approx_lev = get_temp_const_lapse(p_lev, temp_env_lower, p_lower, lapse_bulk)\n    temp_env_approx_upper = get_temp_const_lapse(p_upper, temp_env_lower, p_lower, lapse_bulk)\n\n    if sanity_check:\n        # sanity check, this should be the same as lapse_integral\n        lapse_integral_approx = integral_lapse_dlnp_hydrostatic(temp_env_approx_lev, p_lev, p_lower, p_upper,\n                                                                temp_env_lower, temp_env_upper)\n        print(\n            f'Actual lapse integral: {lapse_integral * 1000:.3f} K/km\\nApprox lapse integral: {lapse_integral_approx * 1000:.3f} K/km')\n        # Will use lapse rate such that approx value of T_upper is exact. Check that here\n        print(f'Actual temp_upper: {temp_env_upper:.3f} K\\nApprox temp_upper: {temp_env_approx_upper:.3f} K')\n\n    # Quantify error in approx of constant lapse rate by integral of absolute deviation between actual lapse rate\n    # and constant approx value\n    lapse_integral_error = integral_lapse_dlnp_hydrostatic(temp_env_lev, p_lev, p_lower, p_upper,\n                                                           temp_env_lower, temp_env_upper, temp_env_approx_lev,\n                                                           temp_env_lower, temp_env_approx_upper, take_abs=True)\n    return lapse_bulk * 1000, lapse_integral * 1000, lapse_integral_error * 1000, temp_env_approx_lev\n</code></pre>"},{"location":"code/thesis/lapse_integral/#isca_tools.thesis.lapse_integral.fitting_2_layer","title":"<code>fitting_2_layer(temp_env_lev, p_lev, temp_env_lower, p_lower, temp_env_upper, p_upper, temp_env_upper2, p_upper2, temp_parcel_lev=None, temp_parcel_lower=None, temp_parcel_upper=None, temp_parcel_upper2=None, method_layer1='const', method_layer2='const', sanity_check=False)</code>","text":"<p>Applies <code>const_lapse_fitting</code> or <code>mod_parcel_lapse_fitting</code> to each layer.</p> <p>Parameters:</p> Name Type Description Default <code>temp_env_lev</code> <code>Union[DataArray, ndarray]</code> <p><code>[n_lev]</code> Environmental temperature at pressure <code>p_lev</code>.</p> required <code>p_lev</code> <code>Union[DataArray, ndarray]</code> <p><code>[n_lev]</code> Model pressure levels in Pa.</p> required <code>temp_env_lower</code> <code>float</code> <p>Environmental temperature at lower pressure level (nearer surface) <code>p_lower</code>.</p> required <code>p_lower</code> <code>float</code> <p>Pressure level to start profile (near surface).</p> required <code>temp_env_upper</code> <code>float</code> <p>Environmental temperature at the upper pressure level of the first layer (layer closest to surface) <code>p_upper</code>.</p> required <code>p_upper</code> <code>float</code> <p>Pressure level to end profile of first layer (layer closest to surface).</p> required <code>temp_env_upper2</code> <code>float</code> <p>Environmental temperature at the upper pressure level of the second layer (layer furthest from surface) <code>p_upper2</code>.</p> required <code>p_upper2</code> <code>float</code> <p>Pressure level to end profile of second layer (layer furthest from surface).</p> required <code>temp_parcel_lev</code> <code>Optional[Union[DataArray, ndarray]]</code> <p><code>[n_lev]</code> Parcel temperature (following moist adiabat) at pressure <code>p_lev</code>. Only required if either <code>method_layer1</code> or <code>method_layer2</code> are <code>'mod_parcel'</code>.</p> <code>None</code> <code>temp_parcel_lower</code> <code>Optional[float]</code> <p>Parcel temperature at lower pressure level (nearer surface) <code>p_lower</code>. Only required if either <code>method_layer1 = 'mod_parcel'</code>.</p> <code>None</code> <code>temp_parcel_upper</code> <code>Optional[float]</code> <p>Parcel temperature at the upper pressure level of the first layer (layer closest to surface) <code>p_upper</code>. Only required if either <code>method_layer1</code> or <code>method_layer2</code> are <code>'mod_parcel'</code>.</p> <code>None</code> <code>temp_parcel_upper2</code> <code>Optional[float]</code> <p>Parcel temperature at the upper pressure level of the second layer (layer furthest from surface) <code>p_upper2</code>. Only required if either <code>method_layer2 = 'mod_parcel'</code>.</p> <code>None</code> <code>method_layer1</code> <code>Literal['const', 'mod_parcel']</code> <p>Which fitting method to use for layer 1.</p> <code>'const'</code> <code>method_layer2</code> <code>Literal['const', 'mod_parcel']</code> <p>Which fitting method to use for layer 2.</p> <code>'const'</code> <code>sanity_check</code> <code>bool</code> <p>If <code>True</code> will print a sanity check to ensure the calculation is correct.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>lapse</code> <code>ndarray</code> <p>Lapse rate info for each layer. Bulk lapse rate if <code>method_layer='const'</code> or lapse rate adjustment if <code>method_layer='mod_parcel'</code>. Units are K/km.</p> <code>integral</code> <code>ndarray</code> <p>Result of integral \\(\\int_{p_1}^{p_2} \\Gamma_{env}(p) d\\ln p\\) of each layer. Units are K/km.</p> <code>integral_error</code> <code>ndarray</code> <p>Result of integral \\(\\int_{p_1}^{p_2} |\\Gamma_{env}(p) - \\Gamma_{approx}| d\\ln p\\) of each layer. Units are K/km.</p> <code>temp_env_approx_lev</code> <code>Union[DataArray, ndarray]</code> <p><code>[n_lev]</code> Estimate of environmental temperature at pressure <code>p_lev</code>.</p> Source code in <code>isca_tools/thesis/lapse_integral.py</code> <pre><code>def fitting_2_layer(temp_env_lev: Union[xr.DataArray, np.ndarray], p_lev: Union[xr.DataArray, np.ndarray],\n                    temp_env_lower: float, p_lower: float,\n                    temp_env_upper: float, p_upper: float,\n                    temp_env_upper2: float, p_upper2: float,\n                    temp_parcel_lev: Optional[Union[xr.DataArray, np.ndarray]] = None,\n                    temp_parcel_lower: Optional[float] = None,\n                    temp_parcel_upper: Optional[float] = None, temp_parcel_upper2: Optional[float] = None,\n                    method_layer1: Literal['const', 'mod_parcel'] = 'const',\n                    method_layer2: Literal['const', 'mod_parcel'] = 'const',\n                    sanity_check: bool = False) -&gt; Tuple[\n    np.ndarray, np.ndarray, np.ndarray, Union[xr.DataArray, np.ndarray]]:\n    \"\"\"\n    Applies `const_lapse_fitting` or `mod_parcel_lapse_fitting` to each layer.\n\n    Args:\n        temp_env_lev: `[n_lev]` Environmental temperature at pressure `p_lev`.\n        p_lev: `[n_lev]` Model pressure levels in Pa.\n        temp_env_lower: Environmental temperature at lower pressure level (nearer surface) `p_lower`.\n        p_lower: Pressure level to start profile (near surface).\n        temp_env_upper: Environmental temperature at the upper pressure level of the first layer (layer closest to surface) `p_upper`.\n        p_upper: Pressure level to end profile of first layer (layer closest to surface).\n        temp_env_upper2: Environmental temperature at the upper pressure level of the second layer (layer furthest from surface) `p_upper2`.\n        p_upper2: Pressure level to end profile of second layer (layer furthest from surface).\n        temp_parcel_lev: `[n_lev]` Parcel temperature (following moist adiabat) at pressure `p_lev`.&lt;/br&gt;\n            Only required if either `method_layer1` or `method_layer2` are `'mod_parcel'`.\n        temp_parcel_lower: Parcel temperature at lower pressure level (nearer surface) `p_lower`.&lt;/br&gt;\n            Only required if either `method_layer1 = 'mod_parcel'`.\n        temp_parcel_upper: Parcel temperature at the upper pressure level of the first layer (layer closest to surface) `p_upper`.&lt;/br&gt;\n            Only required if either `method_layer1` or `method_layer2` are `'mod_parcel'`.\n        temp_parcel_upper2: Parcel temperature at the upper pressure level of the second layer (layer furthest from surface) `p_upper2`.&lt;/br&gt;\n            Only required if either `method_layer2 = 'mod_parcel'`.\n        method_layer1: Which fitting method to use for layer 1.\n        method_layer2: Which fitting method to use for layer 2.\n        sanity_check: If `True` will print a sanity check to ensure the calculation is correct.\n\n    Returns:\n        lapse: Lapse rate info for each layer. Bulk lapse rate if `method_layer='const'` or lapse rate adjustment\n            if `method_layer='mod_parcel'`. Units are *K/km*.\n        integral: Result of integral $\\int_{p_1}^{p_2} \\Gamma_{env}(p) d\\ln p$ of each layer. Units are *K/km*.\n        integral_error: Result of integral $\\int_{p_1}^{p_2} |\\Gamma_{env}(p) - \\Gamma_{approx}| d\\ln p$ of each layer.\n            Units are *K/km*.\n        temp_env_approx_lev: `[n_lev]` Estimate of environmental temperature at pressure `p_lev`.\n    \"\"\"\n    if method_layer1 == 'const':\n        lapse1, lapse_integral1, lapse_integral_error1, temp_env_approx1 = \\\n            const_lapse_fitting(temp_env_lev, p_lev, temp_env_lower, p_lower, temp_env_upper, p_upper, sanity_check)\n    elif method_layer2 == 'mod_parcel':\n        lapse1, lapse_integral1, lapse_integral_error1, temp_env_approx1 = \\\n            mod_parcel_lapse_fitting(temp_env_lev, p_lev, temp_env_lower, p_lower, temp_env_upper, p_upper,\n                                     temp_parcel_lev, temp_parcel_lower, temp_parcel_upper, sanity_check)\n    else:\n        raise ValueError(f'method_layer1 = {method_layer1} not recognized.')\n\n    if method_layer2 == 'const':\n        lapse2, lapse_integral2, lapse_integral_error2, temp_env_approx2 = \\\n            const_lapse_fitting(temp_env_lev, p_lev, temp_env_upper, p_upper, temp_env_upper2, p_upper2,\n                                sanity_check)\n    elif method_layer2 == 'mod_parcel':\n        lapse2, lapse_integral2, lapse_integral_error2, temp_env_approx2 = \\\n            mod_parcel_lapse_fitting(temp_env_lev, p_lev, temp_env_upper, p_upper, temp_env_upper2, p_upper2,\n                                     temp_parcel_lev, temp_parcel_upper, temp_parcel_upper2, sanity_check)\n    else:\n        raise ValueError(f'method_layer2 = {method_layer2} not recognized.')\n\n    lapse = np.asarray([lapse1, lapse2])\n    lapse_integral = np.asarray([lapse_integral1, lapse_integral2])\n    lapse_integral_error = np.asarray([lapse_integral_error1, lapse_integral_error2])\n    temp_env_approx = np.where(p_lev &lt; p_upper, temp_env_approx2, temp_env_approx1)\n    return lapse, lapse_integral, lapse_integral_error, temp_env_approx\n</code></pre>"},{"location":"code/thesis/lapse_integral/#isca_tools.thesis.lapse_integral.get_temp_const_lapse","title":"<code>get_temp_const_lapse(p_lev, temp_low, p_low, lapse)</code>","text":"<p>Get the temperature at <code>p_lev</code> assuming constant lapse rate up from <code>temp_low</code> at <code>p_low</code>.</p> <p>This assumes hydrostatic balance: \\(\\Gamma(p) = \u2212\\frac{dT}{dz} = \\frac{g}{R} \\frac{d \\ln T}{d\\ln p}\\)</p> <p>Parameters:</p> Name Type Description Default <code>p_lev</code> <code>Union[DataArray, ndarray, float]</code> <p><code>[n_lev]</code> Pressure levels to find temperature. Units: Pa.</p> required <code>temp_low</code> <code>Union[DataArray, ndarray, float]</code> <p>Temperature at low pressure <code>p_low</code>. Units: K.</p> required <code>p_low</code> <code>Union[DataArray, ndarray, float]</code> <p>Pressure level where to start ascent from along constant lapse rate profile. Units: Pa.</p> required <code>lapse</code> <code>Union[DataArray, ndarray, float]</code> <p>Constant lapse rate, \\(\\Gamma\\), to use to find temperature at <code>p_lev</code>. Units: K/m.</p> required <p>Returns:</p> Name Type Description <code>temp_lev</code> <code>Union[DataArray, ndarray, float]</code> <p><code>[n_lev]</code> Temperature at <code>p_lev</code>. Units: K.</p> Source code in <code>isca_tools/thesis/lapse_integral.py</code> <pre><code>def get_temp_const_lapse(p_lev: Union[xr.DataArray, np.ndarray, float],\n                         temp_low: Union[xr.DataArray, np.ndarray, float],\n                         p_low: Union[xr.DataArray, np.ndarray, float],\n                         lapse: Union[xr.DataArray, np.ndarray, float]) -&gt; Union[xr.DataArray, np.ndarray, float]:\n    \"\"\"\n    Get the temperature at `p_lev` assuming constant lapse rate up from `temp_low` at `p_low`.\n\n    This assumes hydrostatic balance: $\\Gamma(p) = \u2212\\\\frac{dT}{dz} = \\\\frac{g}{R} \\\\frac{d \\ln T}{d\\ln p}$\n\n    Args:\n        p_lev: `[n_lev]` Pressure levels to find temperature. Units: Pa.\n        temp_low: Temperature at low pressure `p_low`. Units: K.\n        p_low: Pressure level where to start ascent from along constant lapse rate profile. Units: Pa.\n        lapse: Constant lapse rate, $\\Gamma$, to use to find temperature at `p_lev`. Units: K/m.\n\n    Returns:\n        temp_lev: `[n_lev]` Temperature at `p_lev`. Units: K.\n    \"\"\"\n    return temp_low * (p_lev / p_low) ** (lapse * R / g)\n</code></pre>"},{"location":"code/thesis/lapse_integral/#isca_tools.thesis.lapse_integral.get_temp_mod_parcel_lapse","title":"<code>get_temp_mod_parcel_lapse(p_lev, p_low, temp_parcel_lev, lapse_diff_const)</code>","text":"<p>This finds the temperature at pressure levels <code>p_lev</code> following a lapse rate \\(\\Gamma(p) = \\Gamma_p(p, T_p(p)) + \\eta\\) where \\(\\Gamma_p(p, T)\\) is the parcel (moist adiabatic) lapse rate and \\(\\eta\\) is a constant. \\(T_p(p)\\) refers to parcel temperature at pressure \\(p\\).</p> <p>This assumes hydrostatic balance: \\(\\Gamma(p) = \u2212\\frac{dT}{dz} = \\frac{g}{R} \\frac{d \\ln T}{d\\ln p}\\)</p> <p>Parameters:</p> Name Type Description Default <code>p_lev</code> <code>Union[DataArray, ndarray, float]</code> <p><code>[n_lev]</code> Pressure levels to find environmental temperature. Units: Pa.</p> required <code>p_low</code> <code>Union[DataArray, ndarray, float]</code> <p>Pressure level where to start the ascent from along the modified parcel profile. Units: Pa.</p> required <code>temp_parcel_lev</code> <code>Union[DataArray, ndarray, float]</code> <p><code>[n_lev]</code> Parcel temperature at pressure <code>p_lev</code>. Units: K.</p> required <code>lapse_diff_const</code> <code>Union[DataArray, ndarray, float]</code> <p>Constant, \\(\\eta\\), which is added to the parcel lapse rate at each pressure level. Units: K/m.</p> required <p>Returns:</p> Name Type Description <code>temp_lev</code> <code>Union[DataArray, ndarray, float]</code> <p><code>[n_lev]</code> Temperature at <code>p_lev</code>. Units: K.</p> Source code in <code>isca_tools/thesis/lapse_integral.py</code> <pre><code>def get_temp_mod_parcel_lapse(p_lev: Union[xr.DataArray, np.ndarray, float],\n                              p_low: Union[xr.DataArray, np.ndarray, float],\n                              temp_parcel_lev: Union[xr.DataArray, np.ndarray, float],\n                              lapse_diff_const: Union[xr.DataArray, np.ndarray, float]\n                              ) -&gt; Union[xr.DataArray, np.ndarray, float]:\n    \"\"\"\n    This finds the temperature at pressure levels `p_lev` following a lapse rate $\\Gamma(p) = \\Gamma_p(p, T_p(p)) + \\eta$\n    where $\\Gamma_p(p, T)$ is the parcel (moist adiabatic) lapse rate and $\\eta$ is a constant. $T_p(p)$ refers\n    to parcel temperature at pressure $p$.\n\n    This assumes hydrostatic balance: $\\Gamma(p) = \u2212\\\\frac{dT}{dz} = \\\\frac{g}{R} \\\\frac{d \\ln T}{d\\ln p}$\n\n    Args:\n        p_lev: `[n_lev]` Pressure levels to find environmental temperature. Units: Pa.\n        p_low: Pressure level where to start the ascent from along the modified parcel profile. Units: Pa.\n        temp_parcel_lev: `[n_lev]` Parcel temperature at pressure `p_lev`. Units: K.\n        lapse_diff_const: Constant, $\\eta$, which is added to the parcel lapse rate at each pressure level. Units: K/m.\n\n    Returns:\n        temp_lev: `[n_lev]` Temperature at `p_lev`. Units: K.\n    \"\"\"\n    # Compute temperature at p_upper such that lapse rate at all levels is the same as parcel plus `lapse_diff_const`.\n    # lapse_parcel_integral = integral_lapse_dlnp_hydrostatic(temp_parcel_lev, p_lev, p_lower, p_upper, temp_lower,\n    #                                                         temp_parcel_upper)\n    # lapse_parcel_integral = g / R * np.log(temp_parcel_lev / temp_low)\n    # return temp_lower * (p_lev / p_low) ** (lapse_diff_const * R / g) * np.exp(R / g * lapse_parcel_integral)\n    return get_temp_const_lapse(p_lev, temp_parcel_lev, p_low, lapse_diff_const)\n</code></pre>"},{"location":"code/thesis/lapse_integral/#isca_tools.thesis.lapse_integral.integral_lapse_dlnp_hydrostatic","title":"<code>integral_lapse_dlnp_hydrostatic(temp_lev, p_lev, p1, p2, T_p1, T_p2, temp_ref_lev=None, temp_ref_p1=None, temp_ref_p2=None, take_abs=False)</code>","text":"<p>Compute \\(\\int_{p_1}^{p_2} \\Gamma d\\ln p\\) using the hydrostatic relation (converted to pressure integral) only (no Z required), where \\(\\Gamma = -dT/dz\\) is the lapse rate. Can also compute \\(\\int_{p_1}^{p_2} \\Gamma - \\Gamma_{ref} d\\ln p\\)</p> <p>Uses hydrostatic balance, \\(d\\ln p = -\\frac{g}{RT(p)} dz\\), to convert integral into</p> <p>Parameters:</p> Name Type Description Default <code>temp_lev</code> <code>Union[DataArray, ndarray]</code> <p>xr.DataArray Temperature [K], dims include 'lev' (vertical pressure coordinate)</p> required <code>p_lev</code> <code>Union[DataArray, ndarray]</code> <p>xr.DataArray Pressure [Pa], same 'lev' coordinate as temp_lev</p> required <code>p1</code> <code>float</code> <p>float Lower integration limit [Pa]</p> required <code>p2</code> <code>float</code> <p>float Upper integration limit [Pa]</p> required <code>T_p1</code> <code>float</code> <p>float | None, optional Temperature at p1 [K]; if None, will be log-interpolated from temp_lev</p> required <code>T_p2</code> <code>float</code> <p>float | None, optional Temperature at p2 [K]; if None, will be log-interpolated from temp_lev</p> required <code>temp_ref_lev</code> <code>Optional[Union[DataArray, ndarray]]</code> <p>Temperature of reference profile at pressure <code>p_lev</code>.</p> <code>None</code> <code>temp_ref_p1</code> <code>Optional[float]</code> <p>Temperature of reference profile at pressure <code>p1</code>.</p> <code>None</code> <code>temp_ref_p2</code> <code>Optional[float]</code> <p>Temperature of reference profile at pressure <code>p2</code>.</p> <code>None</code> <code>take_abs</code> <code>bool</code> <p>If <code>True</code>, and provide <code>temp_ref_lev</code>, will compute \\(\\int_{p_1}^{p_2} |\\Gamma - \\Gamma_{ref}| d\\ln p\\). Otherwise, will compute \\(\\int_{p_1}^{p_2} \\Gamma - \\Gamma_{ref} d\\ln p\\).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>integral</code> <code>float</code> <p>Value of the integral</p> Source code in <code>isca_tools/thesis/lapse_integral.py</code> <pre><code>def integral_lapse_dlnp_hydrostatic(temp_lev: Union[xr.DataArray, np.ndarray], p_lev: Union[xr.DataArray, np.ndarray],\n                                    p1: float, p2: float, T_p1: float, T_p2: float,\n                                    temp_ref_lev: Optional[Union[xr.DataArray, np.ndarray]] = None,\n                                    temp_ref_p1: Optional[float] = None, temp_ref_p2: Optional[float] = None,\n                                    take_abs: bool = False) -&gt; float:\n    \"\"\"\n    Compute $\\int_{p_1}^{p_2} \\Gamma d\\ln p$ using the hydrostatic relation (converted to pressure integral) only (no Z required),\n    where $\\Gamma = -dT/dz$ is the lapse rate.\n    Can also compute $\\int_{p_1}^{p_2} \\Gamma - \\Gamma_{ref} d\\ln p$\n\n    Uses hydrostatic balance, $d\\ln p = -\\\\frac{g}{RT(p)} dz$, to convert integral into\n\n    Args:\n        temp_lev: xr.DataArray\n            Temperature [K], dims include 'lev' (vertical pressure coordinate)\n        p_lev: xr.DataArray\n            Pressure [Pa], same 'lev' coordinate as temp_lev\n        p1: float\n            Lower integration limit [Pa]\n        p2: float\n            Upper integration limit [Pa]\n        T_p1: float | None, optional\n            Temperature at p1 [K]; if None, will be log-interpolated from temp_lev\n        T_p2: float | None, optional\n            Temperature at p2 [K]; if None, will be log-interpolated from temp_lev\n        temp_ref_lev: Temperature of reference profile at pressure `p_lev`.\n        temp_ref_p1: Temperature of reference profile at pressure `p1`.\n        temp_ref_p2: Temperature of reference profile at pressure `p2`.\n        take_abs: If `True`, and provide `temp_ref_lev`, will compute $\\int_{p_1}^{p_2} |\\Gamma - \\Gamma_{ref}| d\\ln p$.\n            Otherwise, will compute $\\int_{p_1}^{p_2} \\Gamma - \\Gamma_{ref} d\\ln p$.\n\n    Returns:\n        integral: Value of the integral\n    \"\"\"\n    if isinstance(temp_lev, xr.DataArray):\n        temp_lev = temp_lev.values\n    if isinstance(p_lev, xr.DataArray):\n        p_lev = p_lev.values\n    # Ensure descending pressure order (p_lev decreases with height)\n    if p_lev[0] &lt; p_lev[-1]:\n        temp_lev, p_lev = temp_lev[::-1], p_lev[::-1]\n        if temp_ref_lev is not None:\n            temp_ref_lev = temp_ref_lev[::-1]\n\n    # Build augmented pressure and temperature arrays\n    # P_aug = np.concatenate(([p1], p_lev[(p_lev &gt; min(p1, p2)) &amp; (p_lev &lt; max(p1, p2))], [p2]))\n    T_aug = np.concatenate(([T_p1], temp_lev[(p_lev &gt; min(p1, p2)) &amp; (p_lev &lt; max(p1, p2))], [T_p2]))\n    # T_aug = np.interp(np.log(P_aug), np.log(p_lev.values), temp_lev.values)\n    T_aug[0], T_aug[-1] = T_p1, T_p2  # enforce provided endpoints if given\n\n    # Reference profile, if provided\n    if temp_ref_lev is not None:\n        if isinstance(temp_ref_lev, xr.DataArray):\n            temp_ref_lev = temp_ref_lev.values\n        Tref_aug = np.concatenate(([temp_ref_p1], temp_ref_lev[(p_lev &gt; min(p1, p2)) &amp; (p_lev &lt; max(p1, p2))],\n                                   [temp_ref_p2]))\n        Tref_aug[0], Tref_aug[-1] = temp_ref_p1, temp_ref_p2\n    else:\n        Tref_aug = None\n\n    def compute_integrand(Tprof):\n        # Finite-difference derivative\n        dT = np.diff(Tprof)\n        Tmean = 0.5 * (Tprof[:-1] + Tprof[1:])\n        return dT / Tmean\n\n    if Tref_aug is None:\n        integral = g / R * np.sum(compute_integrand(T_aug))\n    else:\n        if take_abs:\n            integral = g / R * np.sum(np.abs(compute_integrand(T_aug) - compute_integrand(Tref_aug)))\n        else:\n            integral = g / R * np.sum(compute_integrand(T_aug) - compute_integrand(Tref_aug))\n    return float(integral)\n</code></pre>"},{"location":"code/thesis/lapse_integral/#isca_tools.thesis.lapse_integral.mod_parcel_lapse_fitting","title":"<code>mod_parcel_lapse_fitting(temp_env_lev, p_lev, temp_env_lower, p_lower, temp_env_upper, p_upper, temp_parcel_lev, temp_parcel_lower, temp_parcel_upper, sanity_check=False)</code>","text":"<p>Find the constant, \\(\\eta\\) that needs adding to parcel lapse rate such that \\(\\int_{p_1}^{p_2} \\Gamma_{env}(p) d\\ln p = \\int_{p_1}^{p_2} \\Gamma_p(p, T_p(p)) + \\eta d\\ln p\\). Then computes the error in this approximation: \\(\\int_{p_1}^{p_2} |\\Gamma_{env}(p) - \\Gamma_p(p, T_p(p)) - \\eta| d\\ln p\\).</p> <p>where \\(\\Gamma_p(p, T)\\) is the parcel (moist adiabatic) lapse rate and \\(T_p(p)\\) is the parcel temperature at pressure \\(p\\) starting at \\(p_1\\).</p> <p>Parameters:</p> Name Type Description Default <code>temp_env_lev</code> <code>Union[DataArray, ndarray]</code> <p><code>[n_lev]</code> Environmental temperature at pressure <code>p_lev</code>.</p> required <code>p_lev</code> <code>Union[DataArray, ndarray]</code> <p><code>[n_lev]</code> Model pressure levels in Pa.</p> required <code>temp_env_lower</code> <code>float</code> <p>Environmental temperature at lower pressure level (nearer surface) <code>p_lower</code>.</p> required <code>p_lower</code> <code>float</code> <p>Pressure level to start profile (near surface).</p> required <code>temp_env_upper</code> <code>float</code> <p>Environmental temperature at upper pressure level (further from surface) <code>p_upper</code>.</p> required <code>p_upper</code> <code>float</code> <p>Pressure level to end profile (further from surface).</p> required <code>temp_parcel_lev</code> <code>Union[DataArray, ndarray]</code> <p><code>[n_lev]</code> Parcel temperature (following moist adiabat) at pressure <code>p_lev</code>.</p> required <code>temp_parcel_lower</code> <code>float</code> <p>Parcel temperature at lower pressure level (nearer surface) <code>p_lower</code>.</p> required <code>temp_parcel_upper</code> <code>float</code> <p>Parcel temperature at upper pressure level (further from surface) <code>p_upper</code>.</p> required <code>sanity_check</code> <code>bool</code> <p>If <code>True</code> will print a sanity check to ensure the calculation is correct.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>lapse_diff_const</code> <code>float</code> <p>Lapse rate adjustment, \\(\\eta\\) which needs to be added to \\(\\Gamma_{p}(p, T_p(p))\\) so integral matches that of environmental lapse rate. Units are K/km.</p> <code>integral</code> <code>float</code> <p>Result of integral \\(\\int_{p_1}^{p_2} \\Gamma_{env}(p) d\\ln p\\). Units are K/km.</p> <code>integral_error</code> <code>float</code> <p>Result of integral \\(\\int_{p_1}^{p_2} |\\Gamma_{env}(p) - \\Gamma_p(p) - \\eta| d\\ln p\\). Units are K/km.</p> <code>temp_env_approx_lev</code> <code>Union[DataArray, ndarray]</code> <p><code>[n_lev]</code> Estimate of environmental temperature at pressure <code>p_lev</code>.</p> Source code in <code>isca_tools/thesis/lapse_integral.py</code> <pre><code>def mod_parcel_lapse_fitting(temp_env_lev: Union[xr.DataArray, np.ndarray],\n                             p_lev: Union[xr.DataArray, np.ndarray],\n                             temp_env_lower: float, p_lower: float,\n                             temp_env_upper: float, p_upper: float,\n                             temp_parcel_lev: Union[xr.DataArray, np.ndarray],\n                             temp_parcel_lower: float, temp_parcel_upper: float,\n                             sanity_check: bool = False) -&gt; Tuple[\n    float, float, float, Union[xr.DataArray, np.ndarray]]:\n    \"\"\"\n    Find the constant, $\\eta$ that needs adding to parcel lapse rate such that\n    $\\int_{p_1}^{p_2} \\Gamma_{env}(p) d\\ln p = \\int_{p_1}^{p_2} \\Gamma_p(p, T_p(p)) + \\eta d\\ln p$.\n    Then computes the error in this approximation:\n    $\\int_{p_1}^{p_2} |\\Gamma_{env}(p) - \\Gamma_p(p, T_p(p)) - \\eta| d\\ln p$.\n\n    where $\\Gamma_p(p, T)$ is the parcel (moist adiabatic) lapse rate and $T_p(p)$ is the parcel temperature\n    at pressure $p$ starting at $p_1$.\n\n    Args:\n        temp_env_lev: `[n_lev]` Environmental temperature at pressure `p_lev`.\n        p_lev: `[n_lev]` Model pressure levels in Pa.\n        temp_env_lower: Environmental temperature at lower pressure level (nearer surface) `p_lower`.\n        p_lower: Pressure level to start profile (near surface).\n        temp_env_upper: Environmental temperature at upper pressure level (further from surface) `p_upper`.\n        p_upper: Pressure level to end profile (further from surface).\n        temp_parcel_lev: `[n_lev]` Parcel temperature (following moist adiabat) at pressure `p_lev`.\n        temp_parcel_lower: Parcel temperature at lower pressure level (nearer surface) `p_lower`.\n        temp_parcel_upper: Parcel temperature at upper pressure level (further from surface) `p_upper`.\n        sanity_check: If `True` will print a sanity check to ensure the calculation is correct.\n\n    Returns:\n        lapse_diff_const: Lapse rate adjustment, $\\eta$ which needs to be added to $\\Gamma_{p}(p, T_p(p))$\n            so integral matches that of environmental lapse rate. Units are *K/km*.\n        integral: Result of integral $\\int_{p_1}^{p_2} \\Gamma_{env}(p) d\\ln p$. Units are *K/km*.\n        integral_error: Result of integral $\\int_{p_1}^{p_2} |\\Gamma_{env}(p) - \\Gamma_p(p) - \\eta| d\\ln p$.\n            Units are *K/km*.\n        temp_env_approx_lev: `[n_lev]` Estimate of environmental temperature at pressure `p_lev`.\n    \"\"\"\n    # Compute integral of deviation between environmental and parcel lapse rate between p_lower and p_upper\n    # lapse_diff_integral = integral_lapse_dlnp_hydrostatic(temp_env_lev, p_lev, p_lower, p_upper, temp_env_lower,\n    #                                                       temp_env_upper, temp_parcel_lev, temp_parcel_lower,\n    #                                                       temp_parcel_upper)\n    lapse_diff_integral = g / R * (np.log(temp_env_upper / temp_env_lower) - np.log(temp_parcel_upper / temp_parcel_lower))\n    # Compute the constant needed to be added to the parcel lapse rate at each level to make above integral equal 0.\n    lapse_diff_const = lapse_diff_integral / np.log(p_upper / p_lower)\n    temp_env_approx_lev = get_temp_mod_parcel_lapse(p_lev, p_lower, temp_parcel_lev, lapse_diff_const)\n    temp_env_approx_upper = get_temp_mod_parcel_lapse(p_upper, p_lower, temp_parcel_upper, lapse_diff_const)\n    # temp_env_approx_upper = get_temp_mod_parcel_lapse(p_lev, temp_parcel_lev, temp_env_lower, p_lower,\n    #                                                   temp_parcel_upper, p_upper, lapse_diff_const)\n\n    # lapse_integral = integral_lapse_dlnp_hydrostatic(temp_env_lev, p_lev, p_lower, p_upper,\n    #                                                  temp_env_lower, temp_env_upper)\n    lapse_integral = g / R * np.log(temp_env_upper / temp_env_lower)\n    if sanity_check:\n        # sanity check, this should be the same as lapse_integral\n        lapse_integral_approx = integral_lapse_dlnp_hydrostatic(temp_env_approx_lev, p_lev, p_lower, p_upper,\n                                                                temp_env_lower, temp_env_upper)\n        print(\n            f'Actual lapse integral: {lapse_integral * 1000:.3f} K/km\\nApprox lapse integral: {lapse_integral_approx * 1000:.3f} K/km')\n        # Will use lapse rate such that approx value of T_upper is exact. Check that here\n        print(f'Actual temp_upper: {temp_env_upper:.3f} K\\nApprox temp_upper: {temp_env_approx_upper:.3f} K')\n\n    # Quantify error in approx of constant lapse rate by integral of absolute deviation between actual lapse rate\n    # and constant approx value\n    lapse_integral_diff_abs = integral_lapse_dlnp_hydrostatic(temp_env_lev, p_lev, p_lower, p_upper,\n                                                              temp_env_lower, temp_env_upper, temp_env_approx_lev,\n                                                              temp_env_lower, temp_env_approx_upper, take_abs=True)\n    return lapse_diff_const * 1000, lapse_integral * 1000, lapse_integral_diff_abs * 1000, temp_env_approx_lev\n</code></pre>"},{"location":"code/thesis/lapse_theory/","title":"Lapse Theory","text":""},{"location":"code/thesis/lapse_theory/#isca_tools.thesis.lapse_theory.get_bulk_lapse_rate","title":"<code>get_bulk_lapse_rate(temp1, temp2, p1, p2)</code>","text":"<p>Compute the bulk environmental lapse rate, \\(\\Gamma\\), between pressure <code>p1</code> at environmental temperature <code>temp1</code> and <code>p2</code> at environmental temperature <code>temp2</code>:</p> \\[\\Gamma = \\frac{g}{R}\\ln\\left(\\frac{T_1}{T_2}\\right)/\\ln\\left(\\frac{p_1}{p_2}\\right)\\] <p>This equation assumes hydrostatic equilibrium, ideal gas equation of state and that \\(\\Gamma\\) is constant between <code>p1</code> and <code>p2</code>.</p> <p>Parameters:</p> Name Type Description Default <code>temp1</code> <code>DataArray</code> <p>Temperature at pressure <code>p1</code>. Units: K.</p> required <code>temp2</code> <code>DataArray</code> <p>Temperature at pressure <code>p2</code>. Units: K.</p> required <code>p1</code> <code>Union[DataArray, float]</code> <p>Pressure at environmental temperature <code>temp1</code>. Units: Pa.</p> required <code>p2</code> <code>Union[DataArray, float]</code> <p>Pressure at environmental temperature <code>temp2</code>. Units: Pa.</p> required <p>Returns:</p> Type Description <p>Bulk environmental lapse rate, positive if <code>temp2&lt;temp1</code> and <code>p2&lt;p1</code>. Units are K/m.</p> Source code in <code>isca_tools/thesis/lapse_theory.py</code> <pre><code>def get_bulk_lapse_rate(temp1: xr.DataArray, temp2: xr.DataArray, p1: Union[xr.DataArray, float],\n                        p2: Union[xr.DataArray, float]):\n    \"\"\"\n    Compute the bulk environmental lapse rate, $\\Gamma$, between pressure `p1` at environmental temperature `temp1`\n    and `p2` at environmental temperature `temp2`:\n\n    $$\\Gamma = \\\\frac{g}{R}\\ln\\\\left(\\\\frac{T_1}{T_2}\\\\right)/\\ln\\\\left(\\\\frac{p_1}{p_2}\\\\right)$$\n\n    This equation assumes hydrostatic equilibrium, ideal gas equation of state and that $\\Gamma$ is constant\n    between `p1` and `p2`.\n\n    Args:\n        temp1: Temperature at pressure `p1`. Units: *K*.\n        temp2: Temperature at pressure `p2`. Units: *K*.\n        p1: Pressure at environmental temperature `temp1`. Units: *Pa*.\n        p2: Pressure at environmental temperature `temp2`. Units: *Pa*.\n\n    Returns:\n        Bulk environmental lapse rate, positive if `temp2&lt;temp1` and `p2&lt;p1`. Units are *K/m*.\n    \"\"\"\n    return g/R * np.log(temp1/temp2) / np.log(p1/p2)\n</code></pre>"},{"location":"code/thesis/lapse_theory/#isca_tools.thesis.lapse_theory.get_ds_in_pressure_range","title":"<code>get_ds_in_pressure_range(ds, pressure_min, pressure_max, n_pressure=20, pressure_var_name='P', method='log', lev_dim='lev', pressure_dim_name_out='plev_ind')</code>","text":"<p>Extracts dataset variables interpolated (or sampled) at multiple evenly spaced pressure levels between <code>pressure_min</code> and <code>pressure_max</code> for each point.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Input dataset containing at least a pressure variable (e.g. 'P') and one or more other variables dependent on pressure. Expected dims: (..., lev)</p> required <code>pressure_min</code> <code>DataArray</code> <p>Lower pressure bound for range [Pa]. Shape: same as all non-'lev' dims of ds.</p> required <code>pressure_max</code> <code>DataArray</code> <p>Upper pressure bound for range [Pa]. Shape: same as all non-'lev' dims of ds.</p> required <code>n_pressure</code> <code>int</code> <p>Number of evenly spaced pressure levels to sample between pressure_min and pressure_max.</p> <code>20</code> <code>pressure_var_name</code> <code>str</code> <p>Name of pressure variable in <code>ds</code>.</p> <code>'P'</code> <code>method</code> <code>str</code> <p>Method of interpolation either take log10 of pressure first or leave as raw values.</p> <code>'log'</code> <code>lev_dim</code> <code>str</code> <p>Name of model level dimension in <code>pressure_var_name</code>.</p> <code>'lev'</code> <code>pressure_dim_name_out</code> <code>str</code> <p>Name for the new pressure dimension in the output dataset. The out dimension with this name will have the value <code>np.arange(n_pressure)</code>.</p> <code>'plev_ind'</code> <p>Returns:</p> Name Type Description <code>ds_out</code> <code>Dataset</code> <p>Dataset sampled at <code>n_pressure</code> intermediate pressure levels between <code>pressure_min</code> and <code>pressure_max</code>, concatenated along a new dimension named <code>pressure_dim_name_out</code>. Output dims: (..., plev_ind)</p> Source code in <code>isca_tools/thesis/lapse_theory.py</code> <pre><code>def get_ds_in_pressure_range(ds: xr.Dataset, pressure_min: xr.DataArray,\n                             pressure_max: xr.DataArray, n_pressure: int = 20, pressure_var_name: str = 'P',\n                             method: str = 'log', lev_dim: str = 'lev',\n                             pressure_dim_name_out: str = 'plev_ind') -&gt; xr.Dataset:\n    \"\"\"\n    Extracts dataset variables interpolated (or sampled) at multiple evenly spaced\n    pressure levels between `pressure_min` and `pressure_max` for each point.\n\n    Args:\n        ds: Input dataset containing at least a pressure variable (e.g. 'P')\n            and one or more other variables dependent on pressure.\n            Expected dims: (..., lev)\n        pressure_min: Lower pressure bound for range [Pa].&lt;/br&gt;\n            Shape: same as all non-'lev' dims of ds.\n        pressure_max: Upper pressure bound for range [Pa].&lt;/br&gt;\n            Shape: same as all non-'lev' dims of ds.\n        n_pressure: Number of evenly spaced pressure levels to sample between\n            pressure_min and pressure_max.\n        pressure_var_name: Name of pressure variable in `ds`.\n        method: Method of interpolation either take log10 of pressure first or leave as raw values.\n        lev_dim: Name of model level dimension in `pressure_var_name`.\n        pressure_dim_name_out: Name for the new pressure dimension in the output dataset.&lt;/br&gt;\n            The out dimension with this name will have the value `np.arange(n_pressure)`.\n\n    Returns:\n        ds_out: Dataset sampled at `n_pressure` intermediate pressure levels between\n            `pressure_min` and `pressure_max`, concatenated along a new dimension\n            named `pressure_dim_name_out`.&lt;/br&gt;\n            Output dims: (..., plev_ind)\n    \"\"\"\n    if pressure_var_name not in ds.data_vars:\n        raise ValueError(f'Pressure ({pressure_var_name}) not in dataset.')\n    if len(ds.data_vars) == 1:\n        raise ValueError(f'Must have another variable other than {pressure_var_name} in dataset.')\n    ds_out = []\n    pressure_range = pressure_max - pressure_min\n    for i in range(n_pressure):\n        p_use = pressure_min + i / np.clip(n_pressure - 1, 1, np.inf) * pressure_range\n        ds_use = get_var_at_plev(ds.drop_vars(pressure_var_name), ds[pressure_var_name], p_use, method=method,\n                                 lev_dim=lev_dim)\n        ds_use[pressure_var_name] = p_use\n        ds_out.append(ds_use)\n    return xr.concat(ds_out,\n                     dim=xr.DataArray(np.arange(n_pressure), name=pressure_dim_name_out, dims=pressure_dim_name_out))\n</code></pre>"},{"location":"code/thesis/lapse_theory/#isca_tools.thesis.lapse_theory.get_var_at_plev","title":"<code>get_var_at_plev(var_env, p_env, p_desired, method='log', lev_dim='lev')</code>","text":"<p>Find the value of <code>var_env</code> at pressure <code>p_desired</code>.</p> <p>Similar to <code>interp_hybrid_to_pressure</code> but handles the case where want different <code>p_desired</code> at each latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>var_env</code> <code>Union[Dataset, DataArray]</code> <p>float <code>n_lat x n_lon x n_lev x ...</code> Variable to find value of at <code>p_desired</code>.</p> required <code>p_env</code> <code>DataArray</code> <p>float <code>n_lat x n_lon x n_lev x ...</code> Pressure levels corresponding to <code>var_env</code>.</p> required <code>p_desired</code> <code>DataArray</code> <p>float <code>n_lat x n_lon x ...</code> Pressure levels to find <code>var_env</code> at for each coordinate.</p> required <code>method</code> <code>str</code> <p>Method of interpolation either take log10 of pressure first or leave as raw values.</p> <code>'log'</code> <code>lev_dim</code> <code>str</code> <p>String that is the name of level dimension in <code>var_env</code> and <code>p_env</code>.</p> <code>'lev'</code> <p>Returns:</p> Name Type Description <code>var_desired</code> <p>float <code>n_lat x n_lon x ...</code> The value of <code>var_env</code> at <code>p_desired</code>.</p> Source code in <code>isca_tools/thesis/lapse_theory.py</code> <pre><code>def get_var_at_plev(var_env: Union[xr.Dataset, xr.DataArray], p_env: xr.DataArray, p_desired: xr.DataArray, method: str ='log',\n                    lev_dim: str ='lev'):\n    \"\"\"\n    Find the value of `var_env` at pressure `p_desired`.\n\n    Similar to `interp_hybrid_to_pressure` but handles the case where want different `p_desired` at each\n    latitude and longitude.\n\n\n    Args:\n        var_env: float `n_lat x n_lon x n_lev x ...`&lt;/br&gt;\n            Variable to find value of at `p_desired`.\n        p_env: float `n_lat x n_lon x n_lev x ...`&lt;/br&gt;\n            Pressure levels corresponding to `var_env`.\n        p_desired: float `n_lat x n_lon x ...`&lt;/br&gt;\n            Pressure levels to find `var_env` at for each coordinate.\n        method: Method of interpolation either take log10 of pressure first or leave as raw values.\n        lev_dim: String that is the name of level dimension in `var_env` and `p_env`.\n\n    Returns:\n        var_desired: float `n_lat x n_lon x ...`&lt;/br&gt;\n            The value of `var_env` at `p_desired`.\n    \"\"\"\n    if not (p_env.diff(dim=lev_dim) &gt; 0).all():\n        # If pressure is not ascending, flip dimension along lev_dim\n        # Requirement for np.interp\n        print(f'Reversed order of {lev_dim} for interpolation so p_env is ascending')\n        lev_dim_ascending = bool((p_env[lev_dim].diff(dim=lev_dim)&gt;0).all())\n        p_env = p_env.sortby(lev_dim, ascending=not lev_dim_ascending)\n        var_env = var_env.sortby(lev_dim, ascending=not lev_dim_ascending)\n        if not (p_env.diff(dim=lev_dim) &gt; 0).all():\n            # Sanity check p_env is now ascending\n            raise ValueError('Pressure variable not ascending')\n\n    out = xr.apply_ufunc(\n        _get_var_at_plev,\n        var_env, p_env, p_desired,\n        input_core_dims=[[lev_dim], [lev_dim], []],\n        output_core_dims=[[]],\n        vectorize=True,\n        dask=\"parallelized\",\n        output_dtypes=[float],\n        kwargs={\"method\": method}\n    )\n    return out\n</code></pre>"},{"location":"code/thesis/lapse_theory/#isca_tools.thesis.lapse_theory.interp_var_at_pressure","title":"<code>interp_var_at_pressure(var, p_desired, p_surf, hyam, hybm, p0, plev_step=1000, extrapolate=False, lev_dim='lev', var_name='new_var')</code>","text":"<p>Function to get the value of variable <code>var</code> at the pressure <code>p_desired</code>, where <code>p_desired</code> is expected to be a different value at each lat and lon.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Union[DataArray, Dataset, ndarray]</code> <p>Variable to do interpolation of. Should have <code>lev</code> dimension as well as lat, lon and possibly time. If give Dataset, will do interpolation on all variables.</p> required <code>p_desired</code> <code>Union[DataArray, ndarray]</code> <p>Desired pressure to find <code>var</code> at. Should have same dimension as <code>var</code> but no <code>lev</code>. Units: Pa.</p> required <code>p_surf</code> <code>Union[DataArray, ndarray]</code> <p>Surface pressure. Should have same dimension as <code>var</code> but no <code>lev</code>. Units: Pa.</p> required <code>hyam</code> <code>DataArray</code> <p>Hybrid a coefficients. Should have dimension of <code>lev</code> only.</p> required <code>hybm</code> <code>DataArray</code> <p>Hybrid b coefficients. Should have dimension of <code>lev</code> only.</p> required <code>p0</code> <code>float</code> <p>Reference pressure. Units: Pa.</p> required <code>plev_step</code> <code>float</code> <p>Will find var at value closest to <code>p_desired</code> on pressure grid with this spacing, so sets accuracy of interpolation.</p> <code>1000</code> <code>extrapolate</code> <code>bool</code> <p>If True, below ground extrapolation for variable will be done, otherwise will return nan.</p> <code>False</code> <code>lev_dim</code> <code>str</code> <p>String that is the name of level dimension in input data.</p> <code>'lev'</code> <code>var_name</code> <code>str</code> <p>String that is the name of variable in input data. Only used if <code>var</code> is numpy array</p> <code>'new_var'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset with <code>plev</code> indicating approximate value of <code>p_desired</code> used, as well as <code>var</code> interpolated to that pressure level.</p> Source code in <code>isca_tools/thesis/lapse_theory.py</code> <pre><code>def interp_var_at_pressure(var: Union[xr.DataArray, xr.Dataset, np.ndarray], p_desired: Union[xr.DataArray, np.ndarray],\n                           p_surf: Union[xr.DataArray, np.ndarray],\n                           hyam: xr.DataArray, hybm: xr.DataArray, p0: float,\n                           plev_step: float = 1000, extrapolate: bool = False,\n                           lev_dim: str = 'lev', var_name: str = 'new_var') -&gt; xr.Dataset:\n    \"\"\"\n    Function to get the value of variable `var` at the pressure `p_desired`, where `p_desired` is expected to\n    be a different value at each lat and lon.\n\n    Args:\n        var: Variable to do interpolation of. Should have `lev` dimension as well as lat, lon and possibly time.\n            If give Dataset, will do interpolation on all variables.\n        p_desired: Desired pressure to find `var` at.\n            Should have same dimension as `var` but no `lev`. Units: *Pa*.\n        p_surf: Surface pressure.\n            Should have same dimension as `var` but no `lev`. Units: *Pa*.\n        hyam: Hybrid a coefficients. Should have dimension of `lev` only.\n        hybm: Hybrid b coefficients. Should have dimension of `lev` only.\n        p0: Reference pressure. Units: *Pa*.\n        plev_step: Will find var at value closest to `p_desired` on pressure grid with this spacing,\n            so sets accuracy of interpolation.\n        extrapolate: If True, below ground extrapolation for variable will be done, otherwise will return nan.\n        lev_dim: String that is the name of level dimension in input data.\n        var_name: String that is the name of variable in input data. Only used if `var` is numpy array\n\n    Returns:\n        Dataset with `plev` indicating approximate value of `p_desired` used, as well as `var` interpolated\n            to that pressure level.\n    \"\"\"\n    plevs = np.arange(round_any(float(p_desired.min()), plev_step, 'floor'),\n                      round_any(float(p_desired.max()), plev_step, 'ceil')+plev_step/2, plev_step)\n    plevs_expand = xr.DataArray(plevs, dims=[\"plev\"], coords={\"plev\": np.arange(len(plevs))})\n    # Expand to match dimensions in p_surf, preserving order\n    if isinstance(var, np.ndarray) and var.size==hybm.size:\n        # If just numpy array, need to make it a data array for it to work\n        var = xr.DataArray(var, dims=hybm.dims, coords=hybm.coords, name=var_name)\n    if isinstance(p_surf, xr.DataArray):\n        for dim in p_surf.dims:\n            plevs_expand = plevs_expand.expand_dims({dim: p_surf.coords[dim]})\n\n    idx_lcl_closest = np.abs(plevs_expand - p_desired).argmin(dim='plev')\n    var_out = {'plev': plevs_expand.isel(plev=idx_lcl_closest)}     # approx pressure of p_desired used\n\n    # Note that with extrapolate, will obtain values lower than surface\n    if isinstance(var, xr.DataArray):\n        var_out[var.name] = interp_hybrid_to_pressure(data=var, ps=p_surf, hyam=hyam, hybm=hybm, p0=p0,\n                                                      new_levels=plevs, extrapolate=extrapolate, lev_dim=lev_dim,\n                                                      variable='other' if extrapolate else None).isel(plev=idx_lcl_closest)\n    elif isinstance(var, xr.Dataset):\n        for key in var:\n            var_out[key] = interp_hybrid_to_pressure(data=var[key], ps=p_surf, hyam=hyam, hybm=hybm, p0=p0,\n                                                     new_levels=plevs, extrapolate=extrapolate, lev_dim=lev_dim,\n                                                     variable='other' if extrapolate else None).isel(plev=idx_lcl_closest)\n    else:\n        raise ValueError('Unrecognized var. Needs to be a xr.DataArray or xr.Dataset.')\n    for key in var_out:\n        # Drop dimension of plev in all variables\n        var_out[key] = var_out[key].drop_vars('plev')\n    return xr.Dataset(var_out)\n</code></pre>"},{"location":"code/thesis/lapse_theory/#isca_tools.thesis.lapse_theory.reconstruct_temp","title":"<code>reconstruct_temp(temp3, p1, p2, p3, lapse_12, lapse_23)</code>","text":"<p>The temperature, \\(T_1\\), at \\(p_1\\) can be reconstructed from the lapse rate, \\(\\Gamma_{12}\\), between \\(p_1\\) and \\(p_2\\); the lapse rate \\(\\Gamma_{23}\\), between \\(p_2\\) and \\(p_3\\); and the temperature at \\(p_3\\), \\(T_3\\):</p> \\[ T_1 = T_{3}\\left((\\frac{p_2}{p_1})^{\\Gamma_{23}-\\Gamma_{12}}(\\frac{p_3}{p_1})^{-\\Gamma_{23}}\\right)^{R/g} \\] <p>Parameters:</p> Name Type Description Default <code>temp3</code> <code>DataArray</code> <p>Temperature at pressure <code>p3</code>. Units: K.</p> required <code>p1</code> <code>Union[DataArray, float]</code> <p>Pressure at level to reconstruct <code>temp1</code>. Units: Pa.</p> required <code>p2</code> <code>Union[DataArray, float]</code> <p>Pressure at environmental temperature <code>temp2</code>. Units: Pa.</p> required <code>p3</code> <code>Union[DataArray, float]</code> <p>Pressure at environmental temperature <code>temp3</code>. Units: Pa.</p> required <code>lapse_12</code> <code>DataArray</code> <p>Bulk environmental lapse rate between <code>p1</code> and <code>p2</code>. Units are K/m.</p> required <code>lapse_23</code> <code>DataArray</code> <p>Bulk environmental lapse rate between <code>p2</code> and <code>p3</code>. Units are K/m.</p> required <p>Returns:</p> Name Type Description <code>temp1</code> <p>Temperature at pressure <code>p1</code>. Units: K.</p> Source code in <code>isca_tools/thesis/lapse_theory.py</code> <pre><code>def reconstruct_temp(temp3: xr.DataArray, p1: Union[xr.DataArray, float], p2: Union[xr.DataArray, float],\n                     p3: Union[xr.DataArray, float],\n                     lapse_12: xr.DataArray, lapse_23: xr.DataArray):\n    \"\"\"\n    The temperature, $T_1$, at $p_1$ can be reconstructed from the lapse rate, $\\Gamma_{12}$, between $p_1$ and $p_2$;\n    the lapse rate $\\Gamma_{23}$, between $p_2$ and $p_3$; and the temperature at $p_3$, $T_3$:\n\n    $$\n    T_1 = T_{3}\\\\left((\\\\frac{p_2}{p_1})^{\\Gamma_{23}-\\Gamma_{12}}(\\\\frac{p_3}{p_1})^{-\\Gamma_{23}}\\\\right)^{R/g}\n    $$\n\n    Args:\n        temp3: Temperature at pressure `p3`. Units: *K*.\n        p1: Pressure at level to reconstruct `temp1`. Units: *Pa*.\n        p2: Pressure at environmental temperature `temp2`. Units: *Pa*.\n        p3: Pressure at environmental temperature `temp3`. Units: *Pa*.\n        lapse_12: Bulk environmental lapse rate between `p1` and `p2`. Units are *K/m*.\n        lapse_23: Bulk environmental lapse rate between `p2` and `p3`. Units are *K/m*.\n\n    Returns:\n        temp1: Temperature at pressure `p1`. Units: *K*.\n    \"\"\"\n    sigma_12 = p2 / p1     # if p1 is surface, this should be &lt;1\n    sigma_13 = p3 / p1\n    return temp3 * (sigma_12**(lapse_23-lapse_12) * sigma_13**(-lapse_23))**(R/g)\n</code></pre>"},{"location":"code/thesis/profile_fitting/","title":"Profile Fitting","text":""},{"location":"code/thesis/profile_fitting/#isca_tools.thesis.profile_fitting.get_lnb_lev_ind","title":"<code>get_lnb_lev_ind(temp_env, z_env, p_env, p_max=400 * 100, lapse_thresh=5, lapse_change_thresh=2, n_iter=5, lev_dim='lev')</code>","text":"<p>Finds the index of level of neutral buoyancy in dimension <code>lev_dim</code> that satisfies the following conditions (basically so no stratospheric influence between LNB and surface):</p> <ul> <li>LNB must be a pressure lower than <code>p_max</code>.</li> <li>LNB is level above which is the first layer with negative lapse rate.</li> <li>If lapse rate in level immediately below this deviates from the lapse rate in the level below that or two below that by more than <code>lapse_change_thresh</code>, then LNB is moved to a lower level by 1. This process is repeated <code>n_iter</code> times.</li> <li>Lapse rate in level immediately below LNB must be less than <code>lapse_thresh</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>temp_env</code> <code>DataArray</code> <p>Environmental temperature profile in Kelvin.</p> required <code>z_env</code> <code>DataArray</code> <p>Environment geopotential height profile in m.</p> required <code>p_env</code> <code>DataArray</code> <p>Environment pressure profile in Pa (assumed different for each location i.e. same dimensions as <code>temp_env</code> and <code>z_env</code>).</p> required <code>p_max</code> <code>float</code> <p>Pressure of LNB cannot exceed this value (i.e. further from surface than this).</p> <code>400 * 100</code> <code>lapse_thresh</code> <code>float</code> <p>Lapse rate in level immediately below LNB must be less than <code>lapse_thresh</code>.</p> <code>5</code> <code>lapse_change_thresh</code> <code>float</code> <p>If lapse rate in level immediately below LNB deviates from the lapse rate in the level below that or two below that by more than this, then LNB is moved to a lower level by 1.</p> <code>2</code> <code>n_iter</code> <code>int</code> <p>Number of iterations to run <code>lapse_change_thresh</code> process.</p> <code>5</code> <code>lev_dim</code> <code>str</code> <p>Name of dimension for vertical model level.</p> <code>'lev'</code> <p>Returns:</p> Name Type Description <code>lnb_ind</code> <code>DataArray</code> <p>Index of level of neutral buoyancy in dimension <code>lev_dim</code> for each location.</p> Source code in <code>isca_tools/thesis/profile_fitting.py</code> <pre><code>def get_lnb_lev_ind(temp_env: xr.DataArray, z_env: xr.DataArray, p_env: xr.DataArray, p_max: float = 400 * 100,\n                    lapse_thresh: float = 5, lapse_change_thresh: float = 2, n_iter: int = 5,\n                    lev_dim: str = 'lev') -&gt; xr.DataArray:\n    \"\"\"\n    Finds the index of level of neutral buoyancy in dimension `lev_dim` that satisfies the following conditions\n    (basically so no stratospheric influence between LNB and surface):\n\n    * LNB must be a pressure lower than `p_max`.\n    * LNB is level above which is the first layer with negative lapse rate.\n    * If lapse rate in level immediately below this deviates from the lapse rate in the level below that\n    or two below that by more than `lapse_change_thresh`, then LNB is moved to a lower level by 1.\n    This process is repeated `n_iter` times.\n    * Lapse rate in level immediately below LNB must be less than `lapse_thresh`.\n\n    Args:\n        temp_env: Environmental temperature profile in Kelvin.\n        z_env: Environment geopotential height profile in m.\n        p_env: Environment pressure profile in Pa\n            (assumed different for each location i.e. same dimensions as `temp_env` and `z_env`).\n        p_max: Pressure of LNB cannot exceed this value (i.e. further from surface than this).\n        lapse_thresh: Lapse rate in level immediately below LNB must be less than `lapse_thresh`.\n        lapse_change_thresh: If lapse rate in level immediately below LNB deviates from the lapse rate\n            in the level below that or two below that by more than this,\n            then LNB is moved to a lower level by 1.\n        n_iter: Number of iterations to run `lapse_change_thresh` process.\n        lev_dim: Name of dimension for vertical model level.\n\n    Returns:\n        lnb_ind: Index of level of neutral buoyancy in dimension `lev_dim` for each location.\n    \"\"\"\n    lapse = -temp_env.diff(dim=lev_dim, label='lower') / z_env.diff(dim=lev_dim, label='lower') * 1000\n    lapse = lapse.reindex_like(temp_env)  # make same shape\n    lapse = lapse.fillna(lapse_dry * 1000)  # ensure final value satisfies lapse criteria\n    lapse = lapse.where(p_env &lt; p_max)\n    mask = lapse &lt; 0\n    lnb_ind = (mask.where(mask, other=np.nan) * np.arange(lapse.lev.size)).max(dim=lev_dim).astype(int)\n    # lnb_ind = np.where(lapse &lt; 0)[0][-1]\n    # If lapse rate has very big variation, push LNB closer to surface\n    for j in range(n_iter):\n        is_large_lapse_diff = lapse.isel(**{lev_dim: lnb_ind + 2}) - lapse.isel(\n            **{lev_dim: lnb_ind + 1}) &gt; lapse_change_thresh\n        is_large_lapse_diff = is_large_lapse_diff &amp; (lapse.isel(**{lev_dim: lnb_ind + 1}) &lt; lapse_thresh)\n        is_large_lapse_diff2 = lapse.isel(**{lev_dim: lnb_ind + 3}) - lapse.isel(\n            **{lev_dim: lnb_ind + 1}) &gt; lapse_change_thresh\n        is_large_lapse_diff2 = is_large_lapse_diff2 &amp; (lapse.isel(**{lev_dim: lnb_ind + 1}) &lt; lapse_thresh)\n        is_large_lapse_diff = is_large_lapse_diff | is_large_lapse_diff2\n        lnb_ind = lnb_ind + is_large_lapse_diff.astype(int)\n    lnb_ind = lnb_ind + 1  # make it up to and including this level\n    return lnb_ind\n</code></pre>"},{"location":"code/thesis/profile_fitting/#isca_tools.thesis.profile_fitting.get_mse_env","title":"<code>get_mse_env(temp_env, p_env, z_env, p_lcl, prof_type='full', temp_at_lcl=None, sphum_below_lcl=None)</code>","text":"<p>Returns environmental MSE profile as a function of pressure \\(p\\), which satisfies the following if <code>prof_type = full</code>:</p> <ul> <li>Above LCL: \\(MSE_{env}(p) = MSE^*(p)\\)</li> <li>Below LCL: \\(MSE_{env}(p) = DSE(p) + L_v q^*_{LCL}\\)</li> </ul> <p>Where \\(q^*_{LCL}\\) is the saturation-specific humidity evaluated at \\(T_{env}(p_{lcl})\\). The two profiles are the same at the LCL.</p> <p>Parameters:</p> Name Type Description Default <code>temp_env</code> <code>DataArray</code> <p><code>[n_lev]</code> Environment temperature in Kelvin.</p> required <code>p_env</code> <code>DataArray</code> <p><code>[n_lev]</code> Environment pressure in Pa.</p> required <code>z_env</code> <code>DataArray</code> <p><code>[n_lev]</code> Environment geopotential height in m.</p> required <code>p_lcl</code> <code>DataArray</code> <p>Pressure of LCL in Pa.</p> required <code>prof_type</code> <code>Literal['full', 'above_lcl', 'below_lcl']</code> <p>If <code>above_lcl</code>, will return \\(MSE^*(p)\\) for all \\(p\\). If <code>below_lcl</code>, will return \\(DSE(p) + L_v q^*_{LCL}\\) for all \\(p\\). If <code>full</code> will return different profile above and below LCL.</p> <code>'full'</code> <code>temp_at_lcl</code> <code>Optional[DataArray]</code> <p>Environment temperature at <code>p_lcl</code> in Kelvin. Only required if <code>sphum_below_lcl</code> not given.</p> <code>None</code> <code>sphum_below_lcl</code> <code>Optional[DataArray]</code> <p>Specific humidity to use for profile below the LCL. If not given, will set to \\(q^*(T(p_{LCL}), p_{LCL})\\).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mse_env</code> <code>DataArray</code> <p><code>[n_lev]</code> Environment MSE in kJ/kg.</p> Source code in <code>isca_tools/thesis/profile_fitting.py</code> <pre><code>def get_mse_env(temp_env: xr.DataArray, p_env: xr.DataArray, z_env: xr.DataArray,\n                p_lcl: xr.DataArray,\n                prof_type: Literal['full', 'above_lcl', 'below_lcl'] = 'full',\n                temp_at_lcl: Optional[xr.DataArray] = None,\n                sphum_below_lcl: Optional[xr.DataArray] = None) -&gt; xr.DataArray:\n    \"\"\"\n    Returns environmental MSE profile as a function of pressure $p$, which satisfies the following if\n    `prof_type = full`:\n\n    * Above LCL: $MSE_{env}(p) = MSE^*(p)$\n    * Below LCL: $MSE_{env}(p) = DSE(p) + L_v q^*_{LCL}$\n\n    Where $q^*_{LCL}$ is the saturation-specific humidity evaluated at $T_{env}(p_{lcl})$.\n    The two profiles are the same at the LCL.\n\n    Args:\n        temp_env: `[n_lev]` Environment temperature in Kelvin.\n        p_env: `[n_lev]` Environment pressure in Pa.\n        z_env: `[n_lev]` Environment geopotential height in m.\n        p_lcl: Pressure of LCL in Pa.\n        prof_type: If `above_lcl`, will return $MSE^*(p)$ for all $p$.&lt;/br&gt;\n            If `below_lcl`, will return $DSE(p) + L_v q^*_{LCL}$ for all $p$.&lt;/br&gt;\n            If `full` will return different profile above and below LCL.\n        temp_at_lcl: Environment temperature at `p_lcl` in Kelvin. Only required if `sphum_below_lcl` not given.\n        sphum_below_lcl: Specific humidity to use for profile below the LCL.\n            If not given, will set to $q^*(T(p_{LCL}), p_{LCL})$.\n\n    Returns:\n        mse_env: `[n_lev]` Environment MSE in kJ/kg.\n    \"\"\"\n    if sphum_below_lcl is None:\n        sphum_below_lcl = sphum_sat(temp_at_lcl, p_lcl)\n    mse_above_lcl = moist_static_energy(temp_env, sphum_sat(temp_env, p_env), z_env)\n    mse_below_lcl = moist_static_energy(temp_env, sphum_below_lcl, z_env)\n    if prof_type == 'full':\n        return xr.where(p_env &lt;= p_lcl, mse_above_lcl, mse_below_lcl)\n    elif prof_type == 'above_lcl':\n        return mse_above_lcl\n    elif prof_type == 'below_lcl':\n        return mse_below_lcl\n    else:\n        raise ValueError('prof_type must be either full or above_lcl or below_lcl')\n</code></pre>"},{"location":"code/thesis/profile_fitting/#isca_tools.thesis.profile_fitting.get_mse_prof_rms","title":"<code>get_mse_prof_rms(temp_env, p_env, z_env, p_thickness, temp_at_split=None, p_split=None, z_at_split=None, lev_dim='lev', split_dim='lev')</code>","text":"<p>For each possible split level, will compute the RMS error of \\(MSE_{env} - MSE^*_{split}\\) where:</p> <ul> <li>Above Split: \\(MSE_{env}(p) = MSE^*(p)\\)</li> <li>Below Split: \\(MSE_{env}(p) = DSE(p) + L_v q^*_{split}\\)</li> </ul> <p>Idea being that LCL is split level with minimum RMS error.</p> <p>Parameters:</p> Name Type Description Default <code>temp_env</code> <code>DataArray</code> <p>Environmental temperature [K], dims (..., lev_dim)</p> required <code>p_env</code> <code>DataArray</code> <p>Environmental pressure [Pa], dims (..., lev_dim)</p> required <code>z_env</code> <code>DataArray</code> <p>Geopotential height [m], dims (..., lev_dim)</p> required <code>p_thickness</code> <code>DataArray</code> <p>Pressure thickness between levels [Pa], dims (..., lev_dim)</p> required <code>temp_at_split</code> <code>Optional[DataArray]</code> <p>Temperature to use as <code>temp_at_lcl</code> in <code>get_mse_env</code>, dims (..., split_dim). If <code>None</code>, sets to <code>temp_env</code>.</p> <code>None</code> <code>p_split</code> <code>Optional[DataArray]</code> <p>Pressure to use as <code>p_lcl</code> in <code>get_mse_env</code>, dims (..., split_dim). If <code>None</code>, sets to <code>p_env</code>.</p> <code>None</code> <code>z_at_split</code> <code>Optional[ndarray]</code> <p>Geopotential height corresponding to <code>p_split</code>, dims (..., split_dim). If <code>None</code>, sets to <code>z_env</code>.</p> <code>None</code> <code>lev_dim</code> <code>str</code> <p>Model level dimension in <code>temp_env</code>, <code>p_env</code>, <code>z_env</code>, and <code>p_thickness</code>.</p> <code>'lev'</code> <code>split_dim</code> <code>str</code> <p>Dimension corresponding to different split levels in <code>temp_at_split</code> and <code>p_split</code>. If <code>temp_at_split</code> is <code>None</code>, will set to <code>lev_dim</code>.</p> <code>'lev'</code> <p>Returns:</p> Name Type Description <code>mse_prof_error</code> <code>DataArray</code> <p>Mass weighted RMS difference for each possible split level, dims (..., split_dim)</p> Source code in <code>isca_tools/thesis/profile_fitting.py</code> <pre><code>def get_mse_prof_rms(temp_env: xr.DataArray, p_env: xr.DataArray, z_env: xr.DataArray,\n                     p_thickness: xr.DataArray, temp_at_split: Optional[xr.DataArray] = None,\n                     p_split: Optional[xr.DataArray] = None, z_at_split: Optional[np.ndarray] = None,\n                     lev_dim: str = 'lev',\n                     split_dim: str = 'lev') -&gt; xr.DataArray:\n    \"\"\"\n    For each possible split level, will compute the RMS error of $MSE_{env} - MSE^*_{split}$ where:\n\n    * Above Split: $MSE_{env}(p) = MSE^*(p)$\n    * Below Split: $MSE_{env}(p) = DSE(p) + L_v q^*_{split}$\n\n    Idea being that LCL is split level with minimum RMS error.\n\n    Args:\n        temp_env: Environmental temperature [K], dims (..., lev_dim)\n        p_env: Environmental pressure [Pa], dims (..., lev_dim)\n        z_env: Geopotential height [m], dims (..., lev_dim)\n        p_thickness: Pressure thickness between levels [Pa], dims (..., lev_dim)\n        temp_at_split: Temperature to use as `temp_at_lcl` in `get_mse_env`, dims (..., split_dim).&lt;/br&gt;\n            If `None`, sets to `temp_env`.\n        p_split: Pressure to use as `p_lcl` in `get_mse_env`, dims (..., split_dim).&lt;/br&gt;\n            If `None`, sets to `p_env`.\n        z_at_split: Geopotential height corresponding to `p_split`, dims (..., split_dim).&lt;/br&gt;\n            If `None`, sets to `z_env`.\n        lev_dim: Model level dimension in `temp_env`, `p_env`, `z_env`, and `p_thickness`.\n        split_dim: Dimension corresponding to different split levels in `temp_at_split` and `p_split`.&lt;/br&gt;\n            If `temp_at_split` is `None`, will set to `lev_dim`.\n\n    Returns:\n        mse_prof_error: Mass weighted RMS difference for each possible split level, dims (..., split_dim)\n    \"\"\"\n    if (temp_at_split is None) and (p_split is None) and (z_at_split is None):\n        temp_at_split = temp_env\n        p_split = p_env\n        z_at_split = z_env\n        split_dim = lev_dim\n    elif (temp_at_split is None) or (p_split is None) or (z_at_split is None):\n        raise ValueError('Either all or none of temp_at_split, p_split, and z_at_split must be specified.')\n\n    def _core(temp_env, p_env, z_env, p_thickness, temp_at_split, p_split, z_at_split):\n        # temp_env, p_env, z_env, p_thickness: (lev,)\n        norm = []\n        for i in range(temp_at_split.shape[0]):\n            if np.isnan(temp_at_split[i]):\n                norm.append(np.nan)\n                continue\n            var = get_mse_env(temp_env, p_env, z_env,\n                              temp_at_split[i], p_split[i], 'full')\n            var = var - moist_static_energy(temp_at_split[i],\n                                            sphum_sat(temp_at_split[i], p_split[i]),\n                                            z_at_split[i])\n\n            # Set MSE above LCL to mass weighted mean in this layer, not equal to LCL MSE^* when computing optimal\n            # LCL. Because idea is MSE should be constant above, and DSE should be constant below\n            # var[p_env &lt; p_split[i]] = smooth_threshold(var[p_env &lt; p_split[i]], x_thresh=1)\n            # var[p_env &lt; p_split[i]] -= np.sum((var * p_thickness)[p_env &lt; p_split[i]]/g) / np.sum(p_thickness[p_env &lt; p_split[i]]/g)\n            # var[p_env &gt; p_split[i]] -= np.sum((var * p_thickness)[p_env &gt; p_split[i]] / g) / np.sum(\n            #     p_thickness[p_env &gt; p_split[i]] / g)\n            # weight with p_thickness/g across lev\n            # var = np.clip(var, -10,10)  # don't allow extremely large error from any one level\n            norm.append(weighted_RMS(var, p_thickness / g))\n        return np.array(norm)\n\n    # Apply across non-lev dims\n    norm = xr.apply_ufunc(\n        _core,\n        temp_env,\n        p_env,\n        z_env,\n        p_thickness,\n        temp_at_split,\n        p_split,\n        z_at_split,\n        input_core_dims=[[lev_dim], [lev_dim], [lev_dim], [lev_dim], [split_dim], [split_dim], [split_dim]],\n        output_core_dims=[[split_dim]],\n        vectorize=True,\n        dask=\"parallelized\",\n        output_dtypes=[float],\n    )\n\n    # Attach coordinates (pressure levels as p_lcl)\n    norm = norm.assign_coords({split_dim: (split_dim, p_split[split_dim].values)})\n    norm.name = \"mse_prof_error\"\n    return norm\n</code></pre>"},{"location":"code/thesis/profile_fitting/#isca_tools.thesis.profile_fitting.get_p_from_pnorm","title":"<code>get_p_from_pnorm(pnorm, p_low, p_high)</code>","text":"<p>Given the normalized pressure coordinate <code>pnorm</code>, this inverts <code>get_pnorm</code> to give the physical pressure in Pa.</p> <p>Parameters:</p> Name Type Description Default <code>pnorm</code> <code>Union[DataArray, ndarray, float]</code> <p>Normalized pressure coordinate (dimensionless).</p> required <code>p_low</code> <code>Union[DataArray, ndarray, float]</code> <p>Low level pressure used to compute <code>pnorm</code> (closer to surface) in Pa.</p> required <code>p_high</code> <code>Union[DataArray, ndarray, float]</code> <p>High level pressure used to compute <code>pnorm</code> (further from surface so \\(p_{high} &lt; p_{low}\\)) in Pa.</p> required <p>Returns:</p> Name Type Description <code>p</code> <code>Union[DataArray, ndarray, float]</code> <p>Pressure corresponding to <code>pnorm</code> in Pa.</p> Source code in <code>isca_tools/thesis/profile_fitting.py</code> <pre><code>def get_p_from_pnorm(pnorm: Union[xr.DataArray, np.ndarray, float], p_low: Union[xr.DataArray, np.ndarray, float],\n                     p_high: Union[xr.DataArray, np.ndarray, float]) -&gt; Union[xr.DataArray, np.ndarray, float]:\n    \"\"\"\n    Given the normalized pressure coordinate `pnorm`, this inverts `get_pnorm` to give the physical pressure in Pa.\n\n    Args:\n        pnorm: Normalized pressure coordinate (dimensionless).\n        p_low: Low level pressure used to compute `pnorm` (closer to surface) in Pa.\n        p_high: High level pressure used to compute `pnorm` (further from surface so $p_{high} &lt; p_{low}$) in Pa.\n\n    Returns:\n        p: Pressure corresponding to `pnorm` in Pa.\n    \"\"\"\n    return 10 ** (pnorm * (np.log10(p_high) - np.log10(p_low)) + np.log10(p_low))\n</code></pre>"},{"location":"code/thesis/profile_fitting/#isca_tools.thesis.profile_fitting.get_pnorm","title":"<code>get_pnorm(p, p_low, p_high)</code>","text":"<p>Given the pressure, \\(p\\), this returns a normalized pressure coordinate going from 0 at \\(p_{low}\\) to 1 at \\(p_{high}\\):</p> \\[p_{norm} = \\frac{\\log_{10}p - \\log_{10}p_{low}}{\\log_{10}p_{high} - \\log_{10}p_{low}}\\] <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Union[DataArray, ndarray, float]</code> <p>Pressure in Pa.</p> required <code>p_low</code> <code>Union[DataArray, ndarray, float]</code> <p>Low level pressure (closer to surface) in Pa.</p> required <code>p_high</code> <code>Union[DataArray, ndarray, float]</code> <p>High level pressure (further from surface so \\(p_{high} &lt; p_{low}\\)) in Pa.</p> required <p>Returns:</p> Name Type Description <code>pnorm</code> <code>Union[DataArray, ndarray, float]</code> <p>Value of pressure \\(p\\) in normalized pressure coordinates between 0 and 1.</p> Source code in <code>isca_tools/thesis/profile_fitting.py</code> <pre><code>def get_pnorm(p: Union[xr.DataArray, np.ndarray, float], p_low: Union[xr.DataArray, np.ndarray, float],\n              p_high: Union[xr.DataArray, np.ndarray, float]) -&gt; Union[xr.DataArray, np.ndarray, float]:\n    \"\"\"\n    Given the pressure, $p$, this returns a normalized pressure coordinate going from 0 at $p_{low}$ to 1 at $p_{high}$:\n\n    $$p_{norm} = \\\\frac{\\log_{10}p - \\log_{10}p_{low}}{\\log_{10}p_{high} - \\log_{10}p_{low}}$$\n\n    Args:\n        p: Pressure in Pa.\n        p_low: Low level pressure (closer to surface) in Pa.\n        p_high: High level pressure (further from surface so $p_{high} &lt; p_{low}$) in Pa.\n\n    Returns:\n        pnorm: Value of pressure $p$ in normalized pressure coordinates between 0 and 1.\n    \"\"\"\n    return (np.log10(p) - np.log10(p_low)) / (np.log10(p_high) - np.log10(p_low))\n</code></pre>"},{"location":"code/thesis/profile_fitting/#isca_tools.thesis.profile_fitting.interp_var_to_pnorm","title":"<code>interp_var_to_pnorm(var, p, var_at_low, p_low, var_at_high, p_high, lnb_ind, d_pnorm=0.1, pnorm_custom_grid=None, extrapolate=True, insert_low=True, insert_high=True, lev_dim='lev', pnorm_dim_name='pnorm', subtract_var_at_low=True)</code>","text":"<p>Interpolate <code>var</code> as a function of pressure, \\(p\\), into \\(p_{norm}\\) that goes from 0 at \\(p_{low}\\) to 1 at \\(p_{high}\\) according to:</p> \\[p_{norm} = \\frac{\\log_{10}p - \\log_{10}p_{low}}{\\log_{10}p_{high} - \\log_{10}p_{low}}\\] <p>Parameters:</p> Name Type Description Default <code>var</code> <code>DataArray</code> <p><code>[n_lev]</code> Variable to interpolate.</p> required <code>p</code> <code>DataArray</code> <p><code>[n_lev]</code> Pressure in Pa.</p> required <code>var_at_low</code> <code>DataArray</code> <p>Value of <code>var</code> at <code>p_low</code>.</p> required <code>p_low</code> <code>Union[DataArray, float]</code> <p>Pressure level that will correspond to \\(p_{norm}=0\\).</p> required <code>var_at_high</code> <code>DataArray</code> <p>Value of <code>var</code> at <code>p_high</code>.</p> required <code>p_high</code> <code>Union[DataArray, float]</code> <p>Pressure level that will correspond to \\(p_{norm}=1\\).</p> required <code>lnb_ind</code> <code>DataArray</code> <p>The value of <code>var</code> at model levels further from the surface than this will be set to <code>nan</code> or extrapolated from below this level if <code>extrapolate=True</code>.</p> required <code>d_pnorm</code> <code>float</code> <p>Desired spacing in \\(p_{norm}\\) coordinates.</p> <code>0.1</code> <code>pnorm_custom_grid</code> <code>Optional[ndarray]</code> <p>Option to provide the \\(p_{norm}\\) coordinates. Will override the <code>d_pnorm</code>.</p> <code>None</code> <code>extrapolate</code> <code>bool</code> <p>Whether to extrapolate above LNB.</p> <code>True</code> <code>insert_low</code> <code>bool</code> <p>Whether to add <code>var_at_high</code> to <code>var</code> before interpolation.</p> <code>True</code> <code>insert_high</code> <code>bool</code> <p>Whether to add <code>var_at_low</code> to <code>var</code> before interpolation.</p> <code>True</code> <code>lev_dim</code> <code>str</code> <p>Name of dimension for vertical model level.</p> <code>'lev'</code> <code>pnorm_dim_name</code> <code>str</code> <p>Name of the output <code>pnorm</code> dimension.</p> <code>'pnorm'</code> <code>subtract_var_at_low</code> <code>bool</code> <p>If <code>True</code> will subtract <code>var_at_low</code> from the interpolated <code>var</code>.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>var</code> <code>DataArray</code> <p><code>[n_pnorm]</code> Value of <code>var</code> on the new $ p_{norm} $ grid.</p> <code>n_extrapolate</code> <code>DataArray</code> <p>Number of model levels used above LNB that were extrapolated. Only non-zero if <code>extrapolate</code> is True.</p> Source code in <code>isca_tools/thesis/profile_fitting.py</code> <pre><code>def interp_var_to_pnorm(var: xr.DataArray, p: xr.DataArray, var_at_low: xr.DataArray, p_low: Union[xr.DataArray, float],\n                        var_at_high: xr.DataArray, p_high: Union[xr.DataArray, float], lnb_ind: xr.DataArray,\n                        d_pnorm: float = 0.1, pnorm_custom_grid: Optional[np.ndarray] = None, extrapolate: bool = True,\n                        insert_low: bool = True, insert_high: bool = True,\n                        lev_dim: str = 'lev', pnorm_dim_name: str = 'pnorm',\n                        subtract_var_at_low: bool = True) -&gt; Tuple[xr.DataArray, xr.DataArray]:\n    \"\"\"\n    Interpolate `var` as a function of pressure, $p$, into $p_{norm}$ that goes from 0 at\n    $p_{low}$ to 1 at $p_{high}$ according to:\n\n    $$p_{norm} = \\\\frac{\\log_{10}p - \\log_{10}p_{low}}{\\log_{10}p_{high} - \\log_{10}p_{low}}$$\n\n    Args:\n        var: `[n_lev]` Variable to interpolate.\n        p: `[n_lev]` Pressure in Pa.\n        var_at_low: Value of `var` at `p_low`.\n        p_low: Pressure level that will correspond to $p_{norm}=0$.\n        var_at_high: Value of `var` at `p_high`.\n        p_high: Pressure level that will correspond to $p_{norm}=1$.\n        lnb_ind: The value of `var` at model levels further from the surface than this will be set to `nan`\n            or extrapolated from below this level if `extrapolate=True`.\n        d_pnorm: Desired spacing in $p_{norm}$ coordinates.\n        pnorm_custom_grid: Option to provide the $p_{norm}$ coordinates. Will override the `d_pnorm`.\n        extrapolate: Whether to extrapolate above LNB.\n        insert_low: Whether to add `var_at_high` to `var` before interpolation.\n        insert_high: Whether to add `var_at_low` to `var` before interpolation.\n        lev_dim: Name of dimension for vertical model level.\n        pnorm_dim_name: Name of the output `pnorm` dimension.\n        subtract_var_at_low: If `True` will subtract `var_at_low` from the interpolated `var`.\n\n    Returns:\n        var: `[n_pnorm]` Value of `var` on the new $ p_{norm} $ grid.\n        n_extrapolate: Number of model levels used above LNB that were extrapolated.\n            Only non-zero if `extrapolate` is True.\n    \"\"\"\n    small = 1  # a small value in units of Pa, much less than spacing of model levels\n    var = var.where(p &gt;= p.isel(**{lev_dim: lnb_ind}) - small)  # set var to nan above LNB\n    # Define target pnorm-grid\n    if pnorm_custom_grid is None:\n        pnorm = np.arange(0, 1 + d_pnorm, d_pnorm)\n    else:\n        pnorm = pnorm_custom_grid\n\n    def _interp_onecell(var_prof, p_prof, var_at_low, p_low, var_at_high, p_high):\n        # Skip missing\n        if np.all(np.isnan(var_prof)):\n            return np.full_like(pnorm, np.nan, dtype=float)\n\n        # Shift to pnorm grid\n        logp_target = np.log10(get_p_from_pnorm(pnorm, p_low, p_high))\n        # Add to dataset used to perform interpolation\n        if insert_low and (\n                np.min(np.abs(p_prof - p_low)) &gt; small):  # only insert if not already present even if request\n            ind_low = np.searchsorted(p_prof, p_low)\n            p_prof = np.insert(p_prof, ind_low, p_low)\n            var_prof = np.insert(var_prof, ind_low, var_at_low)\n        if insert_high and (np.min(np.abs(p_prof - p_high)) &gt; small):\n            ind_high = np.searchsorted(p_prof, p_high)\n            p_prof = np.insert(p_prof, ind_high, p_high)\n            var_prof = np.insert(var_prof, ind_high, var_at_high)\n\n        var_target = np.interp(logp_target, np.log10(p_prof), var_prof)\n        n_extrap = 0\n        if extrapolate:\n            var_target, extrap_ind = interp_nan(logp_target, var_target)\n            n_extrap = extrap_ind.size\n        if subtract_var_at_low:\n            var_target = var_target - var_at_low\n        return var_target, n_extrap\n\n    out = xr.apply_ufunc(\n        _interp_onecell,\n        var, p, var_at_low, p_low, var_at_high, p_high,\n        input_core_dims=[[lev_dim], [lev_dim], [], [], [], []],\n        output_core_dims=[[pnorm_dim_name], []],\n        vectorize=True,\n        dask=\"parallelized\",\n        output_dtypes=[float, int],\n        kwargs={}\n    )\n    out = list(out)\n    out[0] = out[0].assign_coords(**{pnorm_dim_name: pnorm})\n    return out[0], out[1]\n</code></pre>"},{"location":"code/thesis/surface_energy_budget/","title":"Surface Energy Budget","text":""},{"location":"code/thesis/surface_energy_budget/#isca_tools.thesis.surface_energy_budget.gamma_linear_approx","title":"<code>gamma_linear_approx(time, temp, lambda_const, lambda_time_lag=None, lambda_nl=None, temp_anom_nl=None)</code>","text":"<p>OUTDATED FUNCTION - USED FOR <code>get_temp_fourier</code> but now use <code>get_temp_fourier_numerical</code></p> <p>This approximates \\(\\Gamma^{\\uparrow} = LW^{\\uparrow} - LW^{\\downarrow} + LH^{\\uparrow} + SH^{\\uparrow}\\) as:</p> \\[\\Gamma^{\\uparrow} \\approx \\lambda_0 + \\sum_{i=1}^{N_{\\lambda}}\\lambda_i T(t-\\Lambda_i) + \\sum_{j=2}^{j_{max}}\\lambda_{nl_j}\\left(T'^{j}(t) - \\overline{T'^j}\\right)\\] <p>where:</p> <ul> <li>\\(T' = T(t) - \\overline{T}\\) is the surface temperature anomaly</li> <li>\\(LW^{\\uparrow}\\) is upward longwave radiation at the surface i.e. \\(\\sigma T^4\\) or <code>lwup_sfc</code> output by Isca.     Units: \\(Wm^{-2}\\).</li> <li>\\(LW^{\\downarrow}\\) is downward longwave radiation at the surface i.e. <code>lwdn_sfc</code> output by Isca.     Units: \\(Wm^{-2}\\).</li> <li>\\(LH^{\\uparrow}\\) is the upward latent heat radiation at the surface i.e. <code>flux_lhe</code> output by Isca.     Units: \\(Wm^{-2}\\).</li> <li>\\(SH^{\\uparrow}\\) is the upward sensible heat radiation at the surface i.e. <code>flux_t</code> output by Isca.     Units: \\(Wm^{-2}\\).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>ndarray</code> <p><code>float [n_time]</code> Time in days (assumes periodic e.g. annual mean, so <code>time = np.arange(360)</code>).</p> required <code>temp</code> <code>ndarray</code> <p><code>float [n_time]</code> Surface temperature, \\(T\\) for each day in <code>time</code>. Assumes periodic so <code>temp[0]</code> is the temperature at time <code>time[-1]+1</code>.</p> required <code>lambda_const</code> <code>ndarray</code> <p><code>float [n_lambda+1]</code> The constants \\(\\lambda_i\\) used in the approximation. <code>lambda_const[0]</code> is \\(\\lambda_0\\) and <code>lambda_const[i]</code> is \\(\\lambda_{i}\\) for \\(i&gt;0\\).</p> required <code>lambda_time_lag</code> <code>Optional[ndarray]</code> <p><code>float [n_lambda]</code> The constants \\(\\Lambda_i\\) used in the approximation. <code>lambda_time_lag[0]</code> is \\(\\Lambda_1\\) and <code>lambda_time_lag[i]</code> is \\(\\Lambda_{i+1}\\) for \\(i&gt;0\\).</p> <code>None</code> <code>lambda_nl</code> <code>Optional[Union[float, ndarray]]</code> <p><code>float [n_lambda_nl]</code> The constants \\(\\lambda_{nl}\\) used in the approximation. <code>[0]</code> is squared contribution, <code>[1]</code> is cubed, ...</p> <code>None</code> <code>temp_anom_nl</code> <code>Optional[ndarray]</code> <p><code>float [n_time]</code> The value of \\(T'(t)\\) to use in the calculation of non-linear part: \\(\\sum_{j=2}^{j_{max}}\\lambda_{nl_j}\\left(T'^{j}(t) - \\overline{T'^j}\\right)\\). If <code>None</code>, will compute from <code>temp</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_time]</code> The approximation \\(\\Gamma^{\\uparrow} \\approx \\lambda_0 + \\sum_{i=1}^{N_{\\lambda}}\\lambda_i T(t-\\Lambda_i) + \\lambda_{sq}\\left(T(t) - \\overline{T}\\right)^2\\) with units of \\(Wm^{-2}\\).</p> Source code in <code>isca_tools/thesis/surface_energy_budget.py</code> <pre><code>def gamma_linear_approx(time: np.ndarray, temp: np.ndarray,\n                        lambda_const: np.ndarray, lambda_time_lag: Optional[np.ndarray] = None,\n                        lambda_nl: Optional[Union[float, np.ndarray]] = None,\n                        temp_anom_nl: Optional[np.ndarray] = None) -&gt; np.ndarray:\n    \"\"\"\n    OUTDATED FUNCTION - USED FOR `get_temp_fourier` but now use `get_temp_fourier_numerical`\n\n    This approximates $\\Gamma^{\\\\uparrow} = LW^{\\\\uparrow} - LW^{\\\\downarrow} + LH^{\\\\uparrow} + SH^{\\\\uparrow}$ as:\n\n    $$\\Gamma^{\\\\uparrow} \\\\approx \\lambda_0 + \\sum_{i=1}^{N_{\\lambda}}\\lambda_i T(t-\\Lambda_i) +\n    \\sum_{j=2}^{j_{max}}\\lambda_{nl_j}\\\\left(T'^{j}(t) - \\overline{T'^j}\\\\right)$$\n\n    where:\n\n    * $T' = T(t) - \\overline{T}$ is the surface temperature anomaly\n    * $LW^{\\\\uparrow}$ is upward longwave radiation at the surface i.e. $\\\\sigma T^4$ or `lwup_sfc` output by Isca.\n        Units: $Wm^{-2}$.\n    * $LW^{\\\\downarrow}$ is downward longwave radiation at the surface i.e. `lwdn_sfc` output by Isca.\n        Units: $Wm^{-2}$.\n    * $LH^{\\\\uparrow}$ is the upward latent heat radiation at the surface i.e. `flux_lhe` output by Isca.\n        Units: $Wm^{-2}$.\n    * $SH^{\\\\uparrow}$ is the upward sensible heat radiation at the surface i.e. `flux_t` output by Isca.\n        Units: $Wm^{-2}$.\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Time in days (assumes periodic e.g. annual mean, so `time = np.arange(360)`).\n        temp: `float [n_time]`&lt;/br&gt;\n            Surface temperature, $T$ for each day in `time`. Assumes periodic so `temp[0]` is the temperature\n            at time `time[-1]+1`.\n        lambda_const: `float [n_lambda+1]`&lt;/br&gt;\n            The constants $\\lambda_i$ used in the approximation.&lt;/br&gt;\n            `lambda_const[0]` is $\\lambda_0$ and `lambda_const[i]` is $\\lambda_{i}$ for $i&gt;0$.\n        lambda_time_lag: `float [n_lambda]`&lt;/br&gt;\n            The constants $\\Lambda_i$ used in the approximation.&lt;/br&gt;\n            `lambda_time_lag[0]` is $\\Lambda_1$ and `lambda_time_lag[i]` is $\\Lambda_{i+1}$ for $i&gt;0$.\n        lambda_nl: `float [n_lambda_nl]`\n            The constants $\\lambda_{nl}$ used in the approximation. `[0]` is squared contribution, `[1]` is cubed, ...\n        temp_anom_nl: `float [n_time]`&lt;/br&gt;\n            The value of $T'(t)$ to use in the calculation of non-linear part:\n            $\\sum_{j=2}^{j_{max}}\\lambda_{nl_j}\\\\left(T'^{j}(t) - \\overline{T'^j}\\\\right)$.\n            If `None`, will compute from `temp`.\n\n    Returns:\n        `float [n_time]`&lt;/br&gt;\n            The approximation $\\Gamma^{\\\\uparrow} \\\\approx \\lambda_0 + \\sum_{i=1}^{N_{\\lambda}}\\lambda_i T(t-\\Lambda_i)\n            + \\lambda_{sq}\\\\left(T(t) - \\overline{T}\\\\right)^2$ with units of $Wm^{-2}$.\n    \"\"\"\n    if lambda_nl is not None:\n        #  deal with case when float given as lambda_nl\n        if not hasattr(lambda_nl, \"__len__\"):\n            lambda_nl = np.asarray([lambda_nl])\n    n_lambda = len(lambda_const) - 1\n    if lambda_time_lag is None:\n        lambda_temp = np.sum(lambda_const[1:]) * temp\n    else:\n        # To apply phase shift, need spline so can compute temp anomaly at times outside range in `time`.\n        temp_spline_fit = CubicSpline(np.append(time, time[-1] + 1), np.append(temp, temp[0]),\n                                      bc_type='periodic')\n        lambda_temp = np.zeros_like(temp)\n        for i in range(n_lambda):\n            lambda_temp += lambda_const[1 + i] * temp_spline_fit(time - lambda_time_lag[i])\n    gamma_nl = np.zeros_like(temp)\n    if lambda_nl is not None:\n        if temp_anom_nl is None:\n            temp_anom_nl = temp - np.mean(temp)\n        n_lambda_nl = len(lambda_nl)\n        for j in range(n_lambda_nl):\n            gamma_nl += lambda_nl[j] * (temp_anom_nl ** (j + 2) - np.mean(temp_anom_nl ** (j + 2)))\n    return lambda_const[0] + lambda_temp + gamma_nl\n</code></pre>"},{"location":"code/thesis/surface_energy_budget/#isca_tools.thesis.surface_energy_budget.get_temp_extrema_analytic","title":"<code>get_temp_extrema_analytic(sw_fourier_amp1, heat_capacity, lambda_const, lambda_phase=0, lambda_sq=0, sw_fourier_amp2=0, n_year_days=360, day_seconds=86400)</code>","text":"<p>This will return the analytic solution for the times and amplitudes of the extrema of the fourier solution of \\(T'\\) in the following form of the surface energy budget:</p> \\[ C\\frac{\\partial T'}{\\partial t} = F(t) - \\lambda_0 - \\lambda T'(t) - \\lambda_{phase}T'(t-\\mathcal{T}/4) - \\lambda_{sq} T'^2(t) \\] <p>Parameters:</p> Name Type Description Default <code>sw_fourier_amp1</code> <code>Union[float, ndarray]</code> <p><code>float [n_regions]</code> The first harmonic amplitude fourier coefficients for shortwave radiation, \\(SW^{\\downarrow}\\): \\(F_1\\).</p> required <code>heat_capacity</code> <code>float</code> <p>\\(C\\), the heat capacity of the surface in units of \\(JK^{-1}m^{-2}\\). Obtained from mixed layer depth of ocean using <code>get_heat_capacity</code>.</p> required <code>lambda_const</code> <code>Union[float, ndarray]</code> <p>The constant \\(\\lambda\\) used in the approximation for \\(\\Gamma^{\\uparrow} = LW^{\\uparrow} - LW^{\\downarrow} + LH^{\\uparrow} + SH^{\\uparrow}\\).</p> required <code>lambda_phase</code> <code>Union[float, ndarray]</code> <p>The constants \\(\\lambda_{phase}\\) used in the approximation for \\(\\Gamma^{\\uparrow}\\).</p> <code>0</code> <code>lambda_sq</code> <code>Union[float, ndarray]</code> <p>The constant \\(\\lambda_{sq}\\) used in the approximation for \\(\\Gamma^{\\uparrow}\\).</p> <code>0</code> <code>sw_fourier_amp2</code> <code>Union[float, ndarray]</code> <p><code>float [n_regions]</code> The second harmonic amplitude fourier coefficients for shortwave radiation, \\(SW^{\\downarrow}\\): \\(F_1\\).</p> <code>0</code> <code>n_year_days</code> <code>int</code> <p>Number of days in a year.</p> <code>360</code> <code>day_seconds</code> <code>float</code> <p>Duration of a day in seconds.</p> <code>86400</code> <p>Returns:</p> Name Type Description <code>time_extrema1</code> <code>Union[float, ndarray]</code> <p>Time in days of extrema to occur first</p> <code>time_extrema2</code> <code>Union[float, ndarray]</code> <p>Time in days of extrema to occur last</p> <code>amp_extrema1</code> <code>Union[float, ndarray]</code> <p>Absolute amplitude of extrema to occur first</p> <code>amp_extrema2</code> <code>Union[float, ndarray]</code> <p>Absolute amplitude of extrema to occur last</p> Source code in <code>isca_tools/thesis/surface_energy_budget.py</code> <pre><code>def get_temp_extrema_analytic(sw_fourier_amp1: Union[float, np.ndarray], heat_capacity: float,\n                              lambda_const: Union[float, np.ndarray], lambda_phase: Union[float, np.ndarray] = 0,\n                              lambda_sq: Union[float, np.ndarray] = 0, sw_fourier_amp2: Union[float, np.ndarray] = 0,\n                              n_year_days: int = 360, day_seconds: float = 86400\n                              ) -&gt; Tuple[Union[float, np.ndarray], Union[float, np.ndarray], Union[float, np.ndarray],\nUnion[float, np.ndarray]]:\n    \"\"\"\n    This will return the analytic solution for the times and amplitudes of the extrema of the fourier\n    solution of $T'$ in the following form of the surface energy budget:\n\n    $$\n    C\\\\frac{\\partial T'}{\\partial t} = F(t) - \\lambda_0 - \\lambda T'(t) - \\lambda_{phase}T'(t-\\mathcal{T}/4) -\n    \\lambda_{sq} T'^2(t)\n    $$\n\n    Args:\n        sw_fourier_amp1: `float [n_regions]`&lt;/br&gt;\n            The first harmonic amplitude fourier coefficients for shortwave radiation, $SW^{\\\\downarrow}$: $F_1$.\n        heat_capacity: $C$, the heat capacity of the surface in units of $JK^{-1}m^{-2}$.&lt;/br&gt;\n            Obtained from mixed layer depth of ocean using\n            [`get_heat_capacity`](../utils/radiation.md#isca_tools.utils.radiation.get_heat_capacity).\n        lambda_const: The constant $\\lambda$ used in the approximation for\n            $\\Gamma^{\\\\uparrow} = LW^{\\\\uparrow} - LW^{\\\\downarrow} + LH^{\\\\uparrow} + SH^{\\\\uparrow}$.&lt;/br&gt;\n        lambda_phase: The constants $\\lambda_{phase}$ used in the approximation for $\\Gamma^{\\\\uparrow}$.\n        lambda_sq: The constant $\\lambda_{sq}$ used in the approximation for $\\Gamma^{\\\\uparrow}$.\n        sw_fourier_amp2: `float [n_regions]`&lt;/br&gt;\n            The second harmonic amplitude fourier coefficients for shortwave radiation, $SW^{\\\\downarrow}$: $F_1$.\n        n_year_days: Number of days in a year.\n        day_seconds: Duration of a day in seconds.\n\n    Returns:\n        time_extrema1: Time in days of extrema to occur first\n        time_extrema2: Time in days of extrema to occur last\n        amp_extrema1: Absolute amplitude of extrema to occur first\n        amp_extrema2: Absolute amplitude of extrema to occur last\n    \"\"\"\n    f = 1/(n_year_days*day_seconds)\n    if sw_fourier_amp2 == 0:\n        if lambda_sq != 0:\n            raise ValueError('Cannot solve for non-zero lambda_sq with single harmonic - '\n                             'get extrema numerically instead')\n        tan_phase = (2*np.pi*heat_capacity*f - lambda_phase) / lambda_const\n        time_extrema1 = np.arctan(tan_phase)/(2*np.pi) * n_year_days\n        time_extrema2 = time_extrema1 + n_year_days/2\n        amp_extrema1 = np.abs(sw_fourier_amp1/lambda_const) / (np.sqrt(1+tan_phase**2))\n        amp_extrema2 = amp_extrema1\n    return time_extrema1, time_extrema2, amp_extrema1, amp_extrema2\n</code></pre>"},{"location":"code/thesis/surface_energy_budget/#isca_tools.thesis.surface_energy_budget.get_temp_extrema_numerical","title":"<code>get_temp_extrema_numerical(time, temp, smooth_window=1, smooth_method='convolve')</code>","text":"<p>Given the temperature <code>temp</code>, this will return the times and amplitudes of the maxima and minima. The extrema will be returned in time order i.e. if minima occurs first, it will be returned first.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>ndarray</code> <p><code>float [n_time]</code> Time in days (assumes periodic e.g. annual mean, so <code>time = np.arange(360)</code>)</p> required <code>temp</code> <code>ndarray</code> <p><code>float [n_time]</code> Value of temperature at each time. Again, assume periodic.</p> required <code>smooth_window</code> <code>int</code> <p>Number of time steps to use to smooth <code>temp</code> before finding extrema. Smaller equals more accurate fit. <code>1</code> is perfect fit.</p> <code>1</code> <code>smooth_method</code> <code>str</code> <p><code>convolve</code> or <code>spline</code> If <code>convolve</code>, will smooth via convolution with window of length <code>smooth_window</code>. If <code>spline</code>, will fit a spline using every <code>smooth_window</code> values of <code>time</code>.</p> <code>'convolve'</code> <p>Returns:</p> Name Type Description <code>time_extrema1</code> <code>float</code> <p>Time in days of extrema to occur first</p> <code>time_extrema2</code> <code>float</code> <p>Time in days of extrema to occur last</p> <code>amp_extrema1</code> <code>float</code> <p>Absolute amplitude of extrema to occur first</p> <code>amp_extrema2</code> <code>float</code> <p>Absolute amplitude of extrema to occur last</p> Source code in <code>isca_tools/thesis/surface_energy_budget.py</code> <pre><code>def get_temp_extrema_numerical(time: np.ndarray, temp: np.ndarray, smooth_window: int = 1,\n                               smooth_method: str = 'convolve') -&gt; Tuple[float, float, float, float]:\n    \"\"\"\n    Given the temperature `temp`, this will return the times and amplitudes of the maxima and minima. The extrema\n    will be returned in time order i.e. if minima occurs first, it will be returned first.\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Time in days (assumes periodic e.g. annual mean, so `time = np.arange(360)`)\n        temp: `float [n_time]`&lt;/br&gt;\n            Value of temperature at each time. Again, assume periodic.\n        smooth_window: Number of time steps to use to smooth `temp` before finding extrema.\n            Smaller equals more accurate fit. `1` is perfect fit.\n        smooth_method: `convolve` or `spline`&lt;/br&gt;\n            If `convolve`, will smooth via convolution with window of length `smooth_window`.\n            If `spline`, will fit a spline using every `smooth_window` values of `time`.\n\n    Returns:\n        time_extrema1: Time in days of extrema to occur first\n        time_extrema2: Time in days of extrema to occur last\n        amp_extrema1: Absolute amplitude of extrema to occur first\n        amp_extrema2: Absolute amplitude of extrema to occur last\n    \"\"\"\n    time_extrema = {}\n    amp_extrema = {}\n    for key in ['min', 'max']:\n        var_use, spline_use = numerical.get_var_extrema_date(time, temp - np.mean(temp),\n                                                             smooth_window=smooth_window, type=key, max_extrema=1,\n                                                             smooth_method=smooth_method)\n        time_extrema[key] = var_use[0]\n        amp_extrema[key] = np.abs(spline_use(time_extrema[key]))\n    # Put output in time order\n    if time_extrema['min'] &lt;= time_extrema['max']:\n        return time_extrema['min'], time_extrema['max'], amp_extrema['min'], amp_extrema['max']\n    else:\n        return time_extrema['max'], time_extrema['min'], amp_extrema['max'], amp_extrema['min']\n</code></pre>"},{"location":"code/thesis/surface_energy_budget/#isca_tools.thesis.surface_energy_budget.get_temp_fourier","title":"<code>get_temp_fourier(time, swdn, heat_capacity, lambda_const, lambda_time_lag=None, lambda_nl=None, n_harmonics=2, include_sw_phase=False, numerical=False, day_seconds=86400, single_harmonic_nl=False, return_anomaly=True)</code>","text":"<p>OUTDATED FUNCTION - NOW USE <code>get_temp_fourier_analytical</code> or <code>get_temp_fourier_numerical</code></p> <p>Seeks a fourier solution of the form \\(T(t) = \\frac{T_0}{2} + \\sum_{n=1}^{N} T_n\\cos(2n\\pi ft - \\phi_n)\\) to the linearized surface energy budget of the general form:</p> \\[ C\\frac{\\partial T}{\\partial t} = F(t) - \\lambda_0 - \\sum_{i=1}^{N_{\\lambda}}\\lambda_i T(t-\\Lambda_i) - \\sum_{j=2}^{j_{max}}\\lambda_{nl_j}\\left(T'^{j}(t) - \\overline{T'^j}\\right) \\] <p>where:</p> <ul> <li>\\(T' = T(t) - \\overline{T}\\) is the surface temperature anomaly</li> <li>\\(C\\) is the heat capacity of the surface</li> <li>\\(\\overline{T} = T_0/2\\) is the mean temperature.</li> <li>\\(F(t) = \\frac{F_0}{2} + \\sum_{n=1}^{N} F_n\\cos(2n\\pi ft - \\varphi_n)\\) is the Fourier representation of the downward shortwave radiation at the surface, \\(SW^{\\downarrow}\\).</li> <li>\\(\\lambda_0 + \\sum_{i=1}^{N_{\\lambda}}\\lambda_i T(t-\\Lambda_i) + \\sum_{j=2}^{j_{max}}\\lambda_{nl_j}\\left(T'^{j}(t) - \\overline{T'^j}\\right)\\) is the approximation for \\(\\Gamma^{\\uparrow} = LW^{\\uparrow} - LW^{\\downarrow} + LH^{\\uparrow} + SH^{\\uparrow}\\)</li> </ul> <p>The solution is exact if \\(\\lambda_{nl_j}=0 \\forall j\\) and has the form:</p> <ul> <li>\\(T_0 = (F_0-2\\lambda_0)/\\sum_{i=1}^{N_{\\lambda}}\\lambda_i\\)</li> <li>\\(T_n = \\frac{F_n \\cos(\\varphi_n)\\sqrt{1+\\tan^2\\phi_n}}{ (2\\pi nfC - \\sum_i \\lambda_i \\sin \\Phi_{ni})\\tan\\phi_n + \\sum_i \\lambda_i \\cos \\Phi_{ni}}\\)</li> <li>\\(\\tan \\phi_n = \\frac{2\\pi nfC + \\tan \\varphi_n \\sum_i \\lambda_i \\cos \\Phi_{ni} - \\sum_i \\lambda_i \\sin \\Phi_{ni}}{-2\\pi nfC \\tan \\varphi_n + \\sum_i \\lambda_i \\cos \\Phi_{ni} - \\tan \\varphi_n \\sum_i \\lambda_i \\sin \\Phi_{ni}}\\)</li> <li>\\(\\Phi_{ni} = 2\\pi nf \\Lambda_i\\) (units of radians, whereas \\(\\Lambda_i\\) is in units of time - days).</li> </ul> <p>If \\(\\lambda_{nl_j}\\neq 0\\), an approximate numerical solution will be obtained, still of the Fourier form.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>ndarray</code> <p><code>float [n_time]</code> Time in days (assumes periodic e.g. annual mean, so <code>time = np.arange(360)</code>).</p> required <code>swdn</code> <code>ndarray</code> <p><code>float [n_time]</code> Downward shortwave radiation at the surface, \\(SW^{\\downarrow}\\). I.e. <code>swdn_sfc</code> output by Isca. Units: \\(Wm^{-2}\\).</p> required <code>heat_capacity</code> <code>float</code> <p>\\(C\\), the heat capacity of the surface in units of \\(JK^{-1}m^{-2}\\). Obtained from mixed layer depth of ocean using <code>get_heat_capacity</code>.</p> required <code>lambda_const</code> <code>ndarray</code> <p><code>float [n_lambda+1]</code> The constants \\(\\lambda_i\\) used in the approximation for \\(\\Gamma^{\\uparrow} = LW^{\\uparrow} - LW^{\\downarrow} + LH^{\\uparrow} + SH^{\\uparrow}\\). <code>lambda_const[0]</code> is \\(\\lambda_0\\) and <code>lambda_const[i]</code> is \\(\\lambda_{i}\\) for \\(i&gt;0\\).</p> required <code>lambda_time_lag</code> <code>Optional[ndarray]</code> <p><code>float [n_lambda]</code> The constants \\(\\Lambda_i\\) used in the approximation for \\(\\Gamma^{\\uparrow}\\). <code>lambda_time_lag[0]</code> is \\(\\Lambda_1\\) and <code>lambda_time_lag[i]</code> is \\(\\Lambda_{i+1}\\) for \\(i&gt;0\\).</p> <code>None</code> <code>lambda_nl</code> <code>Optional[Union[float, ndarray]]</code> <p><code>float [n_lambda_nl]</code> The constants \\(\\lambda_{nl}\\) used in the approximation for \\(\\Gamma^{\\uparrow}\\). <code>[0]</code> is squared contribution, <code>[1]</code> is cubed, ...</p> <code>None</code> <code>n_harmonics</code> <code>int</code> <p>Number of harmonics to use to fit fourier series for both \\(T(t)\\) and \\(F(t)\\), \\(N\\).</p> <code>2</code> <code>include_sw_phase</code> <code>bool</code> <p>If <code>False</code>, will set all phase factors, \\(\\varphi_n=0\\), in Fourier expansion of \\(F(t)\\). These phase factors are usually very small, and it makes the solution for \\(T(t)\\) more simple if they are set to 0, hence the option.</p> <code>False</code> <code>numerical</code> <code>bool</code> <p>If <code>True</code>, will compute solution for \\(T(t)\\) numerically using <code>scipy.optimize.curve_fit</code> rather than using the analytic solution. Will always return numerical solution if <code>lambda_sq</code> \\(\\neq 0\\).</p> <code>False</code> <code>day_seconds</code> <code>float</code> <p>Duration of a day in seconds.</p> <code>86400</code> <code>single_harmonic_nl</code> <code>bool</code> <p>If <code>True</code>, the \\(\\lambda_{nl_j}T'^j\\) terms in \\(\\Gamma^{\\uparrow}\\) will only use the first harmonic, not all harmonics.</p> <code>False</code> <code>return_anomaly</code> <code>bool</code> <p>If <code>True</code>, the first return variable, <code>temp_fourier</code> will be the temperature anomaly, i.e. it will not include \\(T_0\\).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>temp_fourier</code> <code>ndarray</code> <p><code>float [n_time]</code> The Fourier series solution that was found for surface temperature, \\(T\\).</p> <code>temp_fourier_amp</code> <code>ndarray</code> <p><code>float [n_harmonics+1]</code> The amplitude Fourier coefficients for surface temperature: \\(T_n\\).</p> <code>temp_fourier_phase</code> <code>ndarray</code> <p><code>float [n_harmonics]</code> The phase Fourier coefficients for surface temperature: \\(\\phi_n\\).</p> <code>sw_fourier_amp</code> <code>ndarray</code> <p><code>float [n_harmonics+1]</code> The amplitude Fourier coefficients for shortwave radiation, \\(SW^{\\downarrow}\\): \\(F_n\\).</p> <code>sw_fourier_phase</code> <code>ndarray</code> <p><code>float [n_harmonics]</code> The phase Fourier coefficients for shortwave radiation, \\(SW^{\\downarrow}\\): \\(\\varphi_n\\).</p> Source code in <code>isca_tools/thesis/surface_energy_budget.py</code> <pre><code>def get_temp_fourier(time: np.ndarray, swdn: np.ndarray, heat_capacity: float,\n                     lambda_const: np.ndarray, lambda_time_lag: Optional[np.ndarray] = None,\n                     lambda_nl: Optional[Union[float, np.ndarray]] = None, n_harmonics: int = 2,\n                     include_sw_phase: bool = False, numerical: bool = False,\n                     day_seconds: float = 86400,\n                     single_harmonic_nl: bool = False, return_anomaly: bool = True) -&gt; Tuple[\n    np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    OUTDATED FUNCTION - NOW USE `get_temp_fourier_analytical` or `get_temp_fourier_numerical`\n\n    Seeks a fourier solution of the form $T(t) = \\\\frac{T_0}{2} + \\\\sum_{n=1}^{N} T_n\\\\cos(2n\\\\pi ft - \\\\phi_n)$\n    to the linearized surface energy budget of the general form:\n\n    $$\n    C\\\\frac{\\partial T}{\\partial t} = F(t) - \\lambda_0 - \\sum_{i=1}^{N_{\\lambda}}\\lambda_i T(t-\\Lambda_i) -\n    \\sum_{j=2}^{j_{max}}\\lambda_{nl_j}\\\\left(T'^{j}(t) - \\overline{T'^j}\\\\right)\n    $$\n\n    where:\n\n    * $T' = T(t) - \\overline{T}$ is the surface temperature anomaly\n    * $C$ is the heat capacity of the surface\n    * $\\overline{T} = T_0/2$ is the mean temperature.\n    * $F(t) = \\\\frac{F_0}{2} + \\\\sum_{n=1}^{N} F_n\\\\cos(2n\\\\pi ft - \\\\varphi_n)$ is the Fourier representation\n    of the downward shortwave radiation at the surface, $SW^{\\downarrow}$.\n    * $\\lambda_0 + \\sum_{i=1}^{N_{\\lambda}}\\lambda_i T(t-\\Lambda_i) +\n    \\sum_{j=2}^{j_{max}}\\lambda_{nl_j}\\\\left(T'^{j}(t) - \\overline{T'^j}\\\\right)$ is the approximation for\n    $\\Gamma^{\\\\uparrow} = LW^{\\\\uparrow} - LW^{\\\\downarrow} + LH^{\\\\uparrow} + SH^{\\\\uparrow}$\n\n    The solution is exact if $\\lambda_{nl_j}=0 \\\\forall j$ and has the form:\n\n    * $T_0 = (F_0-2\\lambda_0)/\\sum_{i=1}^{N_{\\lambda}}\\lambda_i$\n    * $T_n = \\\\frac{F_n \\cos(\\\\varphi_n)\\sqrt{1+\\\\tan^2\\phi_n}}{\n    (2\\pi nfC - \\sum_i \\lambda_i \\sin \\Phi_{ni})\\\\tan\\phi_n + \\sum_i \\lambda_i \\cos \\Phi_{ni}}$\n    * $\\\\tan \\phi_n = \\\\frac{2\\pi nfC + \\\\tan \\\\varphi_n \\sum_i \\lambda_i \\cos \\Phi_{ni} -\n    \\sum_i \\lambda_i \\sin \\Phi_{ni}}{-2\\pi nfC \\\\tan \\\\varphi_n + \\sum_i \\lambda_i \\cos \\Phi_{ni} -\n    \\\\tan \\\\varphi_n \\sum_i \\lambda_i \\sin \\Phi_{ni}}$\n    * $\\Phi_{ni} = 2\\pi nf \\Lambda_i$ (units of radians, whereas $\\Lambda_i$ is in units of time - days).\n\n    If $\\lambda_{nl_j}\\\\neq 0$, an approximate numerical solution will be obtained, still of the Fourier form.\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Time in days (assumes periodic e.g. annual mean, so `time = np.arange(360)`).\n        swdn: `float [n_time]`&lt;/br&gt;\n            Downward shortwave radiation at the surface, $SW^{\\\\downarrow}$. I.e. `swdn_sfc` output by Isca.\n            Units: $Wm^{-2}$.\n        heat_capacity: $C$, the heat capacity of the surface in units of $JK^{-1}m^{-2}$.&lt;/br&gt;\n            Obtained from mixed layer depth of ocean using\n            [`get_heat_capacity`](../utils/radiation.md#isca_tools.utils.radiation.get_heat_capacity).\n        lambda_const: `float [n_lambda+1]`&lt;/br&gt;\n            The constants $\\lambda_i$ used in the approximation for\n            $\\Gamma^{\\\\uparrow} = LW^{\\\\uparrow} - LW^{\\\\downarrow} + LH^{\\\\uparrow} + SH^{\\\\uparrow}$.&lt;/br&gt;\n            `lambda_const[0]` is $\\lambda_0$ and `lambda_const[i]` is $\\lambda_{i}$ for $i&gt;0$.\n        lambda_time_lag: `float [n_lambda]`&lt;/br&gt;\n            The constants $\\Lambda_i$ used in the approximation for $\\Gamma^{\\\\uparrow}$.&lt;/br&gt;\n            `lambda_time_lag[0]` is $\\Lambda_1$ and `lambda_time_lag[i]` is $\\Lambda_{i+1}$ for $i&gt;0$.\n        lambda_nl: `float [n_lambda_nl]`\n            The constants $\\lambda_{nl}$ used in the approximation for $\\Gamma^{\\\\uparrow}$.\n            `[0]` is squared contribution, `[1]` is cubed, ...\n        n_harmonics: Number of harmonics to use to fit fourier series for both $T(t)$ and $F(t)$, $N$.\n        include_sw_phase: If `False`, will set all phase factors, $\\\\varphi_n=0$, in Fourier expansion of $F(t)$.&lt;/br&gt;\n            These phase factors are usually very small, and it makes the solution for $T(t)$ more simple if they\n            are set to 0, hence the option.\n        numerical: If `True`, will compute solution for $T(t)$ numerically using [`scipy.optimize.curve_fit`](\n            https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) rather than using the\n            analytic solution. Will always return numerical solution if `lambda_sq` $\\\\neq 0$.\n        day_seconds: Duration of a day in seconds.\n        single_harmonic_nl: If `True`, the $\\lambda_{nl_j}T'^j$ terms in $\\Gamma^{\\\\uparrow}$ will only\n            use the first harmonic, not all harmonics.\n        return_anomaly: If `True`, the first return variable, `temp_fourier` will be\n            the temperature anomaly, i.e. it will not include $T_0$.\n\n    Returns:\n        temp_fourier: `float [n_time]`&lt;/br&gt;\n            The Fourier series solution that was found for surface temperature, $T$.\n        temp_fourier_amp: `float [n_harmonics+1]`&lt;/br&gt;\n            The amplitude Fourier coefficients for surface temperature: $T_n$.\n        temp_fourier_phase: `float [n_harmonics]`&lt;/br&gt;\n            The phase Fourier coefficients for surface temperature: $\\phi_n$.\n        sw_fourier_amp: `float [n_harmonics+1]`&lt;/br&gt;\n            The amplitude Fourier coefficients for shortwave radiation, $SW^{\\\\downarrow}$: $F_n$.\n        sw_fourier_phase: `float [n_harmonics]`&lt;/br&gt;\n            The phase Fourier coefficients for shortwave radiation, $SW^{\\\\downarrow}$: $\\\\varphi_n$.\n    \"\"\"\n    if lambda_nl is not None:\n        #  deal with case when float given as lambda_nl\n        if not hasattr(lambda_nl, \"__len__\"):\n            lambda_nl = np.asarray([lambda_nl])\n    n_year_days = len(time)\n    n_lambda = len(lambda_const) - 1\n    if lambda_time_lag is None:\n        lambda_time_lag = np.zeros(n_lambda)\n    else:\n        if len(lambda_time_lag) != n_lambda:\n            raise ValueError(f'Size of lambda_time_lag should be {n_lambda} not {len(lambda_time_lag)}.')\n\n    # Get fourier representation of SW radiation\n    sw_fourier_amp, sw_fourier_phase = fourier.get_fourier_fit(time, swdn, n_harmonics)[1:]\n    if not include_sw_phase:\n        sw_fourier_phase = np.zeros(n_harmonics)\n    sw_fourier = fourier.fourier_series(time, sw_fourier_amp, sw_fourier_phase)\n    sw_tan = np.tan(sw_fourier_phase)\n    sw_cos = np.cos(sw_fourier_phase)\n    sw_sin = np.sin(sw_fourier_phase)\n    f = 1 / (\n            n_year_days * day_seconds)  # must have frequency in units of s^{-1} to deal with phase stuff in radians\n\n    if numerical or (lambda_nl is not None and not single_harmonic_nl):\n        if not numerical:\n            warnings.warn('Analytic solution not possible with lambda_nl non zero and single_hamrmonic_nl=False')\n\n        def fit_func(time_array, *args):\n            fourier_amp_coef = np.asarray([args[i] for i in range(n_harmonics + 1)])\n            fourier_phase_coef = np.asarray([args[i] for i in range(n_harmonics + 1, len(args))])\n            return swdn_from_temp_fourier(time_array, fourier_amp_coef, fourier_phase_coef, heat_capacity, lambda_const,\n                                          lambda_time_lag, lambda_nl, day_seconds, single_harmonic_nl)\n\n        # force positive phase coefficient to match analytic solution\n        bounds_lower = [-np.inf] * (n_harmonics + 1) + [0] * n_harmonics\n        bounds_upper = [np.inf] * (n_harmonics + 1) + [2 * np.pi] * n_harmonics\n\n        # Starting solution is 1 harmonic analytical solution\n        p0 = np.zeros(2 * n_harmonics + 1)\n        lambda_phase_const = lambda_time_lag * 2 * np.pi / n_year_days\n        lambda_cos = np.sum(lambda_const[1:] * np.cos(lambda_phase_const))\n        lambda_sin = np.sum(lambda_const[1:] * np.sin(lambda_phase_const))\n        p0[0] = (sw_fourier_amp[0] - 2 * lambda_const[0]) / np.sum(lambda_const[1:])\n        p0[n_harmonics + 1] = np.arctan(\n            (2 * np.pi * f * heat_capacity + sw_tan[0] * lambda_cos - lambda_sin) / (\n                    -2 * np.pi * f * heat_capacity * sw_tan[0] + lambda_cos - sw_tan[0] * lambda_sin))\n        p0[1] = sw_fourier_amp[1] * sw_cos[0] / np.cos(p0[n_harmonics + 1]) / (\n                (2 * np.pi * f * heat_capacity - lambda_sin) * np.tan(\n            p0[n_harmonics + 1]) + lambda_cos)\n\n        try:\n            args_found = optimize.curve_fit(fit_func, time, sw_fourier, p0,\n                                            bounds=(bounds_lower, bounds_upper))[0]\n        except RuntimeError:\n            warnings.warn('Hit Runtime Error, trying without bounds')\n            args_found = optimize.curve_fit(fit_func, time, sw_fourier, p0)[0]\n        temp_fourier_amp = args_found[:n_harmonics + 1]\n        temp_fourier_phase = args_found[n_harmonics + 1:]\n    else:\n        temp_fourier_amp = np.zeros(n_harmonics + 1)\n        temp_fourier_phase = np.zeros(n_harmonics)\n        temp_fourier_amp[0] = (sw_fourier_amp[0] - 2 * lambda_const[0]) / np.sum(lambda_const[1:])\n\n        for n in range(1, n_harmonics + 1):\n            # convert lambda time lag from days to units of radians\n            lambda_phase_const = lambda_time_lag * 2 * n * np.pi / n_year_days\n            lambda_cos = np.sum(lambda_const[1:] * np.cos(lambda_phase_const))\n            lambda_sin = np.sum(lambda_const[1:] * np.sin(lambda_phase_const))\n\n            temp_fourier_phase[n - 1] = np.arctan(\n                (2 * np.pi * n * f * heat_capacity + sw_tan[n - 1] * lambda_cos - lambda_sin) / (\n                        -2 * np.pi * n * f * heat_capacity * sw_tan[n - 1] + lambda_cos - sw_tan[n - 1] * lambda_sin))\n            temp_fourier_amp[n] = sw_fourier_amp[n] * sw_cos[n - 1] / np.cos(\n                temp_fourier_phase[n - 1]) / (\n                                          (2 * np.pi * n * f * heat_capacity - lambda_sin) * np.tan(\n                                      temp_fourier_phase[n - 1]) + lambda_cos)\n            if n == 1 and lambda_nl is not None and single_harmonic_nl:\n                if len(lambda_nl) &gt; 1:\n                    raise ValueError(f'Analytic solution only possible with 1 not {len(lambda_nl)} non-linear terms')\n                # Get analytic solution when include squared term in energy budget\n                # Assuming squared term dominated by 1st harmonic\n                sw_tan[1] = (sw_fourier_amp[2] * sw_sin[1] -\n                             0.5 * lambda_nl[0] * temp_fourier_amp[1] ** 2 * np.sin(2 * temp_fourier_phase[0])) / (\n                                    sw_fourier_amp[2] * sw_cos[1] -\n                                    0.5 * lambda_nl[0] * temp_fourier_amp[1] ** 2 * np.cos(2 * temp_fourier_phase[0]))\n                sw_cos[1] -= 0.5 * lambda_nl[0] * temp_fourier_amp[1] ** 2 * np.cos(2 * temp_fourier_phase[0]\n                                                                                    ) / sw_fourier_amp[2]\n    if return_anomaly:\n        temp_fourier = fourier.fourier_series(time, np.append([0], temp_fourier_amp[1:]),\n                                              temp_fourier_phase)\n    else:\n        temp_fourier = fourier.fourier_series(time, temp_fourier_amp, temp_fourier_phase)\n    return temp_fourier, temp_fourier_amp, temp_fourier_phase, sw_fourier_amp, sw_fourier_phase\n</code></pre>"},{"location":"code/thesis/surface_energy_budget/#isca_tools.thesis.surface_energy_budget.get_temp_fourier_analytic","title":"<code>get_temp_fourier_analytic(time, swdn_sfc, heat_capacity, lambda_const, lambda_phase=0, lambda_sq=0, lambda_cos=0, lambda_sin=0, n_harmonics_sw=2, n_harmonics_temp=None, include_sw_phase=False, day_seconds=86400)</code>","text":"<p>Seeks a fourier solution of the form \\(T'(t) = \\sum_{n=1}^{N} T_n\\cos(2n\\pi t/\\mathcal{T} - \\phi_n)\\) to the surface energy budget of the general form:</p> \\[ \\begin{align} \\begin{split} C\\frac{\\partial T'}{\\partial t} = &amp;F(t) - \\lambda_0 - \\frac{1}{2}\\lambda_{phase}(T'(t-\\mathcal{T}/4) - T'(t+\\mathcal{T}/4)) -  \\\\ &amp;\\lambda T'^(t) - \\lambda_{sq} T'^{2}(t) - \\Lambda_{cos}\\cos(4\\pi t/\\mathcal{T}) - \\Lambda_{sin}\\sin(4\\pi t/\\mathcal{T}) \\end{split} \\end{align} \\] <p>where:</p> <ul> <li>\\(T' = T(t) - \\overline{T}\\) is the surface temperature anomaly</li> <li>\\(C\\) is the heat capacity of the surface</li> <li>\\(\\overline{T} = T_0/2\\) is the mean temperature.</li> <li>\\(F(t) = \\frac{F_0}{2} + \\sum_{n=1}^{N} F_n\\cos(2n\\pi t/\\mathcal{T} - \\varphi_n)\\) is the Fourier representation of the downward shortwave radiation at the surface, \\(SW^{\\downarrow}\\).</li> <li>\\(\\lambda_0 + \\lambda T' - \\lambda_{sq} T'^{2} + \\frac{1}{2}\\lambda_{phase}(T'(t-\\mathcal{T}/4) - T'(t+\\mathcal{T}/4)) +\\) \\(\\Lambda_{cos}\\cos(4\\pi t/\\mathcal{T}) + \\Lambda_{sin}\\sin(4\\pi t/\\mathcal{T})\\) is the approximation for \\(\\Gamma^{\\uparrow} = LW^{\\uparrow} - LW^{\\downarrow} + LH^{\\uparrow} + SH^{\\uparrow}\\).</li> <li>\\(\\mathcal{T}\\) is the period i.e. one year.</li> </ul> <p>The solution is exact if \\(\\lambda_{sq}=0\\) and has the form:</p> <ul> <li>\\(T_0 = (F_0-2\\lambda_0)/\\sum_{i=1}^{N_{\\lambda}}\\lambda_i\\)</li> <li>\\(T_n = \\frac{F_n \\cos(\\varphi_n)\\sqrt{1+\\tan^2\\phi_n}}{ (2\\pi nfC - \\sum_i \\lambda_i \\sin \\Phi_{ni})\\tan\\phi_n + \\sum_i \\lambda_i \\cos \\Phi_{ni}}\\)</li> <li>\\(\\tan \\phi_n = \\frac{2\\pi nfC + \\tan \\varphi_n \\sum_i \\lambda_i \\cos \\Phi_{ni} - \\sum_i \\lambda_i \\sin \\Phi_{ni}}{-2\\pi nfC \\tan \\varphi_n + \\sum_i \\lambda_i \\cos \\Phi_{ni} - \\tan \\varphi_n \\sum_i \\lambda_i \\sin \\Phi_{ni}}\\)</li> <li>\\(i=1, 2\\) with \\(\\lambda_1=\\lambda\\) and \\(\\lambda_2=\\lambda_{phase}\\).</li> <li>\\(\\Phi_{n1}=0\\) and \\(\\Phi_{n2} = n\\pi/2\\) (from \\(2n\\pi / \\mathcal{T} \\times \\mathcal{T}/4\\)).</li> </ul> <p>If \\(\\lambda_{sq}\\neq 0\\), an approximate analytical solution will be obtained, assuming \\(T'^2(t) \\approx (T_1\\cos(2\\pi ft - \\phi_1))^2\\).</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>ndarray</code> <p><code>float [n_time]</code> Time in days (assumes periodic e.g. annual mean, so <code>time = np.arange(360)</code>).</p> required <code>swdn_sfc</code> <code>ndarray</code> <p><code>float [n_time]</code> Downward shortwave radiation at the surface, \\(SW^{\\downarrow}\\). I.e. <code>swdn_sfc</code> output by Isca. Units: \\(Wm^{-2}\\).</p> required <code>heat_capacity</code> <code>float</code> <p>\\(C\\), the heat capacity of the surface in units of \\(JK^{-1}m^{-2}\\). Obtained from mixed layer depth of ocean using <code>get_heat_capacity</code>.</p> required <code>lambda_const</code> <code>float</code> <p>The constant \\(\\lambda\\) used in the approximation for \\(\\Gamma^{\\uparrow} = LW^{\\uparrow} - LW^{\\downarrow} + LH^{\\uparrow} + SH^{\\uparrow}\\).</p> required <code>lambda_phase</code> <code>float</code> <p>The constants \\(\\lambda_{phase}\\) used in the approximation for \\(\\Gamma^{\\uparrow}\\).</p> <code>0</code> <code>lambda_sq</code> <code>float</code> <p>The constant \\(\\lambda_{sq}\\) used in the approximation for \\(\\Gamma^{\\uparrow}\\).</p> <code>0</code> <code>lambda_cos</code> <code>float</code> <p>The constant \\(\\Lambda_{cos}\\) used in the approximation for \\(\\Gamma^{\\uparrow}\\).</p> <code>0</code> <code>lambda_sin</code> <code>float</code> <p>The constant \\(\\Lambda_{sin}\\) used in the approximation for \\(\\Gamma^{\\uparrow}\\).</p> <code>0</code> <code>n_harmonics_sw</code> <code>int</code> <p>Number of harmonics to use to fit fourier series for \\(SW^{\\downarrow}\\). Cannot exceed <code>n_harmonics_temp</code> as extra harmonics would not be used.</p> <code>2</code> <code>n_harmonics_temp</code> <code>Optional[int]</code> <p>Number, \\(N\\), of harmonics in fourier solution of temperature anomaly. If not given, will set to <code>n_harmonics_sw</code>.</p> <code>None</code> <code>include_sw_phase</code> <code>bool</code> <p>If <code>False</code>, will set all phase factors, \\(\\varphi_n=0\\), in Fourier expansion of \\(SW^{\\downarrow}\\). These phase factors are usually very small, and it makes the solution for \\(T'(t)\\) more simple if they are set to 0, hence the option.</p> <code>False</code> <code>day_seconds</code> <code>float</code> <p>Duration of a day in seconds.</p> <code>86400</code> <p>Returns:</p> Name Type Description <code>temp_fourier</code> <code>ndarray</code> <p><code>float [n_time]</code> The Fourier series solution that was found for surface temperature, \\(T\\).</p> <code>temp_fourier_amp</code> <code>ndarray</code> <p><code>float [n_harmonics+1]</code> The amplitude Fourier coefficients for surface temperature: \\(T_n\\).</p> <code>temp_fourier_phase</code> <code>ndarray</code> <p><code>float [n_harmonics]</code> The phase Fourier coefficients for surface temperature: \\(\\phi_n\\).</p> <code>sw_fourier_amp</code> <code>ndarray</code> <p><code>float [n_harmonics+1]</code> The amplitude Fourier coefficients for shortwave radiation, \\(SW^{\\downarrow}\\): \\(F_n\\).</p> <code>sw_fourier_phase</code> <code>ndarray</code> <p><code>float [n_harmonics]</code> The phase Fourier coefficients for shortwave radiation, \\(SW^{\\downarrow}\\): \\(\\varphi_n\\).</p> Source code in <code>isca_tools/thesis/surface_energy_budget.py</code> <pre><code>def get_temp_fourier_analytic(time: np.ndarray, swdn_sfc: np.ndarray, heat_capacity: float,\n                              lambda_const: float, lambda_phase: float = 0,\n                              lambda_sq: float = 0, lambda_cos: float = 0, lambda_sin: float = 0,\n                              n_harmonics_sw: int = 2,\n                              n_harmonics_temp: Optional[int] = None,\n                              include_sw_phase: bool = False,\n                              day_seconds: float = 86400) -&gt; Tuple[\n    np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Seeks a fourier solution of the form $T'(t) = \\\\sum_{n=1}^{N} T_n\\\\cos(2n\\\\pi t/\\mathcal{T} - \\\\phi_n)$\n    to the surface energy budget of the general form:\n\n    $$\n    \\\\begin{align}\n    \\\\begin{split}\n    C\\\\frac{\\partial T'}{\\partial t} = &amp;F(t) - \\lambda_0 -\n    \\\\frac{1}{2}\\lambda_{phase}(T'(t-\\mathcal{T}/4) - T'(t+\\mathcal{T}/4)) -  \\\\\\\\\n    &amp;\\lambda T'^(t) - \\lambda_{sq} T'^{2}(t) - \\Lambda_{cos}\\\\cos(4\\\\pi t/\\mathcal{T}) -\n    \\Lambda_{sin}\\\\sin(4\\\\pi t/\\mathcal{T})\n    \\\\end{split}\n    \\\\end{align}\n    $$\n\n    where:\n\n    * $T' = T(t) - \\overline{T}$ is the surface temperature anomaly\n    * $C$ is the heat capacity of the surface\n    * $\\overline{T} = T_0/2$ is the mean temperature.\n    * $F(t) = \\\\frac{F_0}{2} + \\\\sum_{n=1}^{N} F_n\\\\cos(2n\\\\pi t/\\mathcal{T} - \\\\varphi_n)$ is the Fourier representation\n    of the downward shortwave radiation at the surface, $SW^{\\downarrow}$.\n    * $\\lambda_0 + \\lambda T' - \\lambda_{sq} T'^{2} +\n    \\\\frac{1}{2}\\lambda_{phase}(T'(t-\\mathcal{T}/4) - T'(t+\\mathcal{T}/4)) +$&lt;/br&gt;\n    $\\Lambda_{cos}\\\\cos(4\\\\pi t/\\mathcal{T}) + \\Lambda_{sin}\\\\sin(4\\\\pi t/\\mathcal{T})$\n    is the approximation for $\\Gamma^{\\\\uparrow} = LW^{\\\\uparrow} - LW^{\\\\downarrow} + LH^{\\\\uparrow} + SH^{\\\\uparrow}$.\n    * $\\mathcal{T}$ is the period i.e. one year.\n\n    The solution is exact if $\\lambda_{sq}=0$ and has the form:\n\n    * $T_0 = (F_0-2\\lambda_0)/\\sum_{i=1}^{N_{\\lambda}}\\lambda_i$\n    * $T_n = \\\\frac{F_n \\cos(\\\\varphi_n)\\sqrt{1+\\\\tan^2\\phi_n}}{\n    (2\\pi nfC - \\sum_i \\lambda_i \\sin \\Phi_{ni})\\\\tan\\phi_n + \\sum_i \\lambda_i \\cos \\Phi_{ni}}$\n    * $\\\\tan \\phi_n = \\\\frac{2\\pi nfC + \\\\tan \\\\varphi_n \\sum_i \\lambda_i \\cos \\Phi_{ni} -\n    \\sum_i \\lambda_i \\sin \\Phi_{ni}}{-2\\pi nfC \\\\tan \\\\varphi_n + \\sum_i \\lambda_i \\cos \\Phi_{ni} -\n    \\\\tan \\\\varphi_n \\sum_i \\lambda_i \\sin \\Phi_{ni}}$\n    * $i=1, 2$ with $\\lambda_1=\\lambda$ and $\\lambda_2=\\lambda_{phase}$.\n    * $\\Phi_{n1}=0$ and $\\Phi_{n2} = n\\pi/2$ (from $2n\\pi / \\mathcal{T} \\\\times \\mathcal{T}/4$).\n\n    If $\\lambda_{sq}\\\\neq 0$, an approximate analytical solution will be obtained, assuming\n    $T'^2(t) \\\\approx (T_1\\\\cos(2\\\\pi ft - \\\\phi_1))^2$.\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Time in days (assumes periodic e.g. annual mean, so `time = np.arange(360)`).\n        swdn_sfc: `float [n_time]`&lt;/br&gt;\n            Downward shortwave radiation at the surface, $SW^{\\\\downarrow}$. I.e. `swdn_sfc` output by Isca.\n            Units: $Wm^{-2}$.\n        heat_capacity: $C$, the heat capacity of the surface in units of $JK^{-1}m^{-2}$.&lt;/br&gt;\n            Obtained from mixed layer depth of ocean using\n            [`get_heat_capacity`](../utils/radiation.md#isca_tools.utils.radiation.get_heat_capacity).\n        lambda_const: The constant $\\lambda$ used in the approximation for\n            $\\Gamma^{\\\\uparrow} = LW^{\\\\uparrow} - LW^{\\\\downarrow} + LH^{\\\\uparrow} + SH^{\\\\uparrow}$.&lt;/br&gt;\n        lambda_phase: The constants $\\lambda_{phase}$ used in the approximation for $\\Gamma^{\\\\uparrow}$.\n        lambda_sq: The constant $\\lambda_{sq}$ used in the approximation for $\\Gamma^{\\\\uparrow}$.\n        lambda_cos: The constant $\\Lambda_{cos}$ used in the approximation for $\\Gamma^{\\\\uparrow}$.\n        lambda_sin: The constant $\\Lambda_{sin}$ used in the approximation for $\\Gamma^{\\\\uparrow}$.\n        n_harmonics_sw: Number of harmonics to use to fit fourier series for $SW^{\\\\downarrow}$.\n            Cannot exceed `n_harmonics_temp` as extra harmonics would not be used.\n        n_harmonics_temp: Number, $N$, of harmonics in fourier solution of temperature anomaly. If not given, will\n            set to `n_harmonics_sw`.\n        include_sw_phase: If `False`, will set all phase factors, $\\\\varphi_n=0$, in Fourier expansion of\n            $SW^{\\\\downarrow}$.&lt;/br&gt;\n            These phase factors are usually very small, and it makes the solution for $T'(t)$ more simple if they\n            are set to 0, hence the option.\n        day_seconds: Duration of a day in seconds.\n\n    Returns:\n        temp_fourier: `float [n_time]`&lt;/br&gt;\n            The Fourier series solution that was found for surface temperature, $T$.\n        temp_fourier_amp: `float [n_harmonics+1]`&lt;/br&gt;\n            The amplitude Fourier coefficients for surface temperature: $T_n$.\n        temp_fourier_phase: `float [n_harmonics]`&lt;/br&gt;\n            The phase Fourier coefficients for surface temperature: $\\phi_n$.\n        sw_fourier_amp: `float [n_harmonics+1]`&lt;/br&gt;\n            The amplitude Fourier coefficients for shortwave radiation, $SW^{\\\\downarrow}$: $F_n$.\n        sw_fourier_phase: `float [n_harmonics]`&lt;/br&gt;\n            The phase Fourier coefficients for shortwave radiation, $SW^{\\\\downarrow}$: $\\\\varphi_n$.\n    \"\"\"\n    n_year_days = len(time)\n    if n_harmonics_temp is None:\n        n_harmonics_temp = n_harmonics_sw\n\n    if n_harmonics_temp == 1 and lambda_sq != 0:\n        raise ValueError('Cannot solve for non-zero lambda_sq with single harmonic - '\n                         'use get_temp_fourier_numerical instead')\n\n    # Get fourier representation of SW radiation\n    if n_harmonics_sw &gt; n_harmonics_temp:\n        raise ValueError('Cannot have more harmonics for swdn_sfc than have for temperature')\n    sw_fourier_amp = np.zeros(n_harmonics_temp + 1)\n    sw_fourier_phase = np.zeros(n_harmonics_temp)\n    sw_fourier_amp[:n_harmonics_sw + 1], sw_fourier_phase[:n_harmonics_sw] = \\\n        fourier.get_fourier_fit(time, swdn_sfc, n_harmonics_sw)[1:]\n    if not include_sw_phase:\n        sw_fourier_phase = np.zeros(n_harmonics_temp)\n    sw_tan = np.tan(sw_fourier_phase)\n    sw_cos = np.cos(sw_fourier_phase)\n    sw_sin = np.sin(sw_fourier_phase)\n    if n_harmonics_temp &gt;= 2:\n        # For 2nd harmonic, have modification from cos and sin factors\n        sw_cos[1] = (sw_fourier_amp[2] * sw_cos[1] - lambda_cos) / sw_fourier_amp[2]\n        sw_sin[1] = (sw_fourier_amp[2] * sw_sin[1] - lambda_sin) / sw_fourier_amp[2]\n        sw_tan[1] = sw_sin[1] / sw_cos[1]\n    f = 1 / (n_year_days * day_seconds)  # must have frequency in units of s^{-1} to deal with phase stuff in radians\n\n    temp_fourier_amp = np.zeros(n_harmonics_temp + 1)  # 1st component will remain 0 so mean of temperature is 0\n    temp_fourier_phase = np.zeros(n_harmonics_temp)\n\n    # 0.25 multiplier accounts for the phase delay i.e. shift of 90 days in 360 day year - same as in polyfit_phase\n    phase_prefactor = 0.25\n    for n in range(1, n_harmonics_temp + 1):\n        lambda_term_cos = lambda_const\n        lambda_term_sin = 0.5 * lambda_phase * (np.sin(phase_prefactor * 2 * n * np.pi) -\n                                                np.sin(-phase_prefactor * 2 * n * np.pi))\n\n        temp_fourier_phase[n - 1] = np.arctan(\n            (2 * np.pi * n * f * heat_capacity + sw_tan[n - 1] * lambda_term_cos - lambda_term_sin) / (\n                    -2 * np.pi * n * f * heat_capacity * sw_tan[n - 1] + lambda_term_cos - sw_tan[n - 1] * lambda_term_sin))\n        temp_fourier_amp[n] = sw_fourier_amp[n] * sw_cos[n - 1] / np.cos(\n            temp_fourier_phase[n - 1]) / (\n                                      (2 * np.pi * n * f * heat_capacity - lambda_term_sin) * np.tan(\n                                  temp_fourier_phase[n - 1]) + lambda_term_cos)\n\n        if n == 1 and lambda_sq != 0:\n            if include_sw_phase:\n                raise ValueError('Solution not possible with sw phase and lambda_sq.')\n            # Get analytic solution when include squared term in energy budget\n            # Assuming squared term dominated by 1st harmonic\n            sw_tan[1] = (sw_fourier_amp[2] * sw_sin[1] -\n                         0.5 * lambda_sq * temp_fourier_amp[1] ** 2 * np.sin(2 * temp_fourier_phase[0])) / (\n                                sw_fourier_amp[2] * sw_cos[1] -\n                                0.5 * lambda_sq * temp_fourier_amp[1] ** 2 * np.cos(2 * temp_fourier_phase[0]))\n            if sw_fourier_amp[2] == 0:\n                sw_fourier_amp[2] = 1  # this term will cancel out, when do sw_fourier_amp[2]*sw_cos[1]\n                # so is just so don't divide by zero below - exact number not important\n            sw_cos[1] -= 0.5 * lambda_sq * temp_fourier_amp[1] ** 2 * np.cos(2 * temp_fourier_phase[0]\n                                                                             ) / sw_fourier_amp[2]\n    temp_fourier = fourier.fourier_series(time, temp_fourier_amp, temp_fourier_phase)\n    return temp_fourier, temp_fourier_amp, temp_fourier_phase, sw_fourier_amp, sw_fourier_phase\n</code></pre>"},{"location":"code/thesis/surface_energy_budget/#isca_tools.thesis.surface_energy_budget.get_temp_fourier_numerical","title":"<code>get_temp_fourier_numerical(time, temp_anom, gamma, swdn_sfc, heat_capacity, n_harmonics_sw=2, n_harmonics_temp=None, deg_gamma_fit=8, phase_gamma_fit=True, resample=False, gamma_fourier_term=False, include_sw_phase=False, day_seconds=86400)</code>","text":"<p>This uses <code>scipy.optimize.curve_fit</code> to numerically seek a fourier solution of the form \\(T'(t) = \\sum_{n=1}^{N} T_n\\cos(2n\\pi t/\\mathcal{T} - \\phi_n)\\) to the linearized surface energy budget of the general form:</p> \\[ \\begin{align} \\begin{split} C\\frac{\\partial T'}{\\partial t} = &amp;SW^{\\downarrow}(t) - \\lambda_0 - \\frac{1}{2}\\lambda_{phase}(T'(t-\\mathcal{T}/4) - T'(t+\\mathcal{T}/4)) -  \\\\ &amp;\\sum_{j=1}^{N_{\\Gamma}}\\lambda_j T'^{j}(t) - \\sum_{n=2}^N (\\Lambda_{n, cos}\\cos(2n\\pi t/\\mathcal{T}) + \\Lambda_{n, sin}\\sin(2n\\pi t/\\mathcal{T})) \\end{split} \\end{align} \\] <p>where:</p> <ul> <li>\\(T' = T(t) - \\overline{T}\\) is the surface temperature anomaly</li> <li>\\(C\\) is the heat capacity of the surface</li> <li>\\(\\overline{T} = T_0/2\\) is the mean temperature.</li> <li>\\(SW^{\\downarrow}\\) is the downward shortwave radiation at the surface, \\(SW^{\\downarrow}\\).</li> <li>\\(\\lambda_0 + \\frac{1}{2}\\lambda_{phase}(T'(t-\\mathcal{T}/4) - T'(t+\\mathcal{T}/4)) + \\sum_{j=1}^{N_{\\Gamma}}\\lambda_j T'^{j}(t) +\\) \\(\\sum_{n=2}^N (\\Lambda_{n, cos}\\cos(2n\\pi t/\\mathcal{T}) + \\Lambda_{n, sin}\\sin(2n\\pi t/\\mathcal{T}))\\) is the approximation for \\(\\Gamma^{\\uparrow} = LW^{\\uparrow} - LW^{\\downarrow} + LH^{\\uparrow} + SH^{\\uparrow}\\).</li> <li>\\(\\mathcal{T}\\) is the period i.e. one year.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>ndarray</code> <p><code>float [n_time]</code> Time in days (assumes periodic e.g. annual mean, so <code>time = np.arange(360)</code>).</p> required <code>temp_anom</code> <code>ndarray</code> <p><code>float [n_time]</code> Surface temperature anomaly, \\(T'(t)\\), for each day in <code>time</code>. Used for approximating \\(\\Gamma^{\\uparrow}\\). Assumes periodic so temp[0] is the temperature at time step immediately after <code>time[-1]</code>.</p> required <code>gamma</code> <code>ndarray</code> <p><code>float [n_time]</code> Simulated value of \\(\\Gamma^{\\uparrow} = LW^{\\uparrow} - LW^{\\downarrow} + LH^{\\uparrow} + SH^{\\uparrow}\\) where \\(LW\\) is longwave, \\(LH\\) is latent heat and \\(SH\\) is sensible heat. Units: \\(Wm^{-2}\\).</p> required <code>swdn_sfc</code> <code>ndarray</code> <p><code>float [n_time]</code> Downward shortwave radiation at the surface. Units: \\(Wm^{-2}\\).</p> required <code>heat_capacity</code> <code>float</code> <p>\\(C\\), the heat capacity of the surface in units of \\(JK^{-1}m^{-2}\\). Obtained from mixed layer depth of ocean using <code>get_heat_capacity</code>.</p> required <code>n_harmonics_sw</code> <code>int</code> <p>Number of harmonics to use to fit fourier series for \\(SW^{\\downarrow}\\). Cannot exceed <code>n_harmonics_temp</code> as extra harmonics would not be used. Set to None, to use no approximation for \\(SW^{\\downarrow}\\) - but we weary with comparing to analytic solution in this case.</p> <code>2</code> <code>n_harmonics_temp</code> <code>Optional[int]</code> <p>Number, \\(N\\), of harmonics in fourier solution of temperature anomaly. If not given, will set to <code>n_harmonics_sw</code>.</p> <code>None</code> <code>deg_gamma_fit</code> <code>int</code> <p>Power, \\(N_{\\Gamma}\\), to go up to in polyomial approximation of \\(\\Gamma^{\\uparrow}\\) seeked.</p> <code>8</code> <code>phase_gamma_fit</code> <code>bool</code> <p>If <code>False</code> will set \\(\\lambda_{phase}=0\\). Otherwise, will use <code>polyfit_phase</code> to estimate it.</p> <code>True</code> <code>resample</code> <code>bool</code> <p>If <code>True</code>, will use <code>resample_data</code> to make data evenly spaced in \\(x\\) before calling <code>np.polyfit</code>, when obtaining \\(\\Gamma\\) coefficients.</p> <code>False</code> <code>gamma_fourier_term</code> <code>bool</code> <p>Whether to fit the Fourier contribution \\(\\sum_{n=2}^N (\\Lambda_{n, cos}\\cos(2n\\pi t/\\mathcal{T}) + \\Lambda_{n, sin}\\sin(2n\\pi t/\\mathcal{T}))\\)  to \\(\\Gamma^{\\uparrow}\\) with <code>fourier_harmonics=np.arange(2, n_harmonics+1)</code> in <code>polyfit_phase</code>. Idea behind this is to account for contribution of \\(\\Gamma^{\\uparrow}\\) that is not temperature dependent.</p> <code>False</code> <code>include_sw_phase</code> <code>bool</code> <p>If <code>False</code>, will set all phase factors, \\(\\varphi_n=0\\), in Fourier expansion of \\(SW^{\\downarrow}\\). These phase factors are usually very small, and it makes the analytic solution for \\(T'(t)\\) more simple if they are set to 0, hence the option. Only use if <code>n_harmonics_sw</code> not <code>None</code>.</p> <code>False</code> <code>day_seconds</code> <code>float</code> <p>Duration of a day in seconds.</p> <code>86400</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <p>temp_fourier <code>float [n_time]</code> The Fourier series solution that was found for surface temperature anomaly. Units: \\(K\\).</p> <code>temp_fourier_amp</code> <code>ndarray</code> <p><code>float [n_harmonics+1]</code> The amplitude Fourier coefficients for surface temperature: \\(T_n\\).</p> <code>temp_fourier_phase</code> <code>ndarray</code> <p><code>float [n_harmonics]</code> The phase Fourier coefficients for surface temperature: \\(\\phi_n\\).</p> Source code in <code>isca_tools/thesis/surface_energy_budget.py</code> <pre><code>def get_temp_fourier_numerical(time: np.ndarray, temp_anom: np.ndarray, gamma: np.ndarray,\n                               swdn_sfc: np.ndarray, heat_capacity: float,\n                               n_harmonics_sw: int = 2, n_harmonics_temp: Optional[int] = None,\n                               deg_gamma_fit: int = 8, phase_gamma_fit: bool = True,\n                               resample: bool = False,\n                               gamma_fourier_term: bool = False,\n                               include_sw_phase: bool = False,\n                               day_seconds: float = 86400) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    This uses [`scipy.optimize.curve_fit`](\n    https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) to numerically seek\n    a fourier solution of the form $T'(t) = \\\\sum_{n=1}^{N} T_n\\\\cos(2n\\\\pi t/\\mathcal{T} - \\\\phi_n)$\n    to the linearized surface energy budget of the general form:\n\n    $$\n    \\\\begin{align}\n    \\\\begin{split}\n    C\\\\frac{\\partial T'}{\\partial t} = &amp;SW^{\\\\downarrow}(t) - \\lambda_0 -\n    \\\\frac{1}{2}\\lambda_{phase}(T'(t-\\mathcal{T}/4) - T'(t+\\mathcal{T}/4)) -  \\\\\\\\\n    &amp;\\\\sum_{j=1}^{N_{\\Gamma}}\\lambda_j T'^{j}(t) - \\\\sum_{n=2}^N (\\Lambda_{n, cos}\\\\cos(2n\\\\pi t/\\mathcal{T}) +\n    \\Lambda_{n, sin}\\\\sin(2n\\\\pi t/\\mathcal{T}))\n    \\\\end{split}\n    \\\\end{align}\n    $$\n\n    where:\n\n    * $T' = T(t) - \\overline{T}$ is the surface temperature anomaly\n    * $C$ is the heat capacity of the surface\n    * $\\overline{T} = T_0/2$ is the mean temperature.\n    * $SW^{\\downarrow}$ is the downward shortwave radiation at the surface, $SW^{\\downarrow}$.\n    * $\\lambda_0 + \\\\frac{1}{2}\\lambda_{phase}(T'(t-\\mathcal{T}/4) - T'(t+\\mathcal{T}/4)) +\n    \\\\sum_{j=1}^{N_{\\Gamma}}\\lambda_j T'^{j}(t) +$&lt;/br&gt;\n    $\\\\sum_{n=2}^N (\\Lambda_{n, cos}\\\\cos(2n\\\\pi t/\\mathcal{T}) + \\Lambda_{n, sin}\\\\sin(2n\\\\pi t/\\mathcal{T}))$\n    is the approximation for $\\Gamma^{\\\\uparrow} = LW^{\\\\uparrow} - LW^{\\\\downarrow} + LH^{\\\\uparrow} + SH^{\\\\uparrow}$.\n    * $\\mathcal{T}$ is the period i.e. one year.\n\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Time in days (assumes periodic e.g. annual mean, so `time = np.arange(360)`).\n        temp_anom: `float [n_time]`&lt;/br&gt;\n            Surface temperature anomaly, $T'(t)$, for each day in `time`. Used for approximating\n            $\\Gamma^{\\\\uparrow}$.&lt;/br&gt;\n            Assumes periodic so temp[0] is the temperature at time step immediately after `time[-1]`.\n        gamma: `float [n_time]`&lt;/br&gt;\n            Simulated value of $\\Gamma^{\\\\uparrow} = LW^{\\\\uparrow} - LW^{\\\\downarrow} + LH^{\\\\uparrow} + SH^{\\\\uparrow}$\n            where $LW$ is longwave, $LH$ is latent heat and $SH$ is sensible heat.&lt;/br&gt;\n            Units: $Wm^{-2}$.\n        swdn_sfc: `float [n_time]`&lt;/br&gt;\n            Downward shortwave radiation at the surface.&lt;/br&gt;\n            Units: $Wm^{-2}$.\n        heat_capacity: $C$, the heat capacity of the surface in units of $JK^{-1}m^{-2}$.&lt;/br&gt;\n            Obtained from mixed layer depth of ocean using\n            [`get_heat_capacity`](../utils/radiation.md#isca_tools.utils.radiation.get_heat_capacity).\n        n_harmonics_sw: Number of harmonics to use to fit fourier series for $SW^{\\\\downarrow}$.\n            Cannot exceed `n_harmonics_temp` as extra harmonics would not be used.&lt;/n&gt;\n            Set to None, to use no approximation for $SW^{\\\\downarrow}$ - but we weary with comparing to analytic\n            solution in this case.\n        n_harmonics_temp: Number, $N$, of harmonics in fourier solution of temperature anomaly. If not given, will\n            set to `n_harmonics_sw`.\n        deg_gamma_fit: Power, $N_{\\Gamma}$, to go up to in polyomial approximation of $\\Gamma^{\\\\uparrow}$ seeked.\n        phase_gamma_fit: If `False` will set $\\lambda_{phase}=0$.\n            Otherwise, will use [`polyfit_phase`](../utils/numerical.md#isca_tools.utils.numerical.polyfit_phase)\n            to estimate it.\n        resample: If `True`, will use [`resample_data`](../utils/numerical.md#isca_tools.utils.numerical.resample_data)\n            to make data evenly spaced in $x$ before calling `np.polyfit`, when obtaining $\\Gamma$ coefficients.\n        gamma_fourier_term: Whether to fit the Fourier contribution\n            $\\\\sum_{n=2}^N (\\Lambda_{n, cos}\\\\cos(2n\\\\pi t/\\mathcal{T}) + \\Lambda_{n, sin}\\\\sin(2n\\\\pi t/\\mathcal{T}))$\n             to $\\Gamma^{\\\\uparrow}$ with `fourier_harmonics=np.arange(2, n_harmonics+1)` in\n            [`polyfit_phase`](../utils/numerical.md#isca_tools.utils.numerical.polyfit_phase). Idea behind this\n            is to account for contribution of $\\Gamma^{\\\\uparrow}$ that is not temperature dependent.\n        include_sw_phase: If `False`, will set all phase factors, $\\\\varphi_n=0$, in Fourier expansion of\n            $SW^{\\\\downarrow}$.&lt;/br&gt;\n            These phase factors are usually very small, and it makes the analytic solution for $T'(t)$ more simple\n            if they are set to 0, hence the option. Only use if `n_harmonics_sw` not `None`.\n        day_seconds: Duration of a day in seconds.\n\n    Returns:\n        temp_fourier `float [n_time]`&lt;/br&gt;\n            The Fourier series solution that was found for surface temperature anomaly.&lt;/br&gt;\n            Units: $K$.\n        temp_fourier_amp: `float [n_harmonics+1]`&lt;/br&gt;\n            The amplitude Fourier coefficients for surface temperature: $T_n$.\n        temp_fourier_phase: `float [n_harmonics]`&lt;/br&gt;\n            The phase Fourier coefficients for surface temperature: $\\phi_n$.\n    \"\"\"\n    if n_harmonics_temp is None:\n        n_harmonics_temp = n_harmonics_sw\n    if gamma_fourier_term and phase_gamma_fit:\n        gamma_approx_coefs, gamma_fourier_term_coefs_amp, gamma_fourier_term_coefs_phase = \\\n            numerical.polyfit_phase(temp_anom, gamma, deg_gamma_fit, resample=resample,\n                                    include_phase=phase_gamma_fit, fourier_harmonics=np.arange(2, n_harmonics_temp+1))\n    else:\n        gamma_approx_coefs = numerical.polyfit_phase(temp_anom, gamma, deg_gamma_fit, resample=resample,\n                                                     include_phase=phase_gamma_fit)\n        gamma_fourier_term_coefs_amp = None\n        gamma_fourier_term_coefs_phase = None\n\n    def fit_func(time_array, *args):\n        # first n_harmonic values in args are the temperature amplitude coefficients (excluding T_0)\n        # last n_harmonic values in args are the temperature phase coefficients\n        fourier_amp_coef = np.asarray([args[i] for i in range(n_harmonics_temp)])\n        # make first coefficient 0 so gives anomaly\n        fourier_amp_coef = np.append([0], fourier_amp_coef)\n        fourier_phase_coef = np.asarray([args[i] for i in range(n_harmonics_temp, len(args))])\n        temp_anom_fourier = fourier.fourier_series(time_array, fourier_amp_coef, fourier_phase_coef)\n        dtemp_dt_fourier = fourier.fourier_series_deriv(time_array, fourier_amp_coef,\n                                                        fourier_phase_coef, day_seconds)\n        if phase_gamma_fit:\n            gamma_approx = numerical.polyval_phase(gamma_approx_coefs, temp_anom_fourier,\n                                                   coefs_fourier_amp=gamma_fourier_term_coefs_amp,\n                                                   coefs_fourier_phase=gamma_fourier_term_coefs_phase)\n        else:\n            gamma_approx = np.polyval(gamma_approx_coefs, temp_anom_fourier)\n        return heat_capacity * dtemp_dt_fourier + gamma_approx\n\n    # Starting guess is linear 1 harmonic linear analytical solution given gamma=lambda_const_guess*temp\n    # so only 1st harmonic coefficients of amplitude and phase are needed, rest are set to zero\n    p0 = np.zeros(2 * n_harmonics_temp)\n    f = 1 / (time.size * day_seconds)\n    p0[n_harmonics_temp] = np.arctan((2 * np.pi * f * heat_capacity) / gamma_approx_coefs[-2])\n    if n_harmonics_sw is None:\n        sw_fourier_amp = fourier.get_fourier_fit(time, swdn_sfc, 1)[1]\n        # find temperature solution which minimises error to full insolation, no fourier approx\n        sw_fourier_fit = swdn_sfc\n    else:\n        if include_sw_phase:\n            sw_fourier_fit, sw_fourier_amp = fourier.get_fourier_fit(time, swdn_sfc, n_harmonics_sw)[:2]\n        else:\n            sw_fourier_amp, sw_fourier_phase = fourier.get_fourier_fit(time, swdn_sfc, n_harmonics_sw)[1:]\n            sw_fourier_fit = fourier.fourier_series(time, sw_fourier_amp, sw_fourier_phase*0)\n    p0[0] = sw_fourier_amp[1] / np.cos(p0[n_harmonics_temp]) / (\n            (2 * np.pi * f * heat_capacity) * np.tan(p0[n_harmonics_temp]) + gamma_approx_coefs[-2])\n    args_found = optimize.curve_fit(fit_func, time, sw_fourier_fit, p0)[0]\n    temp_fourier_amp = np.append([0], args_found[:n_harmonics_temp])\n    temp_fourier_phase = args_found[n_harmonics_temp:]\n    return fourier.fourier_series(time, temp_fourier_amp, temp_fourier_phase), temp_fourier_amp, temp_fourier_phase\n</code></pre>"},{"location":"code/thesis/surface_energy_budget/#isca_tools.thesis.surface_energy_budget.swdn_from_temp_fourier","title":"<code>swdn_from_temp_fourier(time, temp_fourier_amp, temp_fourier_phase, heat_capacity, lambda_const, lambda_time_lag=None, lambda_nl=None, day_seconds=86400, single_harmonic_nl=False)</code>","text":"<p>OUTDATED FUNCTION - USED FOR <code>get_temp_fourier</code> but now use <code>get_temp_fourier_numerical</code></p> <p>This inverts the linearized surface energy budget to return an approximation for downward shortwave radiation at the surface, \\(F(t)\\), given a Fourier approximation for surface temperature, \\(T(t) = \\frac{T_0}{2} + \\sum_{n=1}^{N} T_n\\cos(2n\\pi ft - \\phi_n)\\):</p> \\[ F(t) \\approx C\\frac{\\partial T}{\\partial t} + \\lambda_0 + \\sum_{i=1}^{N_{\\lambda}}\\lambda_i T(t-\\Lambda_i) + \\sum_{j=2}^{j_{max}}\\lambda_{nl_j}\\left(T'^{j}(t) - \\overline{T'^j}\\right) \\] <p>Parameters:</p> Name Type Description Default <code>time</code> <code>ndarray</code> <p><code>float [n_time]</code> Time in days (assumes periodic e.g. annual mean, so <code>time = np.arange(360)</code>).</p> required <code>temp_fourier_amp</code> <code>ndarray</code> <p><code>float [n_harmonics+1]</code> The amplitude Fourier coefficients for surface temperature: \\(T_n\\).</p> required <code>temp_fourier_phase</code> <code>ndarray</code> <p><code>float [n_harmonics]</code> The phase Fourier coefficients for surface temperature: \\(\\phi_n\\).</p> required <code>heat_capacity</code> <code>float</code> <p>\\(C\\), the heat capacity of the surface in units of \\(JK^{-1}m^{-2}\\). Obtained from mixed layer depth of ocean using <code>get_heat_capacity</code>.</p> required <code>lambda_const</code> <code>ndarray</code> <p><code>float [n_lambda+1]</code> The constants \\(\\lambda_i\\) used in the approximation for \\(\\Gamma^{\\uparrow} = LW^{\\uparrow} - LW^{\\downarrow} + LH^{\\uparrow} + SH^{\\uparrow}\\). <code>lambda_const[0]</code> is \\(\\lambda_0\\) and <code>lambda_const[i]</code> is \\(\\lambda_{i}\\) for \\(i&gt;0\\).</p> required <code>lambda_time_lag</code> <code>Optional[ndarray]</code> <p><code>float [n_lambda]</code> The constants \\(\\Lambda_i\\) used in the approximation for \\(\\Gamma^{\\uparrow}\\). <code>lambda_time_lag[0]</code> is \\(\\Lambda_1\\) and <code>lambda_time_lag[i]</code> is \\(\\Lambda_{i+1}\\) for \\(i&gt;0\\).</p> <code>None</code> <code>lambda_nl</code> <code>Optional[Union[float, ndarray]]</code> <p><code>float [n_lambda_nl]</code> The constants \\(\\lambda_{nl}\\) used in the approximation for \\(\\Gamma^{\\uparrow}\\). <code>[0]</code> is squared contribution, <code>[1]</code> is cubed, ...</p> <code>None</code> <code>day_seconds</code> <code>float</code> <p>Duration of a day in seconds.</p> <code>86400</code> <code>single_harmonic_nl</code> <code>bool</code> <p>If <code>True</code>, the \\(\\lambda_{nl_j}T'^j\\) terms in \\(\\Gamma^{\\uparrow}\\) will only use the first harmonic, not all harmonics.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_time]</code> Approximation for downward shortwave radiation at the surface. Units: \\(Wm^{-2}\\).</p> Source code in <code>isca_tools/thesis/surface_energy_budget.py</code> <pre><code>def swdn_from_temp_fourier(time: np.ndarray, temp_fourier_amp: np.ndarray, temp_fourier_phase: np.ndarray,\n                           heat_capacity: float, lambda_const: np.ndarray,\n                           lambda_time_lag: Optional[np.ndarray] = None,\n                           lambda_nl: Optional[Union[float, np.ndarray]] = None,\n                           day_seconds: float = 86400, single_harmonic_nl: bool = False) -&gt; np.ndarray:\n    \"\"\"\n    OUTDATED FUNCTION - USED FOR `get_temp_fourier` but now use `get_temp_fourier_numerical`\n\n    This inverts the linearized surface energy budget to return an approximation for downward shortwave radiation\n    at the surface, $F(t)$, given a Fourier approximation for surface temperature,\n    $T(t) = \\\\frac{T_0}{2} + \\\\sum_{n=1}^{N} T_n\\\\cos(2n\\\\pi ft - \\\\phi_n)$:\n\n    $$\n    F(t) \\\\approx C\\\\frac{\\partial T}{\\partial t} + \\lambda_0 + \\sum_{i=1}^{N_{\\lambda}}\\lambda_i T(t-\\Lambda_i) +\n    \\sum_{j=2}^{j_{max}}\\lambda_{nl_j}\\\\left(T'^{j}(t) - \\overline{T'^j}\\\\right)\n    $$\n\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Time in days (assumes periodic e.g. annual mean, so `time = np.arange(360)`).\n        temp_fourier_amp: `float [n_harmonics+1]`&lt;/br&gt;\n            The amplitude Fourier coefficients for surface temperature: $T_n$.\n        temp_fourier_phase: `float [n_harmonics]`&lt;/br&gt;\n            The phase Fourier coefficients for surface temperature: $\\phi_n$.\n        heat_capacity: $C$, the heat capacity of the surface in units of $JK^{-1}m^{-2}$.&lt;/br&gt;\n            Obtained from mixed layer depth of ocean using\n            [`get_heat_capacity`](../utils/radiation.md#isca_tools.utils.radiation.get_heat_capacity).\n        lambda_const: `float [n_lambda+1]`&lt;/br&gt;\n            The constants $\\lambda_i$ used in the approximation for\n            $\\Gamma^{\\\\uparrow} = LW^{\\\\uparrow} - LW^{\\\\downarrow} + LH^{\\\\uparrow} + SH^{\\\\uparrow}$.&lt;/br&gt;\n            `lambda_const[0]` is $\\lambda_0$ and `lambda_const[i]` is $\\lambda_{i}$ for $i&gt;0$.\n        lambda_time_lag: `float [n_lambda]`&lt;/br&gt;\n            The constants $\\Lambda_i$ used in the approximation for $\\Gamma^{\\\\uparrow}$.&lt;/br&gt;\n            `lambda_time_lag[0]` is $\\Lambda_1$ and `lambda_time_lag[i]` is $\\Lambda_{i+1}$ for $i&gt;0$.\n        lambda_nl: `float [n_lambda_nl]`\n            The constants $\\lambda_{nl}$ used in the approximation for $\\Gamma^{\\\\uparrow}$.\n            `[0]` is squared contribution, `[1]` is cubed, ...\n        day_seconds: Duration of a day in seconds.\n        single_harmonic_nl: If `True`, the $\\lambda_{nl_j}T'^j$ terms in $\\Gamma^{\\\\uparrow}$ will only\n            use the first harmonic, not all harmonics.\n\n    Returns:\n        `float [n_time]`&lt;/br&gt;\n            Approximation for downward shortwave radiation at the surface.\n            Units: $Wm^{-2}$.\n    \"\"\"\n    if lambda_nl is not None:\n        #  deal with case when float given as lambda_nl\n        if not hasattr(lambda_nl, \"__len__\"):\n            lambda_nl = np.asarray([lambda_nl])\n    n_year_days = len(time)\n    temp_fourier = fourier.fourier_series(time, temp_fourier_amp, temp_fourier_phase)\n    if single_harmonic_nl:\n        temp_anom_nl = fourier.fourier_series(time, [0, temp_fourier_amp[1]],\n                                              [temp_fourier_phase[0]])\n        gamma = gamma_linear_approx(time, temp_fourier, lambda_const,\n                                    lambda_time_lag, lambda_nl, temp_anom_nl)\n    else:\n        gamma = gamma_linear_approx(time, temp_fourier, lambda_const,\n                                    lambda_time_lag, lambda_nl)\n    dtemp_dt = fourier.fourier_series_deriv(time, temp_fourier_amp, temp_fourier_phase, day_seconds)\n    return heat_capacity * dtemp_dt + gamma\n</code></pre>"},{"location":"code/utils/base/","title":"Base","text":""},{"location":"code/utils/base/#isca_tools.utils.base.area_weighting","title":"<code>area_weighting(var)</code>","text":"<p>Apply area weighting to the variable <code>var</code> using the <code>cosine</code> of latitude: \\(\\cos (\\phi)\\).</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>DataArray</code> <p>Variable to weight e.g. <code>ds.t_surf</code> to weight the surface temperature, where <code>ds</code> is the dataset for the experiment which contains all variables.</p> required <p>Returns:</p> Type Description <code>DataArrayWeighted</code> <p>Area weighted version of <code>var</code>.</p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def area_weighting(var: xr.DataArray) -&gt; DataArrayWeighted:\n    \"\"\"\n    Apply area weighting to the variable `var` using the `cosine` of latitude: $\\cos (\\phi)$.\n\n    Args:\n        var: Variable to weight e.g. `ds.t_surf` to weight the surface temperature, where\n            `ds` is the dataset for the experiment which contains all variables.\n\n    Returns:\n        Area weighted version of `var`.\n    \"\"\"\n    weights = np.cos(np.deg2rad(var.lat))\n    weights.name = \"weights\"\n    return var.weighted(weights)\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.dp_from_pressure","title":"<code>dp_from_pressure(p, dim='lev')</code>","text":"<p>Compute layer pressure thickness \u0394p, preserving extra dims and coord order.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>DataArray</code> <p>Pressure [Pa], with vertical dimension <code>dim</code> (n_lev). Can include other dims (e.g., time, lat, lon).</p> required <code>dim</code> <code>str</code> <p>Name of the vertical coordinate.</p> <code>'lev'</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>xr.DataArray: Pressure thickness \u0394p [Pa], same shape and coords as <code>p</code>.</p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def dp_from_pressure(p: xr.DataArray, dim: str = \"lev\") -&gt; xr.DataArray:\n    \"\"\"Compute layer pressure thickness \u0394p, preserving extra dims and coord order.\n\n    Args:\n        p: Pressure [Pa], with vertical dimension `dim` (n_lev).\n            Can include other dims (e.g., time, lat, lon).\n        dim: Name of the vertical coordinate.\n\n    Returns:\n        xr.DataArray: Pressure thickness \u0394p [Pa], same shape and coords as `p`.\n    \"\"\"\n\n    def _dp_1d(p_1d: np.ndarray) -&gt; np.ndarray:\n        # ensure increasing order (bottom\u2192top) for calculation\n        reversed_flag = p_1d[0] &lt; p_1d[-1]\n        if reversed_flag:\n            p_1d = p_1d[::-1]\n\n        # edges &amp; dp calculation\n        p_edge_mid = 0.5 * (p_1d[:-1] + p_1d[1:])\n        p_edge_bot = p_1d[0] + 0.5 * (p_1d[0] - p_1d[1])\n        p_edge_top = p_1d[-1] - 0.5 * (p_1d[-2] - p_1d[-1])\n        p_edges = np.concatenate([[p_edge_bot], p_edge_mid, [p_edge_top]])\n        dp = p_edges[:-1] - p_edges[1:]\n        dp = np.abs(dp)  # ensure positive\n\n        # if we reversed order, un-reverse result to match original orientation\n        if reversed_flag:\n            dp = dp[::-1]\n        return dp\n\n    dp = xr.apply_ufunc(\n        _dp_1d,\n        p,\n        input_core_dims=[[dim]],\n        output_core_dims=[[dim]],\n        vectorize=True,\n        dask=\"parallelized\",\n        output_dtypes=[float],\n    )\n\n    dp.name = \"dp\"\n    dp.attrs.update({\"long_name\": \"pressure thickness\", \"units\": \"Pa\"})\n    return dp\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.get_memory_usage","title":"<code>get_memory_usage()</code>","text":"<p>Get current process\u2019s memory in MB.</p> <p>Returns:</p> Name Type Description <code>mem_mb</code> <code>float</code> <p>Memory usage in MB</p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def get_memory_usage() -&gt; float:\n    \"\"\"\n    Get current process\u2019s memory in MB.\n\n    Returns:\n        mem_mb: Memory usage in MB\n    \"\"\"\n    process = psutil.Process(os.getpid())\n    mem_mb = process.memory_info().rss / (1024 * 1024)\n    return mem_mb\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.has_out_of_range","title":"<code>has_out_of_range(val, min_range, max_range)</code>","text":"<p>Check if any number within <code>val</code> is outside the range between <code>min_range</code> and <code>max_range</code>.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>Union[List, Tuple, ndarray, float]</code> <p>Numbers to check</p> required <code>min_range</code> <code>float</code> <p>Minimum allowed value.</p> required <code>max_range</code> <code>float</code> <p>Maximum allowed value.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if there is a value outside the range between <code>min_range</code> and <code>max_range</code>.</p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def has_out_of_range(val: Union[List, Tuple, np.ndarray, float], min_range: float, max_range: float) -&gt; bool:\n    \"\"\"\n    Check if any number within `val` is outside the range between `min_range` and `max_range`.\n\n    Args:\n        val: Numbers to check\n        min_range: Minimum allowed value.\n        max_range: Maximum allowed value.\n\n    Returns:\n        True if there is a value outside the range between `min_range` and `max_range`.\n    \"\"\"\n    # If it's a single number, make it a list\n    vals = val if isinstance(val, (list, tuple, np.ndarray)) else [val]\n    return any((x &lt; min_range or x &gt; max_range) for x in vals)\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.insert_to_array","title":"<code>insert_to_array(x_values, y_values, x_new, y_new)</code>","text":"<p>Insert multiple (x, y) pairs into arrays while preserving the sort order of x (ascending or descending).</p> <p>Works for both NumPy arrays and xarray.DataArray objects.</p> <p>Parameters:</p> Name Type Description Default <code>x_values</code> <code>Union[ndarray, DataArray]</code> <p>Array of x-values (must be sorted, ascending or descending).</p> required <code>y_values</code> <code>Union[ndarray, DataArray]</code> <p>Array of corresponding y-values.</p> required <code>x_new</code> <code>Union[ndarray, DataArray, List, float]</code> <p>New x-values to insert.</p> required <code>y_new</code> <code>Union[ndarray, DataArray, List, float]</code> <p>Corresponding y-values to insert.</p> required <p>Returns:</p> Name Type Description <code>x_updated</code> <code>Union[ndarray, DataArray]</code> <p><code>x_values</code> with <code>x_new</code> inserted in correct location.</p> <code>y_updated</code> <code>Union[ndarray, DataArray]</code> <p><code>y_values</code> with <code>y_new</code> inserted in correct location.</p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def insert_to_array(x_values: Union[np.ndarray, xr.DataArray], y_values: Union[np.ndarray, xr.DataArray],\n                    x_new: Union[np.ndarray, xr.DataArray, List, float], y_new: Union[np.ndarray, xr.DataArray, List, float]\n                    ) -&gt; tuple[Union[np.ndarray, xr.DataArray], Union[np.ndarray, xr.DataArray]]:\n    \"\"\"Insert multiple (x, y) pairs into arrays while preserving the sort order of x (ascending or descending).\n\n    Works for both NumPy arrays and xarray.DataArray objects.\n\n    Args:\n        x_values: Array of x-values (must be sorted, ascending or descending).\n        y_values: Array of corresponding y-values.\n        x_new: New x-values to insert.\n        y_new: Corresponding y-values to insert.\n\n    Returns:\n        x_updated: `x_values` with `x_new` inserted in correct location.\n        y_updated: `y_values` with `y_new` inserted in correct location.\n    \"\"\"\n    # Extract data if xarray\n    x_is_xr = isinstance(x_values, xr.DataArray)\n    y_is_xr = isinstance(y_values, xr.DataArray)\n\n    x_data = x_values.data if x_is_xr else np.asarray(x_values)\n    y_data = y_values.data if y_is_xr else np.asarray(y_values)\n    x_new = np.atleast_1d(x_new)\n    y_new = np.atleast_1d(y_new)\n\n    # Determine if x_values are ascending or descending\n    ascending = x_data[0] &lt; x_data[-1]\n\n    # If descending, temporarily flip for insertion logic\n    if not ascending:\n        x_data = x_data[::-1]\n        y_data = y_data[::-1]\n        x_new = -x_new\n        x_data = -x_data\n\n    # Sort new inputs by x_new\n    sort_idx = np.argsort(x_new)\n    x_new = x_new[sort_idx]\n    y_new = y_new[sort_idx]\n\n    # Find insertion indices and insert\n    insert_indices = np.searchsorted(x_data, x_new)\n    x_combined = np.insert(x_data, insert_indices, x_new)\n    y_combined = np.insert(y_data, insert_indices, y_new)\n\n    # Flip back if descending\n    if not ascending:\n        x_combined = -x_combined[::-1]\n        y_combined = y_combined[::-1]\n\n    # Wrap back into xarray if needed\n    if x_is_xr or y_is_xr:\n        # Try to preserve dimension and coordinate naming\n        dim = x_values.dims[0] if x_is_xr else (y_values.dims[0] if y_is_xr else \"dim_0\")\n        x_combined = xr.DataArray(x_combined, dims=[dim], name=x_values.name if x_is_xr else None)\n        y_combined = xr.DataArray(y_combined, dims=[dim], name=y_values.name if y_is_xr else None)\n\n        # Rebuild coordinates if x-values represent coordinate axis\n        x_combined = x_combined.assign_coords({dim: x_combined})\n\n    return x_combined, y_combined\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.len_safe","title":"<code>len_safe(x)</code>","text":"<p>Return length of <code>x</code> which can have multiple values, or just be a number.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Variable to return length of.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of elements in <code>x</code>.</p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def len_safe(x) -&gt; int:\n    \"\"\"\n    Return length of `x` which can have multiple values, or just be a number.\n\n    Args:\n        x: Variable to return length of.\n\n    Returns:\n        Number of elements in `x`.\n    \"\"\"\n    if isinstance(x, numbers.Number):\n        return 1\n    try:\n        return len(x)\n    except TypeError:\n        raise TypeError(f\"Unsupported type with no length: {type(x)}\")\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.parse_int_list","title":"<code>parse_int_list(value, format_func=lambda x: str(x), all_values=None)</code>","text":"<p>Takes in a value or list of values e.g. <code>[1, 2, 3]</code> and converts it into a list of strings where each string has the format given by <code>format_func</code> e.g. <code>['1', '2', '3']</code> for the default case.</p> <p>There are three string options for <code>value</code>: * <code>value='x:y'</code>, will return all integers between <code>x</code> and <code>y</code> inclusive. * <code>value='firstX'</code> will return first X values of <code>all_values</code>. * <code>value='firstY'</code> will return first Y values of <code>all_values</code>.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, List]</code> <p>Variable to convert into list of strings</p> required <code>format_func</code> <code>Callable</code> <p>How to format each integer within the string.</p> <code>lambda x: str(x)</code> <code>all_values</code> <code>Optional[List]</code> <p>List of all possible integers, must be provided if <code>value='firstX'</code> or <code>value='firstY'</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List</code> <p>List, where each integer in <code>value</code> is converted using <code>format_func</code>.</p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def parse_int_list(value: Union[str, int, List], format_func: Callable = lambda x: str(x),\n                   all_values: Optional[List] = None) -&gt; List:\n    \"\"\"\n    Takes in a value or list of values e.g. `[1, 2, 3]` and converts it into a list of strings where\n    each string has the format given by `format_func` e.g. `['1', '2', '3']` for the default case.\n\n    There are three string options for `value`:\n    * `value='x:y'`, will return all integers between `x` and `y` inclusive.\n    * `value='firstX'` will return first X values of `all_values`.\n    * `value='firstY'` will return first Y values of `all_values`.\n\n    Args:\n        value: Variable to convert into list of strings\n        format_func: How to format each integer within the string.\n        all_values: List of all possible integers, must be provided if `value='firstX'` or `value='firstY'`.\n\n    Returns:\n        List, where each integer in `value` is converted using `format_func`.\n    \"\"\"\n    if isinstance(value, list):\n        pass\n    elif isinstance(value, int):\n        value = [value]\n    elif isinstance(value, str):\n        value = value.strip()  # remove blank space\n        # Can specify just first or last n years\n        if re.search(r'^first(\\d+)', value):\n            if all_values is None:\n                raise ValueError(f'With value={value}, must provide all_values')\n            n_req = int(re.search(r'^first(\\d+)', value).group(1))\n            if n_req &gt; len(all_values):\n                warnings.warn(f\"Requested {value} but there are only \"\n                              f\"{len(all_values)} available:\\n{all_values}\")\n            value = all_values[:n_req]\n        elif re.search(r'^last(\\d+)', value):\n            if all_values is None:\n                raise ValueError(f'With value={value}, must provide all_values')\n            n_req = int(re.search(r'^last(\\d+)', all_values).group(1))\n            if n_req &gt; len(all_values):\n                warnings.warn(f\"Requested {value} but there are only \"\n                              f\"{len(all_values)} available:\\n{all_values}\")\n            value = all_values[-n_req:]\n        elif ':' in value:\n            # If '1979:2023' returns all integers from 1979 to 2023\n            start, end = map(int, value.split(':'))\n            value = list(range(start, end + 1))\n        else:\n            value = [int(value)]\n    else:\n        raise ValueError(f\"Unsupported format: {value}\")\n    return [format_func(i) for i in value]\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.print_log","title":"<code>print_log(text, logger=None)</code>","text":"<p>Quick function to add to log if log exists, otherwise print it.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be printed.</p> required <code>logger</code> <code>Optional[Logger]</code> <code>None</code> <p>Returns:</p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def print_log(text: str, logger: Optional[logging.Logger] = None) -&gt; None:\n    \"\"\"\n    Quick function to add to log if log exists, otherwise print it.\n\n    Args:\n        text: Text to be printed.\n        logger:\n\n    Returns:\n\n    \"\"\"\n    logger.info(text) if logger else print(text)\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.round_any","title":"<code>round_any(x, base, round_type='round')</code>","text":"<p>Rounds <code>x</code> to the nearest multiple of <code>base</code> with the rounding done according to <code>round_type</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[float, ndarray]</code> <p>Number or array to round.</p> required <code>base</code> <code>float</code> <p>Rounds <code>x</code> to nearest integer multiple of value of <code>base</code>.</p> required <code>round_type</code> <code>str</code> <p>One of the following, indicating how to round <code>x</code> -</p> <ul> <li><code>'round'</code></li> <li><code>'ceil'</code></li> <li><code>'float'</code></li> </ul> <code>'round'</code> <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Rounded version of <code>x</code>.</p> Example <pre><code>round_any(3, 5) = 5\nround_any(3, 5, 'floor') = 0\n</code></pre> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def round_any(x: Union[float, np.ndarray], base: float, round_type: str = 'round') -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Rounds `x` to the nearest multiple of `base` with the rounding done according to `round_type`.\n\n    Args:\n        x: Number or array to round.\n        base: Rounds `x` to nearest integer multiple of value of `base`.\n        round_type: One of the following, indicating how to round `x` -\n\n            - `'round'`\n            - `'ceil'`\n            - `'float'`\n\n    Returns:\n        Rounded version of `x`.\n\n    Example:\n        ```\n        round_any(3, 5) = 5\n        round_any(3, 5, 'floor') = 0\n        ```\n    \"\"\"\n    if round_type == 'round':\n        return base * np.round(x / base)\n    elif round_type == 'ceil':\n        return base * np.ceil(x / base)\n    elif round_type == 'floor':\n        return base * np.floor(x / base)\n    else:\n        raise ValueError(f\"round_type specified was {round_type} but it should be one of the following:\\n\"\n                         f\"round, ceil, floor\")\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.run_func_loop","title":"<code>run_func_loop(func, max_wait_time=300, wait_interval=20, func_check=None, logger=None)</code>","text":"<p>Safe way to run a function, such that if hit error, will try again every <code>wait_interval</code> seconds up to a maximum of <code>max_wait_time</code> seconds.</p> <p>If <code>func_check</code> is given and returns <code>True</code> at any point, it will exit the loop without executing <code>func</code>.</p> <p>Most obvious usage is for creating a directory e.g. <code>os.makedirs</code>, especially to a server where connection cuts in and out.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to run. Must have no arguments.</p> required <code>max_wait_time</code> <code>int</code> <p>Maximum number of seconds to try and run <code>func</code>.</p> <code>300</code> <code>wait_interval</code> <code>int</code> <p>Interval in seconds to wait between running <code>func</code>.</p> <code>20</code> <code>func_check</code> <code>Optional[Callable]</code> <p>Function that returns a boolean. If it returns <code>True</code> at any point, the loop will exit the loop without executing <code>func</code>.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Logger to record information</p> <code>None</code> <p>Returns:</p> Type Description <p>Whatever <code>func</code> returns.</p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def run_func_loop(func: Callable, max_wait_time: int = 300, wait_interval: int = 20,\n                  func_check: Optional[Callable] = None, logger: Optional[logging.Logger] = None):\n    \"\"\"\n    Safe way to run a function, such that if hit error, will try again every `wait_interval` seconds up to a\n    maximum of `max_wait_time` seconds.\n\n    If `func_check` is given and returns `True` at any point, it will exit the loop without executing `func`.\n\n    Most obvious usage is for creating a directory e.g. `os.makedirs`, especially to a server where connection\n    cuts in and out.\n\n    Args:\n        func: Function to run. Must have no arguments.\n        max_wait_time: Maximum number of seconds to try and run `func`.\n        wait_interval: Interval in seconds to wait between running `func`.\n        func_check: Function that returns a boolean. If it returns `True` at any point, the loop\n            will exit the loop without executing `func`.\n        logger: Logger to record information\n\n    Returns:\n        Whatever `func` returns.\n    \"\"\"\n    i = 0\n    j = 0\n    success = False\n    start_time = time.time()\n    output = None\n    while not success and (time.time() - start_time) &lt; max_wait_time:\n        if func_check is not None:\n            if func_check():\n                print_log(\"func_check passed so did not exectute func\", logger)\n                success = True\n                break\n        try:\n            output = func()\n            success = True\n        except PermissionError as e:\n            i += 1\n            if i == 1:\n                # Only print on first instance of error\n                print_log(f'Permission Error: {e}', logger)\n            time.sleep(wait_interval)\n        except Exception as e:\n            j += 1\n            if j == 1:\n                # Only print on first instance of error\n                print_log(f'Unexpected Error: {e}', logger)\n            time.sleep(wait_interval)\n    if not success:\n        raise ValueError(f\"Making output directory - Failed to run function after {max_wait_time} seconds.\")\n    return output\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.split_list_max_n","title":"<code>split_list_max_n(lst, n)</code>","text":"<p>Split <code>lst</code> into balanced chunks with at most <code>n</code> elements each.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>Union[List, ndarray]</code> <p>List to split.</p> required <code>n</code> <code>int</code> <p>Maximum number of elements in each chunk of <code>lst</code>.</p> required <p>Returns:</p> Type Description <code>List</code> <p>List of <code>n</code> chunks of <code>lst</code></p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def split_list_max_n(lst: Union[List, np.ndarray], n: int) -&gt; List:\n    \"\"\"\n    Split `lst` into balanced chunks with at most `n` elements each.\n\n    Args:\n        lst: List to split.\n        n: Maximum number of elements in each chunk of `lst`.\n\n    Returns:\n        List of `n` chunks of `lst`\n    \"\"\"\n    k = int(np.ceil(len(lst) / n))  # Number of chunks needed\n    avg = int(np.ceil(len(lst) / k))\n    return [lst[i * avg: (i + 1) * avg] for i in range(k)]\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.top_n_peaks_ind","title":"<code>top_n_peaks_ind(var, n=1, min_ind_spacing=0)</code>","text":"<p>Return the indices of the N largest values of <code>var</code>, such that the indices of these values  are \u2265<code>min_ind_spacing</code> apart.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>ndarray</code> <p>1D array containing variable values. Assumed in an order</p> required <code>n</code> <code>int</code> <p>Number of peaks to select.</p> <code>1</code> <code>min_ind_spacing</code> <code>int</code> <p>Minimum index spacing between selected peaks.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Indices of <code>n</code> peak values of <code>var</code>.</p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def top_n_peaks_ind(\n        var: np.ndarray,\n        n: int = 1,\n        min_ind_spacing: int = 0,\n) -&gt; np.ndarray:\n    \"\"\"Return the indices of the N largest values of `var`, such that the indices of these values\n     are \u2265`min_ind_spacing` apart.\n\n    Args:\n        var: 1D array containing variable values. Assumed in an order\n        n: Number of peaks to select.\n        min_ind_spacing: Minimum index spacing between selected peaks.\n\n    Returns:\n        Indices of `n` peak values of `var`.\n    \"\"\"\n    # Sort indices by descending value of var\n    order = np.argsort(var)[::-1]\n    selected_ind = []\n\n    for i in order:\n        # Check spacing constraint\n        if all(abs(i - s) &gt;= min_ind_spacing for s in selected_ind):\n            selected_ind.append(i)\n            if len(selected_ind) == n:\n                break\n\n    return np.array(selected_ind, dtype=int)\n</code></pre>"},{"location":"code/utils/base/#isca_tools.utils.base.weighted_RMS","title":"<code>weighted_RMS(var, weight=None, dim=None)</code>","text":"<p>Compute (weighted) RMS of a DataArray or numpy array along specified dimension(s).</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Union[DataArray, ndarray]</code> <p>Variable to compute RMS for (shape [...]).</p> required <code>weight</code> <code>Optional[Union[DataArray, ndarray]]</code> <p>Weights (same shape as var along <code>dim</code>). If None, computes unweighted RMS.</p> <code>None</code> <code>dim</code> <code>Optional[Union[str, int, List[Union[str, int]]]]</code> <p>Dimension(s) to reduce over. - For xarray: names of dimensions. - For numpy: integer axis or list of axes.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rms</code> <code>Union[DataArray, ndarray]</code> <p>Same type as input, reduced along <code>dim</code>.</p> Source code in <code>isca_tools/utils/base.py</code> <pre><code>def weighted_RMS(\n        var: Union[xr.DataArray, np.ndarray],\n        weight: Optional[Union[xr.DataArray, np.ndarray]] = None,\n        dim: Optional[Union[str, int, List[Union[str, int]]]] = None\n) -&gt; Union[xr.DataArray, np.ndarray]:\n    \"\"\"\n    Compute (weighted) RMS of a DataArray or numpy array along specified dimension(s).\n\n    Args:\n        var: Variable to compute RMS for (shape [...]).\n        weight: Weights (same shape as var along `dim`).\n            If None, computes unweighted RMS.\n        dim: Dimension(s) to reduce over.\n            - For xarray: names of dimensions.\n            - For numpy: integer axis or list of axes.\n\n    Returns:\n        rms: Same type as input, reduced along `dim`.\n    \"\"\"\n\n    # --- Handle dim input uniformly ---\n    if isinstance(dim, (str, int)):\n        dims = [dim]\n    elif dim is None:\n        # all dims\n        if isinstance(var, xr.DataArray):\n            dims = list(var.dims)\n        else:\n            dims = list(range(var.ndim))\n    else:\n        dims = dim\n\n    # --- xarray branch ---\n    if isinstance(var, xr.DataArray):\n        if weight is None:\n            rms_sq = (var ** 2).mean(dim=dims)\n        else:\n            rms_sq = ((var ** 2) * weight).sum(dim=dims) / weight.sum(dim=dims)\n        return np.sqrt(rms_sq)\n\n    # --- numpy branch ---\n    else:\n        if weight is None:\n            rms_sq = np.nanmean(var ** 2, axis=tuple(dims))\n        else:\n            rms_sq = np.nansum((var ** 2) * weight, axis=tuple(dims)) / np.nansum(weight, axis=tuple(dims))\n        return np.sqrt(rms_sq)\n</code></pre>"},{"location":"code/utils/calculus/","title":"Calculus","text":""},{"location":"code/utils/calculus/#isca_tools.utils.calculus.divergence_2d","title":"<code>divergence_2d(var_x, var_y, lat, lon, lat_axis=-2, lon_axis=-1)</code>","text":"<p>Calculate the 2D divergence of a vector field with the given x and y components.</p> <p>Parameters:</p> Name Type Description Default <code>var_x</code> <code>ndarray</code> <p>x component of the vector e.g. u for wind. Typical shape is <code>[n_time x n_pressure x n_lat x n_lon]</code>, but can accept all different shapes as long as lat and lon are included.</p> required <code>var_y</code> <code>ndarray</code> <p>y component of the vector e.g. v for wind. Typical shape is <code>[n_time x n_pressure x n_lat x n_lon]</code>, but can accept all different shapes as long as lat and lon are included.</p> required <code>lat</code> <code>ndarray</code> <p><code>float [n_lat]</code> Latitude coordinates in degrees.</p> required <code>lon</code> <code>ndarray</code> <p><code>float [n_lon]</code> Longitude coordinates in degrees.</p> required <code>lat_axis</code> <code>int</code> <p>Axis of var_x and var_y which corresponds to latitude.</p> <code>-2</code> <code>lon_axis</code> <code>int</code> <p>Axis of var_x and var_y which corresponds to longitude.</p> <code>-1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Divergence of var with same shape as <code>var_x</code> and <code>var_y</code>. Units are those of <code>var</code> multiplied by \\(m^{-1}\\).</p> Source code in <code>isca_tools/utils/calculus.py</code> <pre><code>def divergence_2d(var_x: np.ndarray, var_y: np.ndarray, lat: np.ndarray, lon: np.ndarray, lat_axis: int = -2,\n                  lon_axis: int = -1) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the 2D divergence of a vector field with the given x and y components.\n\n    Args:\n        var_x: x component of the vector e.g. u for wind. Typical shape is `[n_time x n_pressure x n_lat x n_lon]`,\n            but can accept all different shapes as long as lat and lon are included.\n        var_y: y component of the vector e.g. v for wind. Typical shape is `[n_time x n_pressure x n_lat x n_lon]`,\n            but can accept all different shapes as long as lat and lon are included.\n        lat: `float [n_lat]`\n            Latitude coordinates in degrees.\n        lon: `float [n_lon]`\n            Longitude coordinates in degrees.\n        lat_axis: Axis of var_x and var_y which corresponds to latitude.\n        lon_axis: Axis of var_x and var_y which corresponds to longitude.\n\n    Returns:\n        Divergence of var with same shape as `var_x` and `var_y`. Units are those of `var` multiplied by $m^{-1}$.\n    \"\"\"\n    if var_x.shape != var_y.shape:\n        raise ValueError('var_x and var_y should have the same shapes')\n    var_shape = np.ones(len(var_x.shape), dtype=int)\n    var_shape[lat_axis] = len(lat)\n    cos_lat = np.asarray(np.cos(np.deg2rad(lat))).reshape(var_shape)\n    div_x = np.gradient(var_x, np.deg2rad(lon), axis=lon_axis) / (radius_earth * cos_lat)\n    div_y = np.gradient(var_y * cos_lat, np.deg2rad(lat), axis=lat_axis) / (radius_earth * cos_lat)\n    return div_x + div_y\n</code></pre>"},{"location":"code/utils/calculus/#isca_tools.utils.calculus.grad_x","title":"<code>grad_x(var, lat, lon, lat_axis=-2, lon_axis=-1)</code>","text":"<p>Finds the gradient in the x direction of a scalar field.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>ndarray</code> <p>Scalar field to find gradient of. Typical shape is <code>[n_time x n_pressure x n_lat x n_lon]</code>, but can accept all different shapes as long as lat and lon are included.</p> required <code>lat</code> <code>ndarray</code> <p><code>float [n_lat]</code> Latitude coordinates in degrees.</p> required <code>lon</code> <code>ndarray</code> <p><code>float [n_lon]</code> Longitude coordinates in degrees.</p> required <code>lat_axis</code> <code>int</code> <p>Axis of var_x and var_y which corresponds to latitude.</p> <code>-2</code> <code>lon_axis</code> <code>int</code> <p>Axis of var_x and var_y which corresponds to longitude.</p> <code>-1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Gradient of <code>var</code> in x direction with same shape as <code>var</code>. Units are those of <code>var</code> multiplied by \\(m^{-1}\\).</p> Source code in <code>isca_tools/utils/calculus.py</code> <pre><code>def grad_x(var: np.ndarray, lat: np.ndarray, lon: np.ndarray, lat_axis: int = -2, lon_axis: int = -1) -&gt; np.ndarray:\n    \"\"\"\n    Finds the gradient in the x direction of a scalar field.\n\n    Args:\n        var: Scalar field to find gradient of. Typical shape is `[n_time x n_pressure x n_lat x n_lon]`,\n            but can accept all different shapes as long as lat and lon are included.\n        lat: `float [n_lat]`\n            Latitude coordinates in degrees.\n        lon: `float [n_lon]`\n            Longitude coordinates in degrees.\n        lat_axis: Axis of var_x and var_y which corresponds to latitude.\n        lon_axis: Axis of var_x and var_y which corresponds to longitude.\n\n    Returns:\n        Gradient of `var` in x direction with same shape as `var`. Units are those of `var` multiplied by $m^{-1}$.\n    \"\"\"\n    var_shape = np.ones(len(var.shape), dtype=int)\n    var_shape[lat_axis] = len(lat)\n    cos_lat = np.asarray(np.cos(np.deg2rad(lat))).reshape(var_shape)\n    return np.gradient(var, np.asarray(np.deg2rad(lon)), axis=lon_axis) / (radius_earth * cos_lat)\n</code></pre>"},{"location":"code/utils/calculus/#isca_tools.utils.calculus.grad_y","title":"<code>grad_y(var, lat, lat_axis=-2)</code>","text":"<p>Finds the gradient in the y direction of a scalar field.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>ndarray</code> <p>Scalar field to find gradient of. Typical shape is <code>[n_time x n_pressure x n_lat x n_lon]</code>, but can accept all different shapes as long as lat and lon are included.</p> required <code>lat</code> <code>ndarray</code> <p><code>float [n_lat]</code> Latitude coordinates in degrees.</p> required <code>lat_axis</code> <code>int</code> <p>Axis of var_x and var_y which corresponds to latitude.</p> <code>-2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Gradient of <code>var</code> in y direction with same shape as <code>var</code>. Units are those of <code>var</code> multiplied by \\(m^{-1}\\).</p> Source code in <code>isca_tools/utils/calculus.py</code> <pre><code>def grad_y(var: np.ndarray, lat: np.ndarray, lat_axis: int = -2) -&gt; np.ndarray:\n    \"\"\"\n    Finds the gradient in the y direction of a scalar field.\n\n    Args:\n        var: Scalar field to find gradient of. Typical shape is `[n_time x n_pressure x n_lat x n_lon]`,\n            but can accept all different shapes as long as lat and lon are included.\n        lat: `float [n_lat]`\n            Latitude coordinates in degrees.\n        lat_axis: Axis of var_x and var_y which corresponds to latitude.\n\n    Returns:\n        Gradient of `var` in y direction with same shape as `var`. Units are those of `var` multiplied by $m^{-1}$.\n    \"\"\"\n    return np.gradient(var, np.asarray(np.deg2rad(lat)), axis=lat_axis) / radius_earth\n</code></pre>"},{"location":"code/utils/circulation/","title":"Circulation","text":""},{"location":"code/utils/circulation/#isca_tools.utils.circulation.get_stream","title":"<code>get_stream(v, p, lat, ax_p=0)</code>","text":"<p>Computes the streamfunction \\(\\psi\\) at latitude \\(\\phi\\) and pressure \\(p\\) according to:</p> <p>\\(\\psi(\\phi, p) = \\int_0^z v\\rho dx dz = 2\\pi a \\cos \\phi\\int_0^z v\\rho dz = -\\frac{2\\pi a \\cos \\phi}{g} \\int_{p_{surf}}^p vdp\\)</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>ndarray</code> <p><code>float [n_p_levels, n_lat]</code>.  Meridional velocity at each pressure and latitude. Units: m/s.</p> required <code>p</code> <code>ndarray</code> <p><code>float [n_p_levels]</code>. Pressure levels corresponding to meridional velocity. Must be descending so first value is closest to surface. Units: Pa.</p> required <code>lat</code> <code>ndarray</code> <p><code>float [n_lat]</code>. Latitude corresponding to meridional velocity. Units: degrees.</p> required <code>ax_p</code> <code>int</code> <p>Axis corresponding to pressure levels in \\(v\\).</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_lat]</code>. Streamfunction at pressure level given by <code>p[-1]</code>. Units: kg/s.</p> Source code in <code>isca_tools/utils/circulation.py</code> <pre><code>def get_stream(v: np.ndarray, p: np.ndarray, lat: np.ndarray, ax_p: int = 0) -&gt; np.ndarray:\n    \"\"\"\n    Computes the streamfunction $\\psi$ at latitude $\\phi$ and pressure $p$\n    [according to](https://sandrolubis.wordpress.com/2012/06/17/mass-streamfunction/):\n\n    $\\psi(\\phi, p) = \\int_0^z v\\\\rho dx dz = 2\\pi a \\cos \\phi\\int_0^z v\\\\rho dz =\n    -\\\\frac{2\\pi a \\cos \\phi}{g} \\int_{p_{surf}}^p vdp$\n\n    Args:\n        v: `float [n_p_levels, n_lat]`. &lt;/br&gt;\n            Meridional velocity at each pressure and latitude. Units: *m/s*.\n        p: `float [n_p_levels]`.&lt;/br&gt;\n            Pressure levels corresponding to meridional velocity.&lt;/br&gt;\n            Must be descending so first value is closest to surface. Units: *Pa*.\n        lat: `float [n_lat]`.&lt;/br&gt;\n            Latitude corresponding to meridional velocity. Units: *degrees*.\n        ax_p: Axis corresponding to pressure levels in $v$.\n\n    Returns:\n        `float [n_lat]`.&lt;/br&gt;\n            Streamfunction at pressure level given by `p[-1]`. Units: *kg/s*.\n\n    \"\"\"\n    if len(p) &gt; 1:\n        if p[1] &gt; p[0]:\n            raise ValueError(f'Pressure is not in correct order, expect p[0] to be surface value but it is {p[0]}Pa')\n    cos_lat = np.cos(np.deg2rad(lat))\n    stream = -2 * np.pi * radius_earth * cos_lat / g * integrate.simpson(v, p, axis=ax_p)\n    return stream\n</code></pre>"},{"location":"code/utils/circulation/#isca_tools.utils.circulation.get_u_thermal","title":"<code>get_u_thermal(temp, p, lat, ax_p=0, ax_lat=1)</code>","text":"<p>Computes thermal wind at pressure \\(p\\) and latitude \\(\\phi\\) according to Equation 1 in shaw_2023 paper:</p> <p>\\(u_T(p, \\phi) = \\int_{p_s}^{p}\\frac{R}{fap'}\\frac{\\partial T}{\\partial \\phi} dp'\\)</p> <p>where \\(p_s\\) is near-surface pressure, \\(f\\) is the coriolis parameter, \\(R\\) is the gas constant for dry air, \\(a\\) is the radius of the Earth and \\(T\\) is temperature.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>ndarray</code> <p><code>float [n_p_levels, n_lat, ...]</code>.  Temperature at each pressure and latitude. Units: K.</p> required <code>p</code> <code>ndarray</code> <p><code>float [n_p_levels]</code>. Pressure levels corresponding to meridional velocity. Must be descending so first value is closest to surface. Units: Pa.</p> required <code>lat</code> <code>ndarray</code> <p><code>float [n_lat]</code>. Latitude corresponding to meridional velocity. Units: degrees.</p> required <code>ax_p</code> <code>int</code> <p>Axis corresponding to pressure levels in <code>temp</code>. Must be \\(0\\) or \\(1\\).</p> <code>0</code> <code>ax_lat</code> <code>int</code> <p>Axis corresponding to latitude in <code>temp</code>. Must be \\(0\\) or \\(1\\).</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_lat, ...]</code>. Thermal wind at pressure level given by <code>p[-1]</code>. Units: m/s.</p> Source code in <code>isca_tools/utils/circulation.py</code> <pre><code>def get_u_thermal(temp: np.ndarray, p: np.ndarray, lat: np.ndarray, ax_p: int = 0, ax_lat: int = 1) -&gt; np.ndarray:\n    \"\"\"\n    Computes thermal wind at pressure $p$ and latitude $\\phi$ according to Equation 1\n    in [shaw_2023](https://www.nature.com/articles/s41558-023-01884-1) paper:\n\n    $u_T(p, \\phi) = \\int_{p_s}^{p}\\\\frac{R}{fap'}\\\\frac{\\partial T}{\\partial \\phi} dp'$\n\n    where $p_s$ is near-surface pressure, $f$ is the coriolis parameter, $R$ is the gas constant for dry air,\n    $a$ is the radius of the Earth and $T$ is temperature.\n\n    Args:\n        temp: `float [n_p_levels, n_lat, ...]`. &lt;/br&gt;\n            Temperature at each pressure and latitude. Units: *K*.\n        p: `float [n_p_levels]`.&lt;/br&gt;\n            Pressure levels corresponding to meridional velocity.&lt;/br&gt;\n            Must be descending so first value is closest to surface. Units: *Pa*.\n        lat: `float [n_lat]`.&lt;/br&gt;\n            Latitude corresponding to meridional velocity. Units: *degrees*.\n        ax_p: Axis corresponding to pressure levels in `temp`.&lt;/br&gt;\n            Must be $0$ or $1$.\n        ax_lat: Axis corresponding to latitude in `temp`.&lt;/br&gt;\n            Must be $0$ or $1$.\n\n    Returns:\n        `float [n_lat, ...]`.&lt;/br&gt;\n            Thermal wind at pressure level given by `p[-1]`. Units: *m/s*.\n    \"\"\"\n    if len(p) &gt; 1:\n        if p[1] &gt; p[0]:\n            raise ValueError(f'Pressure is not in correct order, expect p[0] to be surface value but it is {p[0]}Pa')\n    n_ax = len(temp.shape)  # all axis 2 or higher are not p or lat\n    ax_expand_dims = list(np.append(np.asarray([ax_lat]), np.arange(2, n_ax)))\n    integrand = np.gradient(temp, np.deg2rad(lat), axis=ax_lat) / np.expand_dims(p, axis=ax_expand_dims)\n    f_coriolis = 2 * rot_earth * np.sin(np.deg2rad(lat).to_numpy())\n    if n_ax &gt; 2:\n        f_coriolis = np.expand_dims(f_coriolis, list(np.arange(2, n_ax) - 1))\n    return integrate.simpson(integrand, p, axis=ax_p) * R / (radius_earth * f_coriolis)\n</code></pre>"},{"location":"code/utils/debug/","title":"Debug","text":""},{"location":"code/utils/debug/#isca_tools.utils.debug.load_workspace","title":"<code>load_workspace(path, target_globals=None)</code>","text":"<p>Load variables from a joblib file into the specified global namespace.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the <code>.joblib</code> file containing the saved workspace.</p> required <code>target_globals</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary (typically <code>globals()</code>) into which variables are loaded. Defaults to the caller's global namespace.</p> <code>None</code> Example <p>load_workspace(\"workspace.joblib\")</p> Source code in <code>isca_tools/utils/debug.py</code> <pre><code>def load_workspace(path: str, target_globals: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Load variables from a joblib file into the specified global namespace.\n\n    Args:\n        path (str): Path to the `.joblib` file containing the saved workspace.\n        target_globals (Optional[Dict[str, Any]]):\n            Dictionary (typically `globals()`) into which variables are loaded.\n            Defaults to the caller's global namespace.\n\n    Example:\n        &gt;&gt;&gt; load_workspace(\"workspace.joblib\")\n    \"\"\"\n    # Expand '~' and normalize path\n    load_path = os.path.expanduser(path)\n\n    # Get the caller's globals if none provided\n    if target_globals is None:\n        frame = inspect.currentframe().f_back\n        target_globals = frame.f_globals\n\n    # Load dictionary of saved variables\n    data = load(load_path)\n\n    if not isinstance(data, dict):\n        raise ValueError(\"Joblib file does not contain a dictionary of variables.\")\n\n    # Inject into globals\n    for name, value in data.items():\n        target_globals[name] = value\n</code></pre>"},{"location":"code/utils/debug/#isca_tools.utils.debug.save_workspace","title":"<code>save_workspace(filename, variables=None, compress=('lz4', 3))</code>","text":"<p>Save selected or all variables from the current workspace to a joblib file.</p> <p>If no variables are specified, saves all local variables from the caller's scope.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the <code>.joblib</code> file to save.</p> required <code>variables</code> <code>Optional[Union[Dict[str, Any], list[str]]]</code> <p>Either: - A dictionary of variable names to values to save, or - A list of variable names (as strings) to extract from the caller's workspace. If None, all variables from the caller's local scope are saved.</p> <code>None</code> <code>compress</code> <code>Union[bool, str, tuple]</code> <p>Compression method for joblib. Examples:     - False (no compression)     - 'lz4' or 'gzip'     - ('lz4', 3) for method + compression level</p> <code>('lz4', 3)</code> Example <p>x, y = 1, [1, 2, 3] save_workspace(\"test.joblib\", [\"x\", \"y\"]) save_workspace(\"all_vars.joblib\")  # saves everything in current workspace</p> Source code in <code>isca_tools/utils/debug.py</code> <pre><code>def save_workspace(\n    filename: str,\n    variables: Optional[Union[Dict[str, Any], list[str]]] = None,\n    compress: Union[bool, str, tuple] = (\"lz4\", 3)\n) -&gt; None:\n    \"\"\"Save selected or all variables from the current workspace to a joblib file.\n\n    If no variables are specified, saves all local variables from the caller's scope.\n\n    Args:\n        filename (str): Path to the `.joblib` file to save.\n        variables (Optional[Union[Dict[str, Any], list[str]]]):\n            Either:\n            - A dictionary of variable names to values to save, or\n            - A list of variable names (as strings) to extract from the caller's workspace.\n            If None, all variables from the caller's local scope are saved.\n        compress (Union[bool, str, tuple]):\n            Compression method for joblib.\n            Examples:\n                - False (no compression)\n                - 'lz4' or 'gzip'\n                - ('lz4', 3) for method + compression level\n\n    Example:\n        &gt;&gt;&gt; x, y = 1, [1, 2, 3]\n        &gt;&gt;&gt; save_workspace(\"test.joblib\", [\"x\", \"y\"])\n        &gt;&gt;&gt; save_workspace(\"all_vars.joblib\")  # saves everything in current workspace\n    \"\"\"\n    # Get caller's frame\n    frame = inspect.currentframe().f_back\n\n    # If no variables provided, grab everything from local scope\n    if variables is None:\n        variables_to_save = frame.f_locals.copy()\n    elif isinstance(variables, list):\n        # Extract listed variable names from caller's local scope\n        variables_to_save = {name: frame.f_locals[name] for name in variables if name in frame.f_locals}\n    elif isinstance(variables, dict):\n        variables_to_save = variables\n    else:\n        raise TypeError(\"`variables` must be a dict, list of variable names, or None\")\n\n    dump(variables_to_save, filename, compress=compress)\n</code></pre>"},{"location":"code/utils/decomposition/","title":"Decomposition","text":""},{"location":"code/utils/decomposition/#isca_tools.utils.decomposition.best_score_excluding_atom","title":"<code>best_score_excluding_atom(norm_reduction, combinations, atom)</code>","text":"<p>For each sample, find the maximum norm_reduction value among all combinations that do NOT contain atom[i].</p> <p>Parameters:</p> Name Type Description Default <code>norm_reduction</code> <code>ndarray</code> <p>(n_sample, n_comb) Score or reduction value for each sample\u2013combination pair.</p> required <code>combinations</code> <code>ndarray</code> <p>(n_comb, n_atom_select) Atom indices used in each combination.</p> required <code>atom</code> <code>ndarray</code> <p>(n_sample,) Atom index to exclude for each sample.</p> required <p>Returns:</p> Name Type Description <code>best_score_excl</code> <code>ndarray</code> <p>(n_sample,) Max norm_reduction for each sample excluding combinations that contain atom[i].</p> Source code in <code>isca_tools/utils/decomposition.py</code> <pre><code>def best_score_excluding_atom(norm_reduction: np.ndarray,\n                              combinations: np.ndarray,\n                              atom: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    For each sample, find the maximum norm_reduction value\n    among all combinations that do NOT contain atom[i].\n\n    Args:\n        norm_reduction: (n_sample, n_comb)\n            Score or reduction value for each sample\u2013combination pair.\n        combinations: (n_comb, n_atom_select)\n            Atom indices used in each combination.\n        atom: (n_sample,)\n            Atom index to exclude for each sample.\n\n    Returns:\n        best_score_excl: (n_sample,)\n            Max norm_reduction for each sample excluding combinations that contain atom[i].\n    \"\"\"\n    # (n_sample, n_comb): True if this combo contains that sample\u2019s excluded atom\n    contains_atom = np.any(combinations[None, :, :] == atom[:, None, None], axis=2)\n\n    # Mask those out\n    masked_scores = np.where(~contains_atom, norm_reduction, -np.inf)\n\n    # Take max over combinations\n    best_score_excl = masked_scores.max(axis=1)\n\n    return best_score_excl\n</code></pre>"},{"location":"code/utils/decomposition/#isca_tools.utils.decomposition.pca_on_xarray","title":"<code>pca_on_xarray(data, n_modes=4, standardize=True, valid=None, feature_dim_name='lev', reference_mean=True)</code>","text":"<p>Perform PCA (via SVD) on xarray dataset. The PCA is fit only on samples where <code>valid</code> is True. The components found are then fit to all samples in <code>data</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data </code> <p>DataArray with dims (..., feature_dim_name) (e.g. (co2, lat, lon, lev) with <code>feature_dim_name=lev</code>).</p> required <code>n_modes</code> <code>int</code> <p>Number of PCA modes to keep.</p> <code>4</code> <code>standardize</code> <code>bool</code> <p>If True, divide each feature by its std (computed from valid samples) before SVD so that features with different variances are equalized. If False, SVD is performed on raw deviations from <code>reference_mean</code>.</p> <code>True</code> <code>valid</code> <code>Optional[DataArray]</code> <p>Boolean mask with the same non-feature dims as <code>data</code> (e.g. (co2, lat, lon)). True indicates the grid cell is used to compute the PCA basis. If None, all grid cells with finite values across lev are considered valid.</p> <code>None</code> <code>feature_dim_name</code> <code>str</code> <p>Name of the dimension containing features of interest in <code>data</code>.</p> <code>'lev'</code> <code>reference_mean</code> <code>Union[bool, DataArray]</code> <p>1-D DataArray (dim <code>feature_dim_name</code>) to subtract before SVD. If False, a zero reference mean is used (i.e. PCA on deviations from zero). If True, a reference mean will be computed from all <code>valid</code> samples.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>components</code> <code>DataArray</code> <p>EOFs (modes) with dims (mode, feature_dim_name).</p> <code>scores</code> <code>DataArray</code> <p>PC coefficients with same dims as <code>data</code> but <code>mode</code> replacing <code>feature_dim_name</code>.</p> <code>mean_profile</code> <code>DataArray</code> <p>The reference_mean actually used (dim <code>feature_dim_name</code>).</p> <code>std_profile</code> <code>DataArray</code> <p>Std used for scaling (dim <code>feature_dim_name</code>). Ones if <code>standardize=False</code>.</p> Notes <ul> <li>This function uses np.linalg.svd directly so there is NO automatic re-centering:   the <code>reference_mean</code> you supply (or zero) is the baseline from which deviations   are computed.</li> <li>If standardize=True, std_profile is computed from the valid set and used both   for the SVD input and for projecting all profiles.</li> </ul> Source code in <code>isca_tools/utils/decomposition.py</code> <pre><code>def pca_on_xarray(data: xr.DataArray, n_modes: int = 4, standardize: bool = True,\n                  valid: Optional[xr.DataArray] = None, feature_dim_name: str = \"lev\",\n                  reference_mean: Union[bool, xr.DataArray] = True,\n                  ) -&gt; Tuple[xr.DataArray, xr.DataArray, xr.DataArray, xr.DataArray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Perform PCA (via SVD) on xarray dataset. The PCA is fit only on samples where `valid` is True. The\n    components found are then fit to all samples in `data`.\n\n    Args:\n        data : DataArray with dims (..., feature_dim_name) (e.g. (co2, lat, lon, lev) with `feature_dim_name=lev`).\n        n_modes (int): Number of PCA modes to keep.\n        standardize (bool): If True, divide each feature by its std (computed from valid samples)\n            *before* SVD so that features with different variances are equalized.\n            If False, SVD is performed on raw deviations from `reference_mean`.\n        valid: Boolean mask with the same non-feature dims as `data` (e.g. (co2, lat, lon)).\n            True indicates the grid cell is used to compute the PCA basis. If None, all\n            grid cells with finite values across lev are considered valid.\n        feature_dim_name: Name of the dimension containing features of interest in `data`.\n        reference_mean: 1-D DataArray (dim `feature_dim_name`) to subtract before SVD.\n            If False, a zero reference mean is used (i.e. PCA on deviations from zero).\n            If True, a reference mean will be computed from all `valid` samples.\n\n    Returns:\n        components: EOFs (modes) with dims (mode, feature_dim_name).\n        scores: PC coefficients with same dims as `data` but `mode` replacing `feature_dim_name`.\n        mean_profile: The reference_mean actually used (dim `feature_dim_name`).\n        std_profile: Std used for scaling (dim `feature_dim_name`). Ones if `standardize=False`.\n\n    Notes:\n        - This function uses np.linalg.svd directly so there is NO automatic re-centering:\n          the `reference_mean` you supply (or zero) is the baseline from which deviations\n          are computed.\n        - If standardize=True, std_profile is computed from the valid set and used both\n          for the SVD input and for projecting all profiles.\n    \"\"\"\n    if feature_dim_name not in data.dims:\n        raise ValueError(f\"X must have a '{feature_dim_name}' dimension\")\n\n    non_feature_dims = [d for d in data.dims if d != feature_dim_name]\n    n_feature = data.sizes[feature_dim_name]\n\n    # prepare reference mean (1d array length n_feature)\n    if reference_mean == False:\n        reference_mean_vals = np.zeros(n_feature)\n    elif reference_mean == True:\n        if valid is None:\n            reference_mean = data.mean(dim=non_feature_dims)\n        else:\n            reference_mean = data.where(valid).mean(dim=non_feature_dims)\n        reference_mean_vals = reference_mean.values\n    else:\n        if feature_dim_name not in reference_mean.dims:\n            raise ValueError(f\"reference_mean must have dimension named {feature_dim_name}\")\n        # align and extract numeric array in feature order of data\n        reference_mean_vals = reference_mean.reindex({feature_dim_name: data[feature_dim_name]}).values\n\n    X_all = flatten_to_numpy(data, feature_dim_name)\n    if valid is None:\n        X_valid = X_all\n    else:\n        if list(valid.dims) != non_feature_dims:\n            raise ValueError(f\"Valid has dims {list(valid.dims)}\\nShould have dims {non_feature_dims}\\nOrder important too.\")\n        X_valid = X_all[flatten_to_numpy(valid)]\n    n_valid = X_valid.shape[0]\n    if n_valid &lt; (n_modes + 1):\n        raise ValueError(\"Too few valid samples for PCA; reduce n_modes or check coverage.\")\n\n    # subtract reference mean\n    Xc_valid = X_valid - reference_mean_vals[None, :]\n\n    # compute std_profile from valid subset if requested\n    if standardize:\n        std_profile_vals = Xc_valid.std(axis=0, ddof=1)\n        # avoid zeros\n        std_profile_vals = np.where(std_profile_vals == 0, 1.0, std_profile_vals)\n        Xc_valid = Xc_valid / std_profile_vals[None, :]\n    else:\n        std_profile_vals = np.ones(n_feature)\n\n    # --- SVD on the prepared valid data (no further centering) ---\n    # Xc_valid shape: (n_valid, n_feature). compute thin SVD\n    U, S, Vt = np.linalg.svd(Xc_valid, full_matrices=False)\n    # components (EOFs) are rows of Vt; keep first n_modes\n    components_vals = Vt[:n_modes, :]  # (n_modes, n_feature)\n\n    # --- project ALL profiles using same transform ---\n    # subtract reference mean and divide by std_profile (if standardize)\n    Xc_all = X_all - reference_mean_vals[None, :]\n    if standardize:\n        Xc_all = Xc_all / std_profile_vals[None, :]\n\n    # scores_all: (n_samples, n_modes)\n    scores_all = Xc_all @ components_vals.T\n\n    # reshape back to original non-feature dims + mode\n    out_shape = [data.sizes[d] for d in non_feature_dims] + [n_modes]\n    scores_da = xr.DataArray(\n        scores_all.reshape(*out_shape),\n        dims=non_feature_dims + [\"mode\"],\n        coords={**{d: data[d] for d in non_feature_dims}, \"mode\": np.arange(n_modes)}\n    )\n\n    components_da = xr.DataArray(\n        components_vals,\n        dims=(\"mode\", feature_dim_name),\n        coords={\"mode\": np.arange(n_modes), feature_dim_name: data[feature_dim_name]}\n    )\n\n    mean_profile_da = xr.DataArray(reference_mean_vals, dims=(feature_dim_name,),\n                                   coords={feature_dim_name: data[feature_dim_name]})\n    std_profile_da = xr.DataArray(std_profile_vals, dims=(feature_dim_name,),\n                                  coords={feature_dim_name: data[feature_dim_name]})\n\n    # Variance explained by each mode\n    var_explained = (S ** 2) / (Xc_valid.shape[0] - 1)\n\n    # Fractional variance explained\n    frac_var_explained = var_explained / var_explained.sum()\n\n    return components_da, scores_da, mean_profile_da, std_profile_da, var_explained[:n_modes], frac_var_explained[\n        :n_modes]\n</code></pre>"},{"location":"code/utils/decomposition/#isca_tools.utils.decomposition.scaled_k_means","title":"<code>scaled_k_means(x, initial_cluster_mean, valid=None, n_atom_select=1, norm_thresh=0, score_thresh=0.5, score_diff_thresh=0.1, score_diff_thresh_test_converge=0.05, score_thresh_multi_atom=0.05, min_cluster_size=10, n_iter=100, remove_perm=None, atom_ind_no_update=None, use_norm=False)</code>","text":"<p>Perform scaled k-means clustering with optional multi-atom combinations.</p> <p>This algorithm generalizes k-means by allowing each data point to be represented as a scaled combination of a small subset of cluster \"atoms\" (mean vectors), optionally including a zero vector (to allow sparse fits). At each iteration, coefficients for all possible atom combinations are computed to minimize residual norm, and cluster means are updated as the dominant direction of assigned samples\u2019 residuals.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data of shape (n_sample, n_feature).</p> required <code>initial_cluster_mean</code> <code>ndarray</code> <p>Initial cluster centroids of shape (n_cluster, n_feature).</p> required <code>valid</code> <code>Optional[ndarray]</code> <p>Boolean mask (n_sample,) specifying valid samples for updates.</p> <code>None</code> <code>n_atom_select</code> <code>int</code> <p>Number of atoms combined to represent each sample. Defaults to 1.</p> <code>1</code> <code>norm_thresh</code> <code>float</code> <p>Threshold for treating samples as small-norm (ignored in fitting). Defaults to 0.</p> <code>0</code> <code>score_thresh</code> <code>float</code> <p>Minimum improvement (norm reduction) for a sample to influence cluster update. Defaults to 0.5.</p> <code>0.5</code> <code>score_diff_thresh</code> <code>float</code> <p>Minimum difference in score between best and next-best atom to be considered distinct. Defaults to 0.1.</p> <code>0.1</code> <code>score_diff_thresh_test_converge</code> <code>float</code> <p>Tolerance for convergence test (difference between old and new best scores). Defaults to 0.05.</p> <code>0.05</code> <code>score_thresh_multi_atom</code> <code>float</code> <p>Threshold for assigning multi-atom fits when residual difference is small. Defaults to 0.05.</p> <code>0.05</code> <code>min_cluster_size</code> <code>int</code> <p>Minimum number of samples required to update a cluster. Defaults to 10.</p> <code>10</code> <code>n_iter</code> <code>int</code> <p>Maximum number of iterations. Defaults to 100.</p> <code>100</code> <code>remove_perm</code> <code>Optional[ndarray]</code> <p>List of atom combinations (indices) to exclude. Defaults to None.</p> <code>None</code> <code>atom_ind_no_update</code> <code>Optional[ndarray]</code> <p>Atom indices that should not be updated. Defaults to None.</p> <code>None</code> <code>use_norm</code> <code>bool</code> <p>Whether to normalize each residual before updating atoms. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>norm_cluster_mean</code> <code>ndarray</code> <p>Updated normalized cluster mean vectors (atoms).</p> <code>cluster_eig_val</code> <code>ndarray</code> <p>Leading eigenvalues for each cluster.</p> <code>cluster_ind</code> <code>ndarray</code> <p>Cluster/combination index assigned to each sample.</p> <code>top_score</code> <code>ndarray</code> <p>Norm reduction score of the assigned combination for each sample.</p> <code>coef_best</code> <code>ndarray</code> <p>Coefficients of best-fitting atom combination per sample.</p> <code>atom_perm</code> <code>ndarray</code> <p>Array of atom index combinations considered.</p> Notes <ul> <li>The algorithm can handle multi-atom fits by enumerating all valid atom combinations.</li> <li>A zero vector is appended as an additional atom to allow sparse representations.</li> <li>Clusters with fewer than <code>min_cluster_size</code> assigned samples are deactivated.</li> </ul> Source code in <code>isca_tools/utils/decomposition.py</code> <pre><code>def scaled_k_means(\n    x: np.ndarray,\n    initial_cluster_mean: np.ndarray,\n    valid: Optional[np.ndarray] = None,\n    n_atom_select: int = 1,\n    norm_thresh: float = 0,\n    score_thresh: float = 0.5,\n    score_diff_thresh: float = 0.1,\n    score_diff_thresh_test_converge: float = 0.05,\n    score_thresh_multi_atom: float = 0.05,\n    min_cluster_size: int = 10,\n    n_iter: int = 100,\n    remove_perm: Optional[np.ndarray] = None,\n    atom_ind_no_update: Optional[np.ndarray] = None,\n    use_norm: bool = False\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Perform scaled k-means clustering with optional multi-atom combinations.\n\n    This algorithm generalizes k-means by allowing each data point to be represented\n    as a *scaled combination* of a small subset of cluster \"atoms\" (mean vectors),\n    optionally including a zero vector (to allow sparse fits). At each iteration,\n    coefficients for all possible atom combinations are computed to minimize\n    residual norm, and cluster means are updated as the dominant direction\n    of assigned samples\u2019 residuals.\n\n    Args:\n        x:\n            Input data of shape (n_sample, n_feature).\n        initial_cluster_mean:\n            Initial cluster centroids of shape (n_cluster, n_feature).\n        valid:\n            Boolean mask (n_sample,) specifying valid samples for updates.\n        n_atom_select:\n            Number of atoms combined to represent each sample. Defaults to 1.\n        norm_thresh:\n            Threshold for treating samples as small-norm (ignored in fitting). Defaults to 0.\n        score_thresh:\n            Minimum improvement (norm reduction) for a sample to influence cluster update. Defaults to 0.5.\n        score_diff_thresh:\n            Minimum difference in score between best and next-best atom to be considered distinct. Defaults to 0.1.\n        score_diff_thresh_test_converge:\n            Tolerance for convergence test (difference between old and new best scores). Defaults to 0.05.\n        score_thresh_multi_atom:\n            Threshold for assigning multi-atom fits when residual difference is small. Defaults to 0.05.\n        min_cluster_size:\n            Minimum number of samples required to update a cluster. Defaults to 10.\n        n_iter:\n            Maximum number of iterations. Defaults to 100.\n        remove_perm:\n            List of atom combinations (indices) to exclude. Defaults to None.\n        atom_ind_no_update:\n            Atom indices that should not be updated. Defaults to None.\n        use_norm:\n            Whether to normalize each residual before updating atoms. Defaults to False.\n\n    Returns:\n        norm_cluster_mean: Updated normalized cluster mean vectors (atoms).\n        cluster_eig_val: Leading eigenvalues for each cluster.\n        cluster_ind: Cluster/combination index assigned to each sample.\n        top_score: Norm reduction score of the assigned combination for each sample.\n        coef_best: Coefficients of best-fitting atom combination per sample.\n        atom_perm: Array of atom index combinations considered.\n\n    Notes:\n        - The algorithm can handle multi-atom fits by enumerating all valid atom combinations.\n        - A zero vector is appended as an additional atom to allow sparse representations.\n        - Clusters with fewer than `min_cluster_size` assigned samples are deactivated.\n    \"\"\"\n\n    n_sample, n_feature = x.shape\n\n    # Normalize initial cluster means (atoms)\n    norm_cluster_mean = initial_cluster_mean / np.linalg.norm(initial_cluster_mean, axis=1).reshape(-1, 1)\n\n    # Append a zero vector atom to allow sparse/no-fit representations\n    norm_cluster_mean = np.vstack([norm_cluster_mean, np.zeros(n_feature)])\n    n_atom = norm_cluster_mean.shape[0]\n\n    # Initialize containers\n    cluster_eig_val = np.zeros(n_atom)\n    cluster_ind = np.full(x.shape[0], -20, dtype=int)\n    x_norm = np.linalg.norm(x, axis=1)\n\n    # Identify samples with very small norms \u2014 skip coefficient computation for them\n    small_norm = x_norm &lt;= norm_thresh\n\n    # Generate all possible atom index combinations (permutations of n_atom_select atoms)\n    atom_perm = np.array(list(itertools.combinations(range(n_atom), n_atom_select)))\n    atom_perm = np.sort(atom_perm, axis=1)  # Sort to ensure zero atom appears last consistently\n\n    # Optionally remove forbidden combinations\n    if remove_perm is not None:\n        remove_perm = np.sort(remove_perm, axis=1)\n        mask = ~np.isin(\n            atom_perm.view([('', atom_perm.dtype)] * atom_perm.shape[1]),\n            remove_perm.view([('', atom_perm.dtype)] * remove_perm.shape[1])\n        ).squeeze()\n        if (~mask).sum() &gt; 0:\n            print(f\"Removing the following atom permutations:\\n{atom_perm[~mask]}\")\n        atom_perm = atom_perm[mask]\n\n    if atom_ind_no_update is None:\n        atom_ind_no_update = np.zeros(0, dtype=int)\n\n    n_perm = len(atom_perm)\n\n    # Identify all permutations that include the zero atom\n    perm_zero_ind = np.where([n_atom - 1 in atom_perm[i] for i in range(n_perm)])[0].squeeze()\n\n    # Track permutations to ignore (e.g., if corresponding atoms become inactive)\n    ignore_perm = np.zeros(n_perm, dtype=bool)\n\n    for i in range(np.clip(n_iter, 1, 1000)):\n        coef = np.zeros((n_sample, n_perm, n_atom_select))  # coefficients for each permutation\n\n        # --- Step 1: Compute coefficients for all permutations ---\n        for j in range(n_perm):\n            if ignore_perm[j]:\n                continue\n\n            if j in perm_zero_ind:\n                if n_atom_select &gt; 1:\n                    # Compute coefficients for non-zero atoms in combinations including zero\n                    A = norm_cluster_mean[atom_perm[j][:-1]]\n                    AAT_inv = np.linalg.inv(A @ A.T)\n                    coef[~small_norm, j, :-1] = (AAT_inv @ A @ x[~small_norm].T).T\n            else:\n                # Compute coefficients for full atom combinations\n                A = norm_cluster_mean[atom_perm[j]]\n                AAT_inv = np.linalg.inv(A @ A.T)\n                coef[~small_norm, j] = (AAT_inv @ A @ x[~small_norm].T).T\n\n        cluster_ind_old = cluster_ind.copy()\n\n        # --- Step 2: Compute residuals and assign each sample to the best combination ---\n        x_residual = x[:, None] - (coef[..., None] * norm_cluster_mean[atom_perm][None]).sum(axis=-2)\n        x_residual_norm = np.linalg.norm(x_residual, axis=-1)\n\n        # Compute fractional norm reduction\n        norm_reduction = (x_norm[:, None] - x_residual_norm) / (x_norm[:, None] + 1e-20)\n\n        # Choose combination with smallest residual\n        cluster_ind = x_residual_norm.argmin(axis=1)\n\n        # If multi-atom case, prefer those with near-zero residuals that include zero atom\n        if n_atom_select &gt; 1:\n            good_with_zero = x_residual_norm[:, perm_zero_ind].min(axis=1) &lt;= norm_thresh\n            good_with_zero |= (\n                norm_reduction.max(axis=1) - norm_reduction[:, perm_zero_ind].max(axis=1) &lt; score_thresh_multi_atom\n            )\n            cluster_ind[good_with_zero] = perm_zero_ind[\n                x_residual_norm[good_with_zero][:, perm_zero_ind].argmin(axis=1)\n            ]\n\n        # Assign -1 for samples below norm threshold\n        cluster_ind[small_norm] = -1\n\n        # Top score per sample (how much norm was reduced)\n        top_score = norm_reduction[np.arange(n_sample), cluster_ind]\n        top_score[x_norm &lt;= norm_thresh] = 0\n\n        if n_iter == 0:\n            print('n_iter=0 so not updating atoms')\n            break\n\n        # --- Step 3: Identify strong assignments to guide cluster updates ---\n        score_exclude_atom = [\n            best_score_excluding_atom(norm_reduction, atom_perm, atom_perm[cluster_ind][:, k])\n            for k in range(n_atom_select)\n        ]\n        high_score = [\n            (top_score &gt; score_thresh) &amp; (top_score - score_exclude_atom[k] &gt; score_diff_thresh)\n            for k in range(n_atom_select)\n        ]\n\n        # Convergence test: low score difference means cluster assignment has stabilized\n        low_score = [\n            top_score - score_exclude_atom[k] &lt; score_diff_thresh_test_converge\n            for k in range(n_atom_select)\n        ]\n        low_score = np.any(low_score, axis=0)\n\n        if valid is not None:\n            # Restrict updates to valid samples\n            high_score = [high_score[k] &amp; valid for k in range(n_atom_select)]\n            low_score = low_score | ~valid\n\n        # --- Step 4: Update cluster means (atoms) ---\n        for c in range(n_atom - 1):  # skip zero atom\n            if c in atom_ind_no_update:\n                continue\n\n            my_points = np.zeros((0, n_feature))\n\n            # Collect residuals corresponding to this atom\n            for k in range(n_atom_select):\n                samples_use = (cluster_ind &gt;= 0) &amp; (atom_perm[cluster_ind, k] == c) &amp; high_score[k]\n                if samples_use.sum() &gt; 0:\n                    x_use_fit = coef[samples_use, cluster_ind[samples_use], :, None] * norm_cluster_mean[\n                        atom_perm[cluster_ind[samples_use]]\n                    ]\n                    x_use_fit = np.delete(x_use_fit, k, axis=1)\n                    x_use_fit = x_use_fit.sum(axis=1)\n                    my_points = np.append(my_points, x[samples_use] - x_use_fit, axis=0)\n\n            n_my_points = my_points.shape[0]\n\n            # Deactivate cluster if too few points assigned\n            if n_my_points &lt; min_cluster_size:\n                norm_cluster_mean[c] = 0\n                ignore_perm[np.where([c in atom_perm[k] for k in range(n_perm)])[0].squeeze()] = True\n                continue\n\n            if use_norm:\n                # Normalize residuals to equalize influence\n                my_points = my_points / (np.linalg.norm(my_points, axis=1)[:, None] + 1e-20)\n\n            # Update atom as the leading eigenvector of covariance matrix\n            eig_vals, eigs = np.linalg.eig(my_points.T @ my_points / n_my_points)\n            best_eig_ind = np.argmax(eig_vals)\n            norm_cluster_mean[c] = eigs[:, best_eig_ind] * np.sign(eigs[:, best_eig_ind].mean())\n            cluster_eig_val[c] = eig_vals[best_eig_ind]\n\n        # Print number of reassignments to monitor convergence\n        print(i + 1, (cluster_ind[~low_score] != cluster_ind_old[~low_score]).sum())\n\n        # Stop if cluster assignments have stabilized\n        if (cluster_ind[~low_score] == cluster_ind_old[~low_score]).all():\n            print(f\"Done after {i + 1} iter\")\n            break\n\n    # Return best-fit coefficients for each sample\n    coef_best = coef[np.arange(x.shape[0]), cluster_ind]\n\n    return norm_cluster_mean, cluster_eig_val, cluster_ind, top_score, coef_best, atom_perm\n</code></pre>"},{"location":"code/utils/ds_slicing/","title":"Dataset Slicing","text":""},{"location":"code/utils/ds_slicing/#isca_tools.utils.ds_slicing.annual_mean","title":"<code>annual_mean(ds, n_year_days=360, first_day=1)</code>","text":"<p>Returns dataset <code>ds</code> with variables being the average over all years i.e. time dimension of <code>ds</code> will now be from 0.5 to 359.5 if a year has 360 days.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset for particular experiment.</p> required <code>n_year_days</code> <code>int</code> <p>Number of days in a year used for the simulation. This depends on the <code>calendar</code> option in the <code>main_nml</code> namelist.</p> <code>360</code> <code>first_day</code> <code>int</code> <p>Day used in starting date for the simulation. It is equal to the third number in the <code>current_date</code> option in the <code>main_nml</code> namelist. <code>1</code> refers to January 1st.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing the annual average of each variable</p> Source code in <code>isca_tools/utils/ds_slicing.py</code> <pre><code>def annual_mean(ds: Dataset, n_year_days: int = 360, first_day: int = 1) -&gt; Dataset:\n    \"\"\"\n    Returns dataset `ds` with variables being the average over all years i.e. time dimension of `ds` will now be from\n    0.5 to 359.5 if a year has 360 days.\n\n    Args:\n        ds: Dataset for particular experiment.\n        n_year_days: Number of days in a year used for the simulation.\n            This depends on the `calendar` option in the `main_nml` namelist.\n        first_day: Day used in starting date for the simulation.\n            It is equal to the third number in the `current_date` option in the `main_nml` namelist.\n            `1` refers to January 1st.\n\n    Returns:\n        Dataset containing the annual average of each variable\n\n    \"\"\"\n    ds_days = (first_day - 1 + ds.time) % n_year_days  # day in a given year that each value in ds.time refers to\n    return ds.groupby(ds_days).mean(dim='time')\n</code></pre>"},{"location":"code/utils/ds_slicing/#isca_tools.utils.ds_slicing.annual_time_slice","title":"<code>annual_time_slice(ds, include_months=None, include_days=None, month_days=30, year_months=12, first_day=1)</code>","text":"<p>Slices dataset <code>ds</code> so only contains data corresponding to given months or days for each year.</p> <p>Examples:</p> <p><code>ds_summer = annual_time_slice(ds, [6, 7, 8])</code>     This return a dataset containing data corresponding to     June, July and August in each year i.e. only northern hemisphere summer.</p> <p><code>ds_day = annual_time_slice(ds, include_days = [36])</code>     This will return a dataset containing data corresponding to the 36th day of the year for each year     of the simulation. The length of the time dimension will be the number of years of the simulation.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset for particular experiment.</p> required <code>include_months</code> <code>Optional[List[int]]</code> <p><code>int [n_months]</code> Months to keep (1 refers to January).</p> <code>None</code> <code>include_days</code> <code>Optional[List[int]]</code> <p><code>int [n_days]</code> Days to keep (1 refers to 1st January). If <code>include_months</code> is provided, this will be ignored.</p> <code>None</code> <code>month_days</code> <code>int</code> <p>Number of days in each month used for the simulation. This depends on the <code>calendar</code> option in the <code>main_nml</code> namelist.</p> <code>30</code> <code>year_months</code> <code>int</code> <p>Number of months in a year. I think this is always likely to be <code>12</code>.</p> <code>12</code> <code>first_day</code> <code>int</code> <p>Day used in starting date for the simulation. It is equal to the third number in the <code>current_date</code> option in the <code>main_nml</code> namelist. <code>1</code> refers to January 1st.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset only including certain months/days for each year.</p> Source code in <code>isca_tools/utils/ds_slicing.py</code> <pre><code>def annual_time_slice(ds: Dataset, include_months: Optional[List[int]] = None, include_days: Optional[List[int]] = None,\n                      month_days: int = 30, year_months: int = 12, first_day: int = 1) -&gt; Dataset:\n    \"\"\"\n    Slices dataset `ds` so only contains data corresponding to given months or days for each year.\n\n    Examples:\n        `ds_summer = annual_time_slice(ds, [6, 7, 8])`&lt;/br&gt;\n            This return a dataset containing data corresponding to\n            June, July and August in each year i.e. only northern hemisphere summer.\n\n        `ds_day = annual_time_slice(ds, include_days = [36])`&lt;/br&gt;\n            This will return a dataset containing data corresponding to the 36th day of the year for each year\n            of the simulation. The length of the time dimension will be the number of years of the simulation.\n\n    Args:\n        ds: Dataset for particular experiment.\n        include_months: `int [n_months]`&lt;/br&gt;\n            Months to keep (1 refers to January).\n        include_days: `int [n_days]`&lt;/br&gt;\n            Days to keep (1 refers to 1st January).\n            If `include_months` is provided, this will be ignored.\n        month_days: Number of days in each month used for the simulation.\n            This depends on the `calendar` option in the `main_nml` namelist.\n        year_months: Number of months in a year. I think this is always likely to be `12`.\n        first_day: Day used in starting date for the simulation.\n            It is equal to the third number in the `current_date` option in the `main_nml` namelist.\n            `1` refers to January 1st.\n\n    Returns:\n        Dataset only including certain months/days for each year.\n\n    \"\"\"\n    year_days = year_months * month_days  # number of days in a year\n    # ceil to deal with daily output data when 1st day is 0.5, 2nd day is 1.5 etc\n    ds_days = (first_day - 1 + np.ceil(ds.time)) % year_days  # day in a given year that each value in ds.time refers to\n    ds_days_step = float(ds_days[1] - ds_days[0])\n    ds_days[ds_days == 0] = year_days  # correction so last day of year has index 360 not 0\n    if include_months is not None:\n        include_days = [np.arange(1, month_days + 1) + month_days * (month - 1) for month in include_months]\n        include_days = np.concatenate(include_days)\n    elif include_days is None:\n        raise ValueError(\"Either include_months or include_days need to be specified but both are None.\")\n\n    # account for ds data being monthly or daily\n    include_days = include_days[(include_days - float(ds_days.min())) % ds_days_step == 0]\n\n    if not np.isin(include_days, ds_days).all():\n        raise ValueError(\"Not all months / days provided in include_months / include_days are valid\")\n    return ds.where(ds_days.isin(include_days), drop=True)\n</code></pre>"},{"location":"code/utils/ds_slicing/#isca_tools.utils.ds_slicing.anom_from_annual_mean","title":"<code>anom_from_annual_mean(ds, combine_lon=False, n_year_days=360, first_day=1)</code>","text":"<p>For each lat, lon and pressure; this computes the annual mean of each variable. It then subtracts it from the initial dataset, to give the anomaly relative to the annual mean value.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset for particular experiment.</p> required <code>combine_lon</code> <code>bool</code> <p>If <code>True</code> will be anomaly with respect to zonal annual mean, otherwise will just be with respect to annual mean.</p> <code>False</code> <code>n_year_days</code> <code>int</code> <p>Number of days in a year used for the simulation. This depends on the <code>calendar</code> option in the <code>main_nml</code> namelist.</p> <code>360</code> <code>first_day</code> <code>int</code> <p>Day used in starting date for the simulation. It is equal to the third number in the <code>current_date</code> option in the <code>main_nml</code> namelist. <code>1</code> refers to January 1st.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset with same <code>time</code> variable as <code>ds</code>, containing the annomaly relative to the</p> <code>Dataset</code> <p>annual average.</p> Source code in <code>isca_tools/utils/ds_slicing.py</code> <pre><code>def anom_from_annual_mean(ds: Dataset, combine_lon: bool = False, n_year_days: int = 360,\n                          first_day: int = 1) -&gt; Dataset:\n    \"\"\"\n    For each lat, lon and pressure; this computes the annual mean of each variable. It then subtracts it\n    from the initial dataset, to give the anomaly relative to the annual mean value.\n\n    Args:\n        ds: Dataset for particular experiment.\n        combine_lon: If `True` will be anomaly with respect to zonal annual mean, otherwise will just\n            be with respect to annual mean.\n        n_year_days: Number of days in a year used for the simulation.\n            This depends on the `calendar` option in the `main_nml` namelist.\n        first_day: Day used in starting date for the simulation.\n            It is equal to the third number in the `current_date` option in the `main_nml` namelist.\n            `1` refers to January 1st.\n\n    Returns:\n        Dataset with same `time` variable as `ds`, containing the annomaly relative to the\n        annual average.\n    \"\"\"\n    if combine_lon:\n        ds_annual_mean = annual_mean(ds.mean(dim='lon'), n_year_days, first_day)\n    else:\n        ds_annual_mean = annual_mean(ds, n_year_days, first_day)\n    ds_annual_mean = ds_annual_mean.rename({'time': 'day_of_year'})  # change coordinate to day of year\n    # make it integer starting at 0\n    ds_annual_mean = ds_annual_mean.assign_coords(day_of_year=(ds_annual_mean.day_of_year -\n                                                               ds_annual_mean.day_of_year.min()).astype(int))\n    ds['day_of_year'] = (ds.time % n_year_days - (ds.time % n_year_days).min()).astype(int)\n    return ds.groupby('day_of_year') - ds_annual_mean\n</code></pre>"},{"location":"code/utils/ds_slicing/#isca_tools.utils.ds_slicing.area_weight_mean_lat","title":"<code>area_weight_mean_lat(ds)</code>","text":"<p>For all variables in <code>ds</code>, an area weighted mean is taken over all latitudes in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset for particular experiment.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing averaged variables with no latitude dependence.</p> Source code in <code>isca_tools/utils/ds_slicing.py</code> <pre><code>def area_weight_mean_lat(ds: Dataset) -&gt; Dataset:\n    \"\"\"\n    For all variables in `ds`, an area weighted mean is taken over all latitudes in the dataset.\n\n    Args:\n        ds: Dataset for particular experiment.\n\n    Returns:\n        Dataset containing averaged variables with no latitude dependence.\n    \"\"\"\n    var_averaged = []\n    for var in ds.keys():\n        if 'lat' in list(ds[var].coords):\n            ds[var] = area_weighting(ds[var]).mean(dim='lat')\n            var_averaged += [var]\n    print(f\"Variables Averaged: {var_averaged}\")\n    return ds\n</code></pre>"},{"location":"code/utils/ds_slicing/#isca_tools.utils.ds_slicing.get_time_sample_indices","title":"<code>get_time_sample_indices(times_sample, times_all)</code>","text":"<p>Return indices of <code>times_sample</code> in <code>times_all</code> for each coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>times_sample</code> <code>DataArray</code> <p>Times for each sample at each location to get index in <code>time_all</code> for (sample, lat, lon).</p> required <code>times_all</code> <code>DataArray</code> <p>All times in a given simulation (time).</p> required <p>Returns:</p> Name Type Description <code>time_index</code> <code>DataArray</code> <p>Indices of <code>times_sample</code> within <code>times_all</code>, with shape (sample, lat, lon),           filled with NaN where times are not found. Hence, output is float to include NaN.</p> Source code in <code>isca_tools/utils/ds_slicing.py</code> <pre><code>def get_time_sample_indices(times_sample: xr.DataArray, times_all: xr.DataArray) -&gt; xr.DataArray:\n    \"\"\"Return indices of `times_sample` in `times_all` for each coordinate.\n\n    Args:\n        times_sample: Times for each sample at each location to get index in `time_all` for (sample, lat, lon).\n        times_all: All times in a given simulation (time).\n\n    Returns:\n        time_index: Indices of `times_sample` within `times_all`, with shape (sample, lat, lon),\n                      filled with NaN where times are not found. Hence, output is float to include NaN.\n    \"\"\"\n\n    # Broadcast ds1 times to full grid (sample, lat, lon)\n    times_sample_val = times_sample.values  # shape (sample, lat, lon)\n\n    # initialize indices with NaNs\n    indices = np.full(times_sample_val.shape, np.nan)\n\n    # Create lookup dict from times_all to indices\n    time_to_index = {t: i for i, t in enumerate(times_all.time.values)}\n\n    # Vectorized mapping (but must loop over unique times for efficiency)\n    unique_times = np.unique(times_sample_val)\n    for t in unique_times:\n        if t in time_to_index:\n            indices[times_sample_val == t] = time_to_index[t]\n    # Return as DataArray\n    return xr.DataArray(\n        indices,\n        dims=times_sample.dims,\n        coords={dim: times_sample.coords[dim] for dim in times_sample.dims},\n        name=\"time_index\"\n    )\n</code></pre>"},{"location":"code/utils/ds_slicing/#isca_tools.utils.ds_slicing.lat_lon_coord_slice","title":"<code>lat_lon_coord_slice(ds, lat, lon)</code>","text":"<p>Returns dataset, <code>ds</code>, keeping only data at the coordinate indicated by <code>(lat[i], lon[i])</code> for all <code>i</code>.</p> <p>If <code>ds</code> contained <code>t_surf</code> then the returned dataset would contain <code>t_surf</code> as a function of the variables <code>time</code> and <code>location</code> with each value of <code>location</code> corresponding to a specific <code>(lat, lon)</code> combination. For the original <code>ds</code>, it would be a function of <code>time</code>, <code>lat</code> and <code>lon</code>.</p> <p>This is inspired by a stack overflow post.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset for a particular experiment.</p> required <code>lat</code> <code>ndarray</code> <p><code>float [n_coords]</code> Latitude coordinates to keep.</p> required <code>lon</code> <code>ndarray</code> <p><code>float [n_coords]</code> Longitude coordinates to keep.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset only including the desired coordinates.</p> Source code in <code>isca_tools/utils/ds_slicing.py</code> <pre><code>def lat_lon_coord_slice(ds: Dataset, lat: np.ndarray, lon: np.ndarray) -&gt; Dataset:\n    \"\"\"\n    Returns dataset, `ds`, keeping only data at the coordinate indicated by `(lat[i], lon[i])` for all `i`.\n\n    If `ds` contained `t_surf` then the returned dataset would contain `t_surf` as a function of the variables\n    `time` and `location` with each value of `location` corresponding to a specific `(lat, lon)` combination.\n    For the original `ds`, it would be a function of `time`, `lat` and `lon`.\n\n    This is inspired by a\n    [stack overflow post](https://stackoverflow.com/questions/72179103/xarray-select-the-data-at-specific-x-and-y-coordinates).\n\n    Args:\n        ds: Dataset for a particular experiment.\n        lat: `float [n_coords]`\n            Latitude coordinates to keep.\n        lon: `float [n_coords]`\n            Longitude coordinates to keep.\n\n    Returns:\n        Dataset only including the desired coordinates.\n    \"\"\"\n    # To get dataset at specific coordinates, not all permutations, turn to xarray first\n    lat_xr = xr.DataArray(lat, dims=['location'])\n    lon_xr = xr.DataArray(lon, dims=['location'])\n    return ds.sel(lat=lat_xr, lon=lon_xr, method=\"nearest\")\n</code></pre>"},{"location":"code/utils/ds_slicing/#isca_tools.utils.ds_slicing.lat_lon_range_slice","title":"<code>lat_lon_range_slice(ds, lat_min=None, lat_max=None, lon_min=None, lon_max=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Union[Dataset, DataArray]</code> required <code>lat_min</code> <code>Optional[float]</code> <code>None</code> <code>lat_max</code> <code>Optional[float]</code> <code>None</code> <code>lon_min</code> <code>Optional[float]</code> <code>None</code> <code>lon_max</code> <code>Optional[float]</code> <code>None</code> <p>Returns:</p> Source code in <code>isca_tools/utils/ds_slicing.py</code> <pre><code>def lat_lon_range_slice(ds: Union[Dataset, DataArray], lat_min: Optional[float] = None,\n                        lat_max: Optional[float] = None, lon_min: Optional[float] = None,\n                        lon_max: Optional[float] = None):\n    \"\"\"\n\n    Args:\n        ds:\n        lat_min:\n        lat_max:\n        lon_min:\n        lon_max:\n\n    Returns:\n\n    \"\"\"\n    if (lon_min is None) and lon_max is None:\n        lon_range = None\n    else:\n        if lon_max is None:\n            raise ValueError('lon_max is required')\n        if lon_min is None:\n            raise ValueError('lon_min is required')\n        lon_range = slice(lon_min, lon_max)\n\n    if (lat_min is None) and (lat_max is None):\n        lat_range = None\n    else:\n        if lat_max is None:\n            raise ValueError('lat_max is required')\n        if lat_min is None:\n            raise ValueError('lat_min is required')\n        lat_range = slice(lat_min, lat_max)\n\n    if lat_range is not None:\n        ds = ds.sel(lat=lat_range)\n    if lon_range is not None:\n        ds = ds.sel(lon=lon_range)\n    return ds\n</code></pre>"},{"location":"code/utils/ds_slicing/#isca_tools.utils.ds_slicing.lat_lon_rolling","title":"<code>lat_lon_rolling(ds, window_lat, window_lon)</code>","text":"<p>This creates a rolling averaged version of the dataset or data-array in the spatial dimension. Returned data will have first <code>np.ceil((window_lat-1)/2)</code> and last <code>np.floor((window_lat-1)/2)</code> values as <code>nan</code> in latitude dimension. The averaging also does not take account of area weighting in latitude dimension.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Union[Dataset, DataArray]</code> <p>Dataset or DataArray to find rolling mean of.</p> required <code>window_lat</code> <code>int</code> <p>Size of window for rolling average in latitude dimension [number of grid points]</p> required <code>window_lon</code> <code>int</code> <p>Size of window for rolling average in longitude dimension [number of grid points].</p> required <p>Returns:</p> Type Description <code>Union[Dataset, DataArray]</code> <p>Rolling averaged dataset or DataArray.</p> Source code in <code>isca_tools/utils/ds_slicing.py</code> <pre><code>def lat_lon_rolling(ds: Union[Dataset, DataArray], window_lat: int, window_lon: int) -&gt; Union[Dataset, DataArray]:\n    \"\"\"\n    This creates a rolling averaged version of the dataset or data-array in the spatial dimension.\n    Returned data will have first `np.ceil((window_lat-1)/2)` and last `np.floor((window_lat-1)/2)`\n    values as `nan` in latitude dimension.\n    The averaging also does not take account of area weighting in latitude dimension.\n\n    Args:\n        ds: Dataset or DataArray to find rolling mean of.\n        window_lat: Size of window for rolling average in latitude dimension [number of grid points]\n        window_lon: Size of window for rolling average in longitude dimension [number of grid points].\n\n    Returns:\n        Rolling averaged dataset or DataArray.\n\n    \"\"\"\n    ds_roll = ds.pad(lon=window_lon, mode='wrap')       # first pad in lon so wraps around when doing rolling mean\n    ds_roll = ds_roll.rolling({'lon': window_lon, 'lat': window_lat}, center=True).mean()\n    return ds_roll.isel(lon=slice(window_lon, -window_lon))     # remove the padded longitude values\n</code></pre>"},{"location":"code/utils/ds_slicing/#isca_tools.utils.ds_slicing.time_rolling","title":"<code>time_rolling(ds, window_time, wrap=True)</code>","text":"<p>This creates a rolling-averaged version of the dataset or data-array in the time dimension. Useful for when you have an annual average dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Union[Dataset, DataArray]</code> <p>Dataset or DataArray to find rolling mean of.</p> required <code>window_time</code> <code>int</code> <p>Size of window for rolling average in time dimension [number of time units e.g. days]</p> required <code>wrap</code> <code>bool</code> <p>If the first time comes immediately after the last time i.e. for annual mean data</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Dataset, DataArray]</code> <p>Rolling averaged dataset or DataArray.</p> Source code in <code>isca_tools/utils/ds_slicing.py</code> <pre><code>def time_rolling(ds: Union[Dataset, DataArray], window_time: int, wrap: bool = True) -&gt; Union[Dataset, DataArray]:\n    \"\"\"\n    This creates a rolling-averaged version of the dataset or data-array in the time dimension. Useful for when\n    you have an annual average dataset.\n\n    Args:\n        ds: Dataset or DataArray to find rolling mean of.\n        window_time: Size of window for rolling average in time dimension [number of time units e.g. days]\n        wrap: If the first time comes immediately after the last time i.e. for annual mean data\n\n    Returns:\n        Rolling averaged dataset or DataArray.\n    \"\"\"\n    if wrap:\n        ds_roll = ds.pad(time=window_time, mode='wrap')  # first pad in time so wraps around when doing rolling mean\n        ds_roll = ds_roll.rolling(time=window_time, center=True).mean()\n        return ds_roll.isel(time=slice(window_time, -window_time))  # remove the padded time values\n    else:\n        return ds.rolling(time=window_time, center=True).mean()\n</code></pre>"},{"location":"code/utils/fourier/","title":"Fourier","text":""},{"location":"code/utils/fourier/#isca_tools.utils.fourier.coef_conversion","title":"<code>coef_conversion(amp_coef=None, phase_coef=None, cos_coef=None, sin_coef=None)</code>","text":"<p>The term for the \\(n^{th}\\) harmonic of a Fourier expansion can be written in two ways:</p> <ol> <li>\\(F_n\\cos(2n\\pi ft - \\Phi_n)\\)</li> <li>\\(F_{n, cos}\\cos(2n\\pi ft) + F_{n, sin}\\sin(2n\\pi ft)\\)</li> </ol> <p>Given the coefficients of one form, this returns the coefficients in the other form. Note \\(\\sin(x) = \\cos(x - \\pi /2)\\).</p> <p>Parameters:</p> Name Type Description Default <code>amp_coef</code> <code>Optional[Union[float, ndarray]]</code> <p><code>float [n_coefs]</code> \\(F_n\\) coefficients. If provided, will return \\(F_{n, cos}\\) and \\(F_{n, sin}\\).</p> <code>None</code> <code>phase_coef</code> <code>Optional[Union[float, ndarray]]</code> <p><code>float [n_coefs]</code> \\(\\Phi_n\\) coefficients. If provided, will return \\(F_{n, cos}\\) and \\(F_{n, sin}\\).</p> <code>None</code> <code>cos_coef</code> <code>Optional[Union[float, ndarray]]</code> <p><code>float [n_coefs]</code> \\(F_{n, cos}\\) coefficients. If provided, will return \\(F_{n}\\) and \\(\\Phi_n\\).</p> <code>None</code> <code>sin_coef</code> <code>Optional[Union[float, ndarray]]</code> <p><code>float [n_coefs]</code> \\(F_{n, sin}\\) coefficients. If provided, will return \\(F_{n}\\) and \\(\\Phi_n\\).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>coef1</code> <code>Union[float, ndarray]</code> <p><code>float [n_coefs]</code> Either \\(F_n\\) or \\(F_{n, cos}\\) depending on input.</p> <code>coef2</code> <code>Union[float, ndarray]</code> <p><code>float [n_coefs]</code> Either \\(\\Phi_n\\) or \\(F_{n, sin}\\) depending on input.</p> Source code in <code>isca_tools/utils/fourier.py</code> <pre><code>def coef_conversion(amp_coef: Optional[Union[float, np.ndarray]] = None,\n                    phase_coef: Optional[Union[float, np.ndarray]] = None,\n                    cos_coef: Optional[Union[float, np.ndarray]] = None,\n                    sin_coef: Optional[Union[float, np.ndarray]] = None) -&gt; Tuple[Union[float, np.ndarray],\nUnion[float, np.ndarray]]:\n    \"\"\"\n    The term for the $n^{th}$ harmonic of a Fourier expansion can be written in two ways:\n\n    1. $F_n\\\\cos(2n\\\\pi ft - \\\\Phi_n)$\n    2. $F_{n, cos}\\\\cos(2n\\\\pi ft) + F_{n, sin}\\\\sin(2n\\\\pi ft)$\n\n    Given the coefficients of one form, this returns the coefficients in the other form.\n    Note $\\\\sin(x) = \\\\cos(x - \\\\pi /2)$.\n\n    Args:\n        amp_coef: `float [n_coefs]`&lt;/br&gt;\n            $F_n$ coefficients. If provided, will return $F_{n, cos}$ and $F_{n, sin}$.\n        phase_coef: `float [n_coefs]`&lt;/br&gt;\n            $\\Phi_n$ coefficients. If provided, will return $F_{n, cos}$ and $F_{n, sin}$.\n        cos_coef: `float [n_coefs]`&lt;/br&gt;\n            $F_{n, cos}$ coefficients. If provided, will return $F_{n}$ and $\\Phi_n$.\n        sin_coef: `float [n_coefs]`&lt;/br&gt;\n            $F_{n, sin}$ coefficients. If provided, will return $F_{n}$ and $\\Phi_n$.\n\n    Returns:\n        coef1: `float [n_coefs]`&lt;/br&gt;\n            Either $F_n$ or $F_{n, cos}$ depending on input.\n        coef2: `float [n_coefs]`&lt;/br&gt;\n            Either $\\Phi_n$ or $F_{n, sin}$ depending on input.\n    \"\"\"\n    if amp_coef is not None:\n        cos_coef = amp_coef * np.cos(phase_coef)\n        sin_coef = amp_coef * np.sin(phase_coef)\n        return  cos_coef, sin_coef\n    else:\n        if cos_coef == 0:\n            phase_coef = np.pi/2\n            amp_coef = sin_coef\n        else:\n            phase_coef = np.arctan(sin_coef/cos_coef)\n            amp_coef = cos_coef / np.cos(phase_coef)\n        return amp_coef, phase_coef\n</code></pre>"},{"location":"code/utils/fourier/#isca_tools.utils.fourier.fourier_series","title":"<code>fourier_series(time, coefs_amp, coefs_phase)</code>","text":"<p>For \\(N\\) harmonics, the fourier series with frequency \\(f\\) is:</p> \\[F(t) \\approx \\frac{F_0}{2} + \\sum_{n=1}^{N} F_n\\cos(2n\\pi ft - \\Phi_n)\\] <p>Parameters:</p> Name Type Description Default <code>time</code> <code>ndarray</code> <p><code>float [n_time]</code> Time in days (assumes periodic e.g. annual mean, so <code>time = np.arange(360)</code>) covering entire period such that period is <code>time[-1] - time[0] + 1</code>.</p> required <code>coefs_amp</code> <code>Union[List[float], ndarray]</code> <p><code>float [N+1]</code> The amplitude coefficients, \\(F_n\\)</p> required <code>coefs_phase</code> <code>Union[List[float], ndarray]</code> <p><code>float [N]</code> The phase coefficients in radians, \\(\\Phi_n\\)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_time]</code> Value of Fourier series solution at each time</p> Source code in <code>isca_tools/utils/fourier.py</code> <pre><code>def fourier_series(time: np.ndarray, coefs_amp: Union[List[float], np.ndarray],\n                   coefs_phase: Union[List[float], np.ndarray]) -&gt; np.ndarray:\n    \"\"\"\n    For $N$ harmonics, the fourier series with frequency $f$ is:\n\n    $$F(t) \\\\approx \\\\frac{F_0}{2} + \\\\sum_{n=1}^{N} F_n\\\\cos(2n\\\\pi ft - \\\\Phi_n)$$\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Time in days (assumes periodic e.g. annual mean, so `time = np.arange(360)`) covering entire period\n            such that period is `time[-1] - time[0] + 1`.\n        coefs_amp: `float [N+1]`&lt;/br&gt;\n            The amplitude coefficients, $F_n$\n        coefs_phase: `float [N]`&lt;/br&gt;\n            The phase coefficients in radians, $\\Phi_n$\n\n    Returns:\n        `float [n_time]`&lt;/br&gt;\n            Value of Fourier series solution at each time\n    \"\"\"\n    period = float(time[-1] - time[0] + 1)\n    n_harmonics = len(coefs_amp)\n    ans = 0.5 * coefs_amp[0]\n    for n in range(1, n_harmonics):\n        ans += coefs_amp[n] * np.cos(2*n*np.pi*time/period - coefs_phase[n-1])\n    return ans\n</code></pre>"},{"location":"code/utils/fourier/#isca_tools.utils.fourier.fourier_series_deriv","title":"<code>fourier_series_deriv(time, coefs_amp, coefs_phase, day_seconds=86400)</code>","text":"<p>For \\(N\\) harmonics, the derivative of a fourier series with frequency \\(f\\) is:</p> \\[\\frac{dF}{dt} \\approx -\\sum_{n=1}^{N} 2n\\pi fF_n\\sin(2n\\pi ft - \\Phi_n)\\] <p>Parameters:</p> Name Type Description Default <code>time</code> <code>ndarray</code> <p><code>float [n_time]</code> Time in days (assumes periodic e.g. annual mean, so <code>time = np.arange(360)</code>) covering entire period such that period is <code>time[-1] - time[0] + 1</code>.</p> required <code>coefs_amp</code> <code>Union[ndarray, List[float]]</code> <p><code>float [N+1]</code> The amplitude coefficients, \\(F_n\\). \\(F_0\\) needs to be provided even though it is not used.</p> required <code>coefs_phase</code> <code>Union[ndarray, List[float]]</code> <p><code>float [N]</code> The phase coefficients in radians, \\(\\Phi_n\\)</p> required <code>day_seconds</code> <code>float</code> <p>Duration of a day in seconds.</p> <code>86400</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>float [n_time]</code> Value of derivative to Fourier series solution at each time. Units is units of \\(F\\) divided by seconds.</p> Source code in <code>isca_tools/utils/fourier.py</code> <pre><code>def fourier_series_deriv(time: np.ndarray, coefs_amp: Union[np.ndarray, List[float]],\n                         coefs_phase: Union[np.ndarray, List[float]], day_seconds: float = 86400) -&gt; np.ndarray:\n    \"\"\"\n    For $N$ harmonics, the derivative of a fourier series with frequency $f$ is:\n\n    $$\\\\frac{dF}{dt} \\\\approx -\\\\sum_{n=1}^{N} 2n\\\\pi fF_n\\\\sin(2n\\\\pi ft - \\\\Phi_n)$$\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Time in days (assumes periodic e.g. annual mean, so `time = np.arange(360)`) covering entire period\n            such that period is `time[-1] - time[0] + 1`.\n        coefs_amp: `float [N+1]`&lt;/br&gt;\n            The amplitude coefficients, $F_n$. $F_0$ needs to be provided even though it is not used.\n        coefs_phase: `float [N]`&lt;/br&gt;\n            The phase coefficients in radians, $\\Phi_n$\n        day_seconds: Duration of a day in seconds.\n\n    Returns:\n        `float [n_time]`&lt;/br&gt;\n            Value of derivative to Fourier series solution at each time. Units is units of $F$ divided by seconds.\n    \"\"\"\n    period = float(time[-1] - time[0] + 1)\n    n_harmonics = len(coefs_amp)\n    ans = np.zeros_like(time, dtype=float)\n    for n in range(1, n_harmonics):\n        ans -= coefs_amp[n] * np.sin(2*n*np.pi*time/period - coefs_phase[n-1]) * (2*n*np.pi/period)\n    return ans / day_seconds     # convert units from per day to per second\n</code></pre>"},{"location":"code/utils/fourier/#isca_tools.utils.fourier.get_fourier_coef","title":"<code>get_fourier_coef(time, var, n, integ_method='spline')</code>","text":"<p>This calculates the analytic solution for the amplitude and phase coefficients for the <code>n</code>th harmonic \ud83d\udd17</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>ndarray</code> <p><code>float [n_time]</code> Time in days (assumes periodic e.g. annual mean, so <code>time = np.arange(360)</code>) covering entire period such that period is <code>time[-1] - time[0] + 1</code>.</p> required <code>var</code> <code>ndarray</code> <p><code>float [n_time]</code> Variable to fit fourier series to.</p> required <code>n</code> <code>int</code> <p>Harmonic to find coefficients for, if 0, will just return amplitude coefficient. Otherwise, will return an amplitude and phase coefficient.</p> required <code>integ_method</code> <code>str</code> <p>How to perform the integration. If <code>spline</code>, will fit a spline and then integrate the spline, otherwise will use <code>scipy.integrate.simpson</code>.</p> <code>'spline'</code> <p>Returns:     <code>amp_coef</code>: The amplitude fourier coefficient \\(F_n\\).     <code>phase_coef</code>: The phase fourier coefficient \\(\\Phi_n\\). Will not return if \\(n=0\\).</p> Source code in <code>isca_tools/utils/fourier.py</code> <pre><code>def get_fourier_coef(time: np.ndarray, var: np.ndarray, n: int,\n                     integ_method: str = 'spline') -&gt; [Union[float, Tuple[float, float]]]:\n    \"\"\"\n    This calculates the analytic solution for the amplitude and phase coefficients for the `n`th harmonic\n    [\ud83d\udd17](https://www.bragitoff.com/2021/05/fourier-series-coefficients-and-visualization-python-program/)\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Time in days (assumes periodic e.g. annual mean, so `time = np.arange(360)`) covering entire period\n            such that period is `time[-1] - time[0] + 1`.\n        var: `float [n_time]`&lt;/br&gt;\n            Variable to fit fourier series to.\n        n: Harmonic to find coefficients for, if 0, will just return amplitude coefficient.\n            Otherwise, will return an amplitude and phase coefficient.\n        integ_method: How to perform the integration.&lt;/br&gt;\n            If `spline`, will fit a spline and then integrate the spline, otherwise will use `scipy.integrate.simpson`.\n    Returns:\n        `amp_coef`: The amplitude fourier coefficient $F_n$.\n        `phase_coef`: The phase fourier coefficient $\\\\Phi_n$. Will not return if $n=0$.\n    \"\"\"\n    # Computes the analytical fourier coefficients for the n harmonic of a given function\n    # With integrate method = spline works very well i.e. fit spline then use spline.integrate functionality\n    # Otherwise, there are problems with the integration especially at the limits e.g. t=0 and t=T.\n    period = float(time[-1] - time[0] + 1)\n    if integ_method == 'spline':\n        var = np.append(var, var[0])\n        time = np.append(time, time[-1]+1)\n    if n == 0:\n        if integ_method == 'spline':\n            spline = CubicSpline(time, var, bc_type='periodic')\n            return 2/period * spline.integrate(0, period)\n        else:\n            return 2/period * scipy.integrate.simpson(var, time)\n    else:\n        # constants for acos(t) + bsin(t) form\n        if integ_method == 'spline':\n            spline = CubicSpline(time,var * np.cos(2*n*np.pi*time/period), bc_type='periodic')\n            cos_coef = 2/period * spline.integrate(0, period)\n            sin_curve = var * np.sin(2*n*np.pi*time/period)\n            sin_curve[-1] = 0       # Need first and last value to be the same to be periodic spline\n                                    # usually have last value equal 1e-10 so not equal\n            spline = CubicSpline(time,sin_curve, bc_type='periodic')\n            sin_coef = 2/period * spline.integrate(0, period)\n        else:\n            cos_coef = 2/period * scipy.integrate.simpson(var * np.cos(2*n*np.pi*time/period), time)\n            sin_coef = 2/period * scipy.integrate.simpson(var * np.sin(2*n*np.pi*time/period), time)\n        # constants for Acos(t-phi) form\n        phase_coef = np.arctan(sin_coef/cos_coef)\n        amp_coef = cos_coef / np.cos(phase_coef)\n        return amp_coef, phase_coef\n</code></pre>"},{"location":"code/utils/fourier/#isca_tools.utils.fourier.get_fourier_fit","title":"<code>get_fourier_fit(time, var, n_harmonics, integ_method='spline')</code>","text":"<p>Obtains the Fourier series solution for \\(F=\\)<code>var</code>, using \\(N=\\)<code>n_harmonics</code>:</p> \\[F(t) \\approx \\frac{F_0}{2} + \\sum_{n=1}^{N} F_n\\cos(2n\\pi ft - \\Phi_n)\\] <p>Parameters:</p> Name Type Description Default <code>time</code> <code>ndarray</code> <p><code>float [n_time]</code> Time in days (assumes periodic e.g. annual mean, so <code>time = np.arange(360)</code>) covering entire period such that period (\\(1/f\\)) is <code>time[-1] - time[0] + 1</code>.</p> required <code>var</code> <code>ndarray</code> <p><code>float [n_time]</code> Variable to fit fourier series to.</p> required <code>n_harmonics</code> <code>int</code> <p>Number of harmonics to use to fit fourier series, \\(N\\).</p> required <code>integ_method</code> <code>str</code> <p>How to perform the integration when obtaining Fourier coefficients. If <code>spline</code>, will fit a spline and then integrate the spline, otherwise will use <code>scipy.integrate.simpson</code>.</p> <code>'spline'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>fourier_solution</code>: <code>float [n_time]</code> The Fourier series solution that was fit to <code>var</code>.</p> <code>ndarray</code> <p><code>amp_coef</code>: <code>float [n_harmonics+1]</code> The amplitude Fourier coefficients \\(F_n\\).</p> <code>ndarray</code> <p><code>phase_coef</code>: <code>float [n_harmonics]</code> The phase Fourier coefficients \\(\\Phi_n\\).</p> Source code in <code>isca_tools/utils/fourier.py</code> <pre><code>def get_fourier_fit(time: np.ndarray, var: np.ndarray, n_harmonics: int,\n                    integ_method: str = 'spline') -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Obtains the Fourier series solution for $F=$`var`, using $N=$`n_harmonics`:\n\n    $$F(t) \\\\approx \\\\frac{F_0}{2} + \\\\sum_{n=1}^{N} F_n\\\\cos(2n\\\\pi ft - \\\\Phi_n)$$\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Time in days (assumes periodic e.g. annual mean, so `time = np.arange(360)`) covering entire period\n            such that period ($1/f$) is `time[-1] - time[0] + 1`.\n        var: `float [n_time]`&lt;/br&gt;\n            Variable to fit fourier series to.\n        n_harmonics: Number of harmonics to use to fit fourier series, $N$.\n        integ_method: How to perform the integration when obtaining Fourier coefficients.&lt;/br&gt;\n            If `spline`, will fit a spline and then integrate the spline, otherwise will use `scipy.integrate.simpson`.\n\n    Returns:\n        `fourier_solution`: `float [n_time]`&lt;/br&gt;\n            The Fourier series solution that was fit to `var`.\n        `amp_coef`: `float [n_harmonics+1]`&lt;/br&gt;\n            The amplitude Fourier coefficients $F_n$.\n        `phase_coef`: `float [n_harmonics]`&lt;/br&gt;\n            The phase Fourier coefficients $\\\\Phi_n$.\n    \"\"\"\n    # Returns the fourier fit of a function using a given number of harmonics\n    amp_coefs = np.zeros(n_harmonics+1)\n    phase_coefs = np.zeros(n_harmonics)\n    amp_coefs[0] = get_fourier_coef(time, var, 0, integ_method)\n    for i in range(1, n_harmonics+1):\n        amp_coefs[i], phase_coefs[i-1] = get_fourier_coef(time, var, i, integ_method)\n    return fourier_series(time, amp_coefs, phase_coefs), amp_coefs, phase_coefs\n</code></pre>"},{"location":"code/utils/land/","title":"Land","text":""},{"location":"code/utils/land/#isca_tools.utils.land.get_land_coords","title":"<code>get_land_coords(namelist_file=None, land_file=None)</code>","text":"<p>Returns the latitude and longitude coordinates that correspond to land for a particular experiment.</p> <p>Parameters:</p> Name Type Description Default <code>namelist_file</code> <code>Optional[str]</code> <p>File path to namelist <code>nml</code> file for the experiment. The location of the land <code>nc</code> file will be obtained from <code>input_dir</code> and <code>land_file_name</code> in this. Not required if <code>land_file</code> given.</p> <code>None</code> <code>land_file</code> <code>Optional[str]</code> <p>File path to the land <code>nc</code> file used for the experiment. Not required if <code>namelist_file</code> given.</p> <code>None</code> <p>Returns:</p> Type Description <code>[ndarray, ndarray]</code> <p><code>land_lat</code>: <code>float [n_land_coords]</code> Land is present at the coordinate indicated by (<code>land_lat[i]</code>, <code>land_lon[i]</code>) for all <code>i</code>. Units are degrees (\\(-90 \\leq \\phi \\leq 90\\)).</p> <code>[ndarray, ndarray]</code> <p><code>land_lon</code>: <code>float [n_land_coords]</code> Land is present at the coordinate indicated by (<code>land_lat[i]</code>, <code>land_lon[i]</code>) for all <code>i</code>. Units are degrees (\\(0 \\leq \\lambda \\leq 360\\)).</p> Source code in <code>isca_tools/utils/land.py</code> <pre><code>def get_land_coords(namelist_file: Optional[str] = None, land_file: Optional[str] = None) -&gt; [np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the latitude and longitude coordinates that correspond to land for a particular experiment.\n\n    Args:\n        namelist_file: File path to namelist `nml` file for the experiment.\n            The location of the land `nc` file will be obtained from `input_dir` and `land_file_name` in this.\n            Not required if `land_file` given.\n        land_file: File path to the land `nc` file used for the experiment.\n            Not required if `namelist_file` given.\n\n    Returns:\n        `land_lat`: `float [n_land_coords]`&lt;/br&gt;\n            Land is present at the coordinate indicated by (`land_lat[i]`, `land_lon[i]`) for all `i`.&lt;/br&gt;\n            Units are degrees ($-90 \\leq \\phi \\leq 90$).\n        `land_lon`: `float [n_land_coords]`&lt;/br&gt;\n            Land is present at the coordinate indicated by (`land_lat[i]`, `land_lon[i]`) for all `i`.&lt;/br&gt;\n            Units are degrees ($0 \\leq \\lambda \\leq 360$).\n    \"\"\"\n    land_data = load_land_file(namelist_file, land_file)\n    # Get 2 arrays, one for latitude and one for longitude indicating where the indices where land is\n    land_ind = np.where(land_data.variables['land_mask'][:] &gt; 0)\n    # See which dimension corresponds to which array (either 0 or 1)\n    lat_dim_ind = np.where(np.asarray(land_data.variables['land_mask'].dimensions) == 'lat')[0][0]\n    lon_dim_ind = np.where(np.asarray(land_data.variables['land_mask'].dimensions) == 'lon')[0][0]\n\n    land_lat = np.asarray(land_data.variables['lat'][land_ind[lat_dim_ind]])\n    land_lon = np.asarray(land_data.variables['lon'][land_ind[lon_dim_ind]])\n    return land_lat, land_lon\n</code></pre>"},{"location":"code/utils/land/#isca_tools.utils.land.get_ocean_coords","title":"<code>get_ocean_coords(namelist_file=None, land_file=None)</code>","text":"<p>Returns the latitude and longitude coordinates that correspond to ocean for a particular experiment.</p> <p>Parameters:</p> Name Type Description Default <code>namelist_file</code> <code>Optional[str]</code> <p>File path to namelist <code>nml</code> file for the experiment. The location of the land <code>nc</code> file will be obtained from <code>input_dir</code> and <code>land_file_name</code> in this. Not required if <code>land_file</code> given.</p> <code>None</code> <code>land_file</code> <code>Optional[str]</code> <p>File path to the land <code>nc</code> file used for the experiment. Not required if <code>namelist_file</code> given.</p> <code>None</code> <p>Returns:</p> Type Description <code>[ndarray, ndarray]</code> <p><code>ocean_lat</code>: <code>float [n_ocean_coords]</code> Ocean is present at the coordinate indicated by (<code>ocean_lat[i]</code>, <code>ocean_lon[i]</code>) for all <code>i</code>. Units are degrees (\\(-90 \\leq \\phi \\leq 90\\)).</p> <code>[ndarray, ndarray]</code> <p><code>ocean_lon</code>: <code>float [n_ocean_coords]</code> Ocean is present at the coordinate indicated by (<code>ocean_lat[i]</code>, <code>ocean_lon[i]</code>) for all <code>i</code>. Units are degrees (\\(0 \\leq \\lambda \\leq 360\\)).</p> Source code in <code>isca_tools/utils/land.py</code> <pre><code>def get_ocean_coords(namelist_file: Optional[str] = None, land_file: Optional[str] = None) -&gt; [np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the latitude and longitude coordinates that correspond to ocean for a particular experiment.\n\n    Args:\n        namelist_file: File path to namelist `nml` file for the experiment.\n            The location of the land `nc` file will be obtained from `input_dir` and `land_file_name` in this.\n            Not required if `land_file` given.\n        land_file: File path to the land `nc` file used for the experiment.\n            Not required if `namelist_file` given.\n\n    Returns:\n        `ocean_lat`: `float [n_ocean_coords]`&lt;/br&gt;\n            Ocean is present at the coordinate indicated by (`ocean_lat[i]`, `ocean_lon[i]`) for all `i`.&lt;/br&gt;\n            Units are degrees ($-90 \\leq \\phi \\leq 90$).\n        `ocean_lon`: `float [n_ocean_coords]`&lt;/br&gt;\n            Ocean is present at the coordinate indicated by (`ocean_lat[i]`, `ocean_lon[i]`) for all `i`.&lt;/br&gt;\n            Units are degrees ($0 \\leq \\lambda \\leq 360$).\n    \"\"\"\n    # Load land info\n    land_data = load_land_file(namelist_file, land_file)\n    land_lat, land_lon = get_land_coords(namelist_file, land_file)\n    lat_lon_land = np.concatenate((land_lat.reshape(-1, 1), land_lon.reshape(-1, 1)), axis=1)\n\n    # Get grid of all possible coordinates - land or ocean\n    lat_all, lon_all = np.meshgrid(np.asarray(land_data.variables['lat'][:]),\n                                   np.asarray(land_data.variables['lon'][:]))\n    lat_lon_all = np.concatenate((lat_all.reshape(-1, 1), lon_all.reshape(-1, 1)), axis=1)\n\n    # Find index of land coords in lat_lon_all\n    land_inds = numpy_indexed.indices(lat_lon_all, lat_lon_land)\n    if len(land_inds) != len(land_lat):\n        raise ValueError(f\"There are {len(land_lat)} land coordinates but {len(land_inds)} found in full \"\n                         f\"latitude/longitude coordinate grid.\")\n\n    # Ocean coords are those which are not land\n    ocean_inds = np.setdiff1d(np.arange(lat_lon_all.shape[0]), land_inds)\n    lat_lon_ocean = lat_lon_all[ocean_inds]\n    return lat_lon_ocean[:, 0], lat_lon_ocean[:, 1]\n</code></pre>"},{"location":"code/utils/land/#isca_tools.utils.land.load_land_file","title":"<code>load_land_file(namelist_file=None, land_file=None)</code>","text":"<p>Loads in the land data file for a given experiment.</p> <p>Parameters:</p> Name Type Description Default <code>namelist_file</code> <code>Optional[str]</code> <p>File path to namelist <code>nml</code> file for the experiment. The location of the land <code>nc</code> file will be obtained from <code>input_dir</code> and <code>land_file_name</code> in this. Not required if <code>land_file</code> given.</p> <code>None</code> <code>land_file</code> <code>Optional[str]</code> <p>File path to the land <code>nc</code> file used for the experiment. Not required if <code>namelist_file</code> given.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>land dataset containing the variables <code>land_mask</code>, <code>zsurf</code>, <code>lat</code> and <code>lon</code>.</p> Source code in <code>isca_tools/utils/land.py</code> <pre><code>def load_land_file(namelist_file: Optional[str] = None, land_file: Optional[str] = None) -&gt; Dataset:\n    \"\"\"\n    Loads in the land data file for a given experiment.\n\n    Args:\n        namelist_file: File path to namelist `nml` file for the experiment.\n            The location of the land `nc` file will be obtained from `input_dir` and `land_file_name` in this.\n            Not required if `land_file` given.\n        land_file: File path to the land `nc` file used for the experiment.\n            Not required if `namelist_file` given.\n\n    Returns:\n        land dataset containing the variables `land_mask`, `zsurf`, `lat` and `lon`.\n    \"\"\"\n    if land_file is None:\n        if namelist_file is None:\n            raise ValueError(f\"Atleast one of namelist_file or land_file must be specified but both are None\")\n        # Determine land file name from info in namelist\n        namelist = load_namelist(namelist_file=namelist_file)\n        land_file = os.path.join(namelist['experiment_details']['input_dir'],\n                                 namelist['idealized_moist_phys_nml']['land_file_name'].replace('INPUT/', ''))\n    land_data = Dataset(land_file, 'r', format='NETCDF3_CLASSIC')\n    return land_data\n</code></pre>"},{"location":"code/utils/load/","title":"Load","text":""},{"location":"code/utils/load/#isca_tools.utils.load.get_file_suffix","title":"<code>get_file_suffix(dir, suffix)</code>","text":"<p>Returns a list of all files in <code>dir</code> which end in <code>suffix</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Directory of interest.</p> required <code>suffix</code> <code>str</code> <p>Usually the file type of interest e.g. <code>.nml</code> or <code>.txt</code>.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of all files with the correct <code>suffix</code>.</p> Source code in <code>isca_tools/utils/load.py</code> <pre><code>def get_file_suffix(dir: str, suffix: str) -&gt; List[str]:\n    \"\"\"\n    Returns a list of all files in `dir` which end in `suffix`.\n\n    Args:\n        dir: Directory of interest.\n        suffix: Usually the file type of interest e.g. `.nml` or `.txt`.\n\n    Returns:\n        List of all files with the correct `suffix`.\n    \"\"\"\n    file_name = []\n    for file in os.listdir(dir):\n        if file.endswith(suffix):\n            file_name += [file]\n    return file_name\n</code></pre>"},{"location":"code/utils/load/#isca_tools.utils.load.load_dataset","title":"<code>load_dataset(exp_name, run_no=None, data_dir=None, decode_times=False, use_cftime=True)</code>","text":"<p>This loads a dataset produced by Isca containing all the diagnostics specified.</p> <p>Parameters:</p> Name Type Description Default <code>exp_name</code> <code>str</code> <p>Name of folder in <code>data_dir</code> where data for this experiment was saved.</p> required <code>run_no</code> <code>Optional[int]</code> <p>Data is saved at intervals in time specified in the <code>main_nml</code> namelist of the namelist.nml file. This is typically monthly and the <code>run_no</code> would then refer to the month to load data for. If <code>None</code>, data for all months is loaded and combined into a single Dataset.</p> <code>None</code> <code>data_dir</code> <code>Optional[str]</code> <p>Directory which contains the <code>exp_name</code> directory. If <code>None</code>, will assume this is the directory specified through the environmental variable <code>GFDL_DATA</code>.</p> <code>None</code> <code>decode_times</code> <code>bool</code> <p>If <code>True</code>, decode times into datetime objects</p> <code>False</code> <code>use_cftime</code> <code>bool</code> <p>If <code>True</code>, will decode times into <code>cftime.datetime</code> objects, rather than <code>np.datetime64</code> objects. Only relevant if <code>decode_times</code> is <code>True</code>. Useful if using 360 day calendar.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing all diagnostics specified for the experiment.</p> Source code in <code>isca_tools/utils/load.py</code> <pre><code>def load_dataset(exp_name: str, run_no: Optional[int] = None,\n                 data_dir: Optional[str] = None, decode_times: bool = False,\n                 use_cftime: bool = True) -&gt; xr.Dataset:\n    \"\"\"\n    This loads a dataset produced by Isca containing all the diagnostics specified.\n\n    Args:\n        exp_name: Name of folder in `data_dir` where data for this experiment was saved.\n        run_no: Data is saved at intervals in time specified in the `main_nml` namelist of the *namelist.nml* file.\n            This is typically monthly and the `run_no` would then refer to the month to load data for.\n            If `None`, data for all months is loaded and combined into a single Dataset.\n        data_dir: Directory which contains the `exp_name` directory. If `None`, will assume this is\n            the directory specified through the environmental variable `GFDL_DATA`.\n        decode_times: If `True`, decode times into datetime objects\n        use_cftime: If `True`, will decode times into `cftime.datetime` objects,\n            rather than `np.datetime64` objects. Only relevant if `decode_times` is `True`.\n            Useful if using 360 day calendar.\n\n    Returns:\n        Dataset containing all diagnostics specified for the experiment.\n\n    \"\"\"\n    if data_dir is None:\n        data_dir = os.environ['GFDL_DATA']\n    exp_dir = os.path.join(data_dir, exp_name)\n\n    # Get index of first run file i.e. which month saved first\n    files_run = [filename for filename in os.listdir(exp_dir) if filename.startswith('run')]\n    files_run.sort()\n    first_month = int(files_run[0][-4:])\n    if first_month != 1:\n        warnings.warn(f'First month saved is {first_month} not 1.')\n\n    # File name is the same for all runs and is the only file with the suffix '.nc' in the run folder\n    file_name = get_file_suffix(os.path.join(exp_dir, 'run%04d' % first_month), '.nc')[0]\n\n    if run_no is None:\n        data_file = os.path.join(exp_dir, 'run*', file_name)\n        d = xr.open_mfdataset(data_file, concat_dim='time', combine='nested',\n                              decode_times=decode_times, use_cftime=use_cftime)\n    else:\n        data_file = os.path.join(exp_dir, 'run%04d' % run_no, file_name)\n        d = xr.open_dataset(data_file, decode_times=decode_times, use_cftime=use_cftime)\n    return d\n</code></pre>"},{"location":"code/utils/load/#isca_tools.utils.load.load_namelist","title":"<code>load_namelist(exp_name=None, data_dir=None, namelist_file=None)</code>","text":"<p>Returns all the namelists options and their corresponding values specified in the namelist .nml file for the experiment indicated by <code>exp_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>exp_name</code> <code>Optional[str]</code> <p>Name of folder in <code>data_dir</code> where data for this experiment was saved.</p> <code>None</code> <code>data_dir</code> <code>Optional[str]</code> <p>Directory which contains the <code>exp_name</code> directory. If <code>None</code>, will assume this is the directory specified through the environmental variable <code>GFDL_DATA</code>.</p> <code>None</code> <code>namelist_file</code> <code>Optional[str]</code> <p>Path to the namelist .nml file to load. Use this option if data for the experiment has not been created yet.</p> <code>None</code> <p>Returns:     Namelist values used for this experiment.</p> Source code in <code>isca_tools/utils/load.py</code> <pre><code>def load_namelist(exp_name: Optional[str] = None, data_dir: Optional[str] = None,\n                  namelist_file: Optional[str] = None) -&gt; f90nml.Namelist:\n    \"\"\"\n    Returns all the namelists options and their corresponding values specified in the namelist *.nml* file\n    for the experiment indicated by `exp_name`.\n\n    Args:\n        exp_name: Name of folder in `data_dir` where data for this experiment was saved.\n        data_dir: Directory which contains the `exp_name` directory. If `None`, will assume this is\n            the directory specified through the environmental variable `GFDL_DATA`.\n        namelist_file: Path to the namelist *.nml* file to load. Use this option if data for the experiment has not\n            been created yet.\n    Returns:\n        Namelist values used for this experiment.\n    \"\"\"\n    if namelist_file is not None:\n        # Make sure file_path has the .nml suffix\n        file_path = namelist_file.replace('.nml', '')\n        file_path = file_path + '.nml'\n    else:\n        if data_dir is None:\n            data_dir = os.environ['GFDL_DATA']\n        exp_dir = os.path.join(data_dir, exp_name)\n\n        # Get index of first run file i.e. which month saved first\n        files_run = [filename for filename in os.listdir(exp_dir) if filename.startswith('run')]\n        files_run.sort()\n        first_month = int(files_run[0][-4:])\n\n        # Namelist file_name is the same for all runs and is the only file with the suffix '.nml' in the run folder\n        file_name = get_file_suffix(os.path.join(exp_dir, 'run%04d' % first_month), '.nml')[0]\n        file_path = os.path.join(exp_dir, 'run%04d' % first_month, file_name)\n    return f90nml.read(file_path)\n</code></pre>"},{"location":"code/utils/moist_physics/","title":"Moist Physics","text":""},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.clausius_clapeyron_factor","title":"<code>clausius_clapeyron_factor(temp, pressure)</code>","text":"<p>This is the factor \\(\\alpha\\), such that \\(dq^*/dT = \\alpha q^*\\).</p> <p>I explicitly compute alpha from the formula for <code>saturation_vapor_pressure</code> in the function here.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>ndarray</code> <p>Temperature at each coordinate considered. Units: Kelvin.</p> required <code>pressure</code> <code>Union[float, ndarray]</code> <p>Pressure level in Pa, temperature corresponds to. If all <code>temp</code> are at the lowest atmospheric level, then pressure<code>will be the lowest level pressure i.e. a</code>float`.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Clausius clapeyron factor, \\(\\alpha\\). Units: Kelvin\\(^{-1}\\)</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def clausius_clapeyron_factor(temp: np.ndarray, pressure: Union[float, np.ndarray]) -&gt; np.ndarray:\n    \"\"\"\n    This is the factor $\\\\alpha$, such that $dq^*/dT = \\\\alpha q^*$.\n\n    I explicitly compute alpha from the formula for `saturation_vapor_pressure` in the function here.\n\n    Args:\n        temp: Temperature at each coordinate considered. Units: *Kelvin*.\n        pressure: Pressure level in *Pa*, temperature corresponds to.\n            If all `temp` are at the lowest atmospheric level, then pressure` will be the lowest level pressure i.e.\n            a `float`.\n\n\n    Returns:\n        Clausius clapeyron factor, $\\\\alpha$. Units: *Kelvin$^{-1}$*\n    \"\"\"\n    lambda_const = 4302.645 / (temp - 29.65) ** 2\n    return lambda_const * pressure / epsilon * sphum_sat(temp, pressure) / saturation_vapor_pressure(temp)\n</code></pre>"},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.get_density","title":"<code>get_density(temp, pressure, sphum=None)</code>","text":"<p>Equation for density using ideal gas equation of state: \\(\\rho = \\frac{p}{RT}\\). If specific humidity given, will compute density using virtual temperature, \\(T_v\\).</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code> Temperature in K to find density at.</p> required <code>pressure</code> <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code> Pressure in Pa to find density at.</p> required <code>sphum</code> <code>Optional[Union[float, ndarray]]</code> <p><code>float [n_p_levels]</code> Specific humidity in kg/kg to find density at.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Density in units of \\(kg/m^3\\).</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def get_density(temp: Union[float, np.ndarray], pressure: Union[float, np.ndarray],\n                sphum: Optional[Union[float, np.ndarray]] = None) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Equation for density using ideal gas equation of state: $\\\\rho = \\\\frac{p}{RT}$.\n    If specific humidity given, will compute density using virtual temperature, $T_v$.\n\n    Args:\n        temp: `float [n_p_levels]`\n            Temperature in *K* to find density at.\n        pressure: `float [n_p_levels]`\n            Pressure in *Pa* to find density at.\n        sphum: `float [n_p_levels]`\n            Specific humidity in *kg/kg* to find density at.\n\n    Returns:\n        Density in units of $kg/m^3$.\n    \"\"\"\n    if sphum is None:\n        return pressure / (R * temp)\n    else:\n        return pressure / (R * virtual_temp(temp, sphum))\n</code></pre>"},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.mixing_ratio_from_partial_pressure","title":"<code>mixing_ratio_from_partial_pressure(partial_pressure, total_pressure)</code>","text":"<p>Computes the mixing ratio, \\(w\\), from partial pressure, \\(e\\), and total atmospheric pressure, \\(p\\), according to:</p> <p>\\(w = \\epsilon \\frac{e}{p-e}\\)</p> <p>Where \\(\\epsilon = R_d/R_v = 0.622\\) is the ratio of molecular weight of water to that of dry air.</p> <p>This is the same equation used by MetPy.</p> <p>Parameters:</p> Name Type Description Default <code>partial_pressure</code> <code>Union[float, ndarray]</code> <p><code>float [n_levels]</code>. Partial pressure at each level, \\(e\\), in Pa.</p> required <code>total_pressure</code> <code>Union[float, ndarray]</code> <p><code>float [n_levels]</code>. Atmospheric pressure at each level, \\(p\\), in Pa.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Mixing ratio, \\(w\\), in units of \\(kg/kg\\).</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def mixing_ratio_from_partial_pressure(partial_pressure: Union[float, np.ndarray],\n                                       total_pressure: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Computes the mixing ratio, $w$, from partial pressure, $e$, and total atmospheric pressure, $p$, according to:\n\n    $w = \\epsilon \\\\frac{e}{p-e}$\n\n    Where $\\epsilon = R_d/R_v = 0.622$ is the ratio of molecular weight of water to that of dry air.\n\n    This is the same equation used by\n    [MetPy](https://unidata.github.io/MetPy/latest/api/generated/metpy.calc.mixing_ratio.html).\n\n    Args:\n        partial_pressure: `float [n_levels]`. Partial pressure at each level, $e$, in *Pa*.\n        total_pressure: `float [n_levels]`. Atmospheric pressure at each level, $p$, in *Pa*.\n\n    Returns:\n        Mixing ratio, $w$, in units of $kg/kg$.\n    \"\"\"\n    return epsilon * partial_pressure / (total_pressure - partial_pressure)\n</code></pre>"},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.mixing_ratio_from_sphum","title":"<code>mixing_ratio_from_sphum(sphum)</code>","text":"<p>Computes the mixing ratio, \\(w\\), from specific humidity, \\(q\\), according to:</p> <p>\\(w = q/(1-q)\\)</p> <p>Parameters:</p> Name Type Description Default <code>sphum</code> <code>Union[float, ndarray]</code> <p>Specific humidity, \\(q\\), in units of \\(kg/kg\\).</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Mixing ratio, \\(w\\), in units of \\(kg/kg\\).</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def mixing_ratio_from_sphum(sphum: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Computes the mixing ratio, $w$, from specific humidity, $q$, according to:\n\n    $w = q/(1-q)$\n\n    Args:\n        sphum: Specific humidity, $q$, in units of $kg/kg$.\n\n    Returns:\n        Mixing ratio, $w$, in units of $kg/kg$.\n    \"\"\"\n    return sphum / (1 - sphum)\n</code></pre>"},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.moist_static_energy","title":"<code>moist_static_energy(temp, sphum, height, c_p_const=c_p)</code>","text":"<p>Returns the moist static energy in units of kJ/kg.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>Union[ndarray, float]</code> <p><code>float [n_lat, n_p_levels]</code>. Temperature at each coordinate considered. Units: Kelvin.</p> required <code>sphum</code> <code>Union[ndarray, float]</code> <p><code>float [n_lat, n_p_levels]</code>. Specific humidity at each coordinate considered. Units: kg/kg.</p> required <code>height</code> <code>Union[ndarray, float]</code> <p><code>float [n_lat, n_p_levels]</code> or <code>float</code>. Geopotential height of each level considered. Just a <code>float</code> if only one pressure level considered for each latitude e.g. common to use 2m values. Units: m.</p> required <code>c_p_const</code> <code>float</code> <p>Heat capacity constant in units of J/K/kg. This gives the option to easily modify the moist static energy but almost always be kept at default <code>c_p</code>.</p> <code>c_p</code> <p>Returns:</p> Type Description <code>Union[ndarray, float]</code> <p>Moist static energy at each coordinate given.</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def moist_static_energy(temp: Union[np.ndarray, float], sphum: Union[np.ndarray, float],\n                        height: Union[np.ndarray, float],\n                        c_p_const: float = c_p) -&gt; Union[np.ndarray, float]:\n    \"\"\"\n    Returns the moist static energy in units of *kJ/kg*.\n\n    Args:\n        temp: `float [n_lat, n_p_levels]`. Temperature at each coordinate considered. Units: *Kelvin*.\n        sphum: `float [n_lat, n_p_levels]`. Specific humidity at each coordinate considered. Units: *kg/kg*.\n        height: `float [n_lat, n_p_levels]` or `float`. Geopotential height of each level considered.\n            Just a `float` if only one pressure level considered for each latitude e.g. common to use 2m values.\n            Units: *m*.\n        c_p_const: Heat capacity constant in units of J/K/kg.\n            This gives the option to easily modify the moist static energy but almost always be kept at default `c_p`.\n\n    Returns:\n        Moist static energy at each coordinate given.\n    \"\"\"\n    return (L_v * sphum + c_p_const * temp + g * height) / 1000\n</code></pre>"},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.partial_pressure_from_sphum","title":"<code>partial_pressure_from_sphum(sphum, total_pressure)</code>","text":"<p>Computes the partial pressure, \\(e\\), from specific humidity, \\(q\\), and total atmospheric pressure, \\(p\\), according to:</p> <p>\\(e = wp / (\\epsilon+w)\\)</p> <p>where \\(w\\) is the mixing ratio, calculated using <code>mixing_ratio_from_sphum</code>, and \\(\\epsilon = R_d/R_v = 0.622\\) is the ratio of molecular weight of water to that of dry air.</p> <p>Parameters:</p> Name Type Description Default <code>sphum</code> <code>Union[float, ndarray]</code> <p>Specific humidity, \\(q\\), in units of \\(kg/kg\\).</p> required <code>total_pressure</code> <code>Union[float, ndarray]</code> <p><code>float [n_levels]</code>. Atmospheric pressure at each level, \\(p\\), in Pa.</p> required <p>Returns:</p> Name Type Description <code>partial_pressure</code> <code>Union[float, ndarray]</code> <p><code>float [n_levels]</code>. Partial pressure at each level, \\(e\\), in Pa.</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def partial_pressure_from_sphum(sphum: Union[float, np.ndarray],\n                                total_pressure: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Computes the partial pressure, $e$, from specific humidity, $q$, and total atmospheric pressure, $p$, according to:\n\n    $e = wp / (\\epsilon+w)$\n\n    where $w$ is the mixing ratio, calculated using `mixing_ratio_from_sphum`, and\n    $\\epsilon = R_d/R_v = 0.622$ is the ratio of molecular weight of water to that of dry air.\n\n    Args:\n        sphum: Specific humidity, $q$, in units of $kg/kg$.\n        total_pressure: `float [n_levels]`. Atmospheric pressure at each level, $p$, in *Pa*.\n\n    Returns:\n        partial_pressure: `float [n_levels]`.\n            Partial pressure at each level, $e$, in *Pa*.\n    \"\"\"\n    w = mixing_ratio_from_sphum(sphum)\n    return w * total_pressure / (epsilon + w)\n</code></pre>"},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.rh_from_sphum","title":"<code>rh_from_sphum(sphum, temp, total_pressure)</code>","text":"<p>Relative humidity, \\(rh\\), is computed from specific humidity, \\(q\\) according to:</p> <p>\\(rh = e / e_s\\)</p> <p>Where, \\(e = pw/(\\epsilon + w)\\), is the partial pressure, \\(e_s\\) is the saturation partial pressure and  \\(w\\) is the mixing ratio.</p> <p>Parameters:</p> Name Type Description Default <code>sphum</code> <code>Union[float, ndarray]</code> <p><code>float [n_levels]</code>. Specific humidity, \\(q\\), at each level considered, in units of \\(kg/kg\\).</p> required <code>temp</code> <code>Union[float, ndarray]</code> <p><code>float [n_levels]</code>. Temperature at each level considered. Units: Kelvin.</p> required <code>total_pressure</code> <code>Union[float, ndarray]</code> <p><code>float [n_levels]</code>. Atmospheric pressure, \\(p\\), at each level considered in Pa.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Percentage relative humidity (\\(0 &lt; rh &lt; 100\\)).</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def rh_from_sphum(sphum: Union[float, np.ndarray], temp: Union[float, np.ndarray],\n                  total_pressure: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Relative humidity, $rh$, is computed from specific humidity, $q$ according to:\n\n    $rh = e / e_s$\n\n    Where, $e = pw/(\\epsilon + w)$, is the partial pressure, $e_s$ is the saturation partial pressure and\n     $w$ is the mixing ratio.\n\n    Args:\n        sphum: `float [n_levels]`. Specific humidity, $q$, at each level considered, in units of $kg/kg$.\n        temp: `float [n_levels]`. Temperature at each level considered. Units: *Kelvin*.\n        total_pressure: `float [n_levels]`. Atmospheric pressure, $p$, at each level considered in *Pa*.\n\n    Returns:\n        Percentage relative humidity ($0 &lt; rh &lt; 100$).\n    \"\"\"\n    sat_mix_ratio = mixing_ratio_from_partial_pressure(saturation_vapor_pressure(temp), total_pressure)\n    mix_ratio = mixing_ratio_from_sphum(sphum)\n    # return 100 * mix_ratio / sat_mix_ratio\n    return 100 * mix_ratio / (epsilon + mix_ratio) * (epsilon + sat_mix_ratio) / sat_mix_ratio\n</code></pre>"},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.saturation_vapor_pressure","title":"<code>saturation_vapor_pressure(temp)</code>","text":"<p>Computes the saturation vapor pressure, \\(e_s(T)\\), corresponding to a given temperature.</p> <p>Uses Equation 10 in Bolton 1980. Valid for \\(-35^\\circ C &lt; T &lt; 35^\\circ C\\).</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>Union[float, ndarray]</code> <p>Temperature to compute vapor pressure at. Units: Kelvin.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Saturation vapor pressure, \\(e_s(T)\\), in units of Pa.</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def saturation_vapor_pressure(temp: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Computes the saturation vapor pressure, $e_s(T)$, corresponding to a given temperature.\n\n    Uses *Equation 10* in *Bolton 1980*. Valid for $-35^\\circ C &lt; T &lt; 35^\\circ C$.\n\n    Args:\n        temp: Temperature to compute vapor pressure at. Units: *Kelvin*.\n\n    Returns:\n        Saturation vapor pressure, $e_s(T)$, in units of *Pa*.\n    \"\"\"\n    # Alternative equation from MATLAB exercise M9.2 in Holdon 2004\n    # return 611 * np.exp(L_v/R_v * (1/temp_kelvin_to_celsius - 1/temp))\n    temp = temp - temp_kelvin_to_celsius  # Convert temperature in kelvin to celsius, as celsius used for this formula.\n    # if np.abs(np.asarray(temp)).max() &gt; 35:\n    #     warnings.warn('This formula is only valid for $-35^\\circ C &lt; T &lt; 35^\\circ C$\\n'\n    #                   'At least one temperature given is outside this range.')\n    # Multiply by 100 below to convert from hPa to Pa.\n    return 611.2 * np.exp(17.67 * temp / (temp + 243.5))\n</code></pre>"},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.sphum_from_dew","title":"<code>sphum_from_dew(temp_dew, pressure)</code>","text":"<p>Calculates specific humidity from dew temperature at a given <code>pressure</code>. The dew temperature is defined such as the temperature at which the saturation vapour pressure is equal to the partial pressure i.e. \\(e_s(T_d) = e(T)\\) where \\(T_d\\) is the dew temperature, and \\(T\\) is the actual temperature.</p> <p>Parameters:</p> Name Type Description Default <code>temp_dew</code> <code>Union[float, ndarray]</code> <p><code>float [n_lat, n_levels]</code>. Dew temperature at each coordinate considered. Units: Kelvin.</p> required <code>pressure</code> <code>Union[float, ndarray]</code> <p><code>float [n_levels]</code>. Atmospheric pressure, \\(p\\), at each level considered in Pa.</p> required <p>Returns:</p> Name Type Description <code>specific_humidity</code> <code>Union[float, ndarray]</code> <p><code>float [n_lat, n_levels]</code>. Specific humidity, \\(q\\), in units of \\(kg/kg\\).</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def sphum_from_dew(temp_dew: Union[float, np.ndarray], pressure: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Calculates specific humidity from dew temperature at a given `pressure`.\n    The dew temperature is defined such as the temperature at which the saturation vapour pressure\n    is equal to the partial pressure i.e. $e_s(T_d) = e(T)$ where $T_d$ is the dew temperature, and\n    $T$ is the actual temperature.\n\n    Args:\n        temp_dew: `float [n_lat, n_levels]`. Dew temperature at each coordinate considered. Units: *Kelvin*.\n        pressure: `float [n_levels]`. Atmospheric pressure, $p$, at each level considered in *Pa*.\n\n    Returns:\n        specific_humidity: `float [n_lat, n_levels]`.\n            Specific humidity, $q$, in units of $kg/kg$.\n    \"\"\"\n    # dew temperature is defined as temp such that saturation vapour pressure equals partial pressure\n    e = saturation_vapor_pressure(temp_dew)\n    return sphum_from_partial_pressure(e, pressure)\n</code></pre>"},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.sphum_from_partial_pressure","title":"<code>sphum_from_partial_pressure(partial_pressure, total_pressure)</code>","text":"<p>Computes the specific humidity, \\(q\\), from partial pressure, \\(e\\), and total atmospheric pressure, \\(p\\), according to:</p> <p>\\(q = w / (1+w)\\)</p> <p>where \\(w\\) is the mixing ratio, calculated using <code>mixing_ratio_from_partial_pressure</code>.</p> <p>Parameters:</p> Name Type Description Default <code>partial_pressure</code> <code>Union[float, ndarray]</code> <p><code>float [n_levels]</code>. Partial pressure at each level, \\(e\\), in Pa.</p> required <code>total_pressure</code> <code>Union[float, ndarray]</code> <p><code>float [n_levels]</code>. Atmospheric pressure at each level, \\(p\\), in Pa.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Specific humidity, \\(q\\), in units of \\(kg/kg\\).</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def sphum_from_partial_pressure(partial_pressure: Union[float, np.ndarray],\n                                       total_pressure: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Computes the specific humidity, $q$, from partial pressure, $e$, and total atmospheric pressure, $p$, according to:\n\n    $q = w / (1+w)$\n\n    where $w$ is the mixing ratio, calculated using `mixing_ratio_from_partial_pressure`.\n\n    Args:\n        partial_pressure: `float [n_levels]`. Partial pressure at each level, $e$, in *Pa*.\n        total_pressure: `float [n_levels]`. Atmospheric pressure at each level, $p$, in *Pa*.\n\n    Returns:\n        Specific humidity, $q$, in units of $kg/kg$.\n    \"\"\"\n    w = mixing_ratio_from_partial_pressure(partial_pressure, total_pressure)\n    return w / (1+w)\n</code></pre>"},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.sphum_sat","title":"<code>sphum_sat(temp, pressure)</code>","text":"<p>Returns the saturation specific humidity, \\(q^*\\), in kg/kg.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>Union[float, ndarray]</code> <p>Temperature at each coordinate considered. Units: Kelvin.</p> required <code>pressure</code> <code>Union[float, ndarray]</code> <p>Pressure level in Pa, temperature corresponds to. If all <code>temp</code> are at the lowest atmospheric level, then pressure<code>will be the lowest level pressure i.e. a</code>float`.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Saturation specific humidity at each coordinate given.</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def sphum_sat(temp: Union[float, np.ndarray], pressure: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Returns the saturation specific humidity, $q^*$, in *kg/kg*.\n\n    Args:\n        temp: Temperature at each coordinate considered. Units: *Kelvin*.\n        pressure: Pressure level in *Pa*, temperature corresponds to.\n            If all `temp` are at the lowest atmospheric level, then pressure` will be the lowest level pressure i.e.\n            a `float`.\n\n    Returns:\n        Saturation specific humidity at each coordinate given.\n    \"\"\"\n    # Saturation specific humidity\n    w_sat = mixing_ratio_from_partial_pressure(saturation_vapor_pressure(temp), pressure)\n    q_sat = w_sat / (1 + w_sat)\n    return q_sat\n</code></pre>"},{"location":"code/utils/moist_physics/#isca_tools.utils.moist_physics.virtual_temp","title":"<code>virtual_temp(temp, sphum)</code>","text":"<p>Equation for virtual temperature using Isca code.</p> <p>The constants <code>d622</code>, <code>d378</code>, <code>d608</code> are to match the Isca code.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code> Temperature in K to find virtual potential temperature at.</p> required <code>sphum</code> <code>Union[float, ndarray]</code> <p><code>float [n_p_levels]</code> Specific humidity of parcel at each pressure level in kg/kg.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Virtual temperature at each pressure level in K.</p> Source code in <code>isca_tools/utils/moist_physics.py</code> <pre><code>def virtual_temp(temp: Union[float, np.ndarray], sphum: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Equation for virtual temperature using [Isca code](https://github.com/jduffield65/Isca/blob/b9249275469583c1723f12ac62333067f9460fea/isca_source/src/coupler/surface_flux.F90#L463).\n\n    The constants `d622`, `d378`, `d608` are to match the\n    [Isca code](https://github.com/jduffield65/Isca/blob/b9249275469583c1723f12ac62333067f9460fea/isca_source/src/coupler/surface_flux.F90#L935-L940).\n\n    Args:\n        temp: `float [n_p_levels]`\n            Temperature in *K* to find virtual potential temperature at.\n        sphum: `float [n_p_levels]`\n            Specific humidity of parcel at each pressure level in *kg/kg*.\n\n    Returns:\n        Virtual temperature at each pressure level in *K*.\n    \"\"\"\n    d622 = R / R_v\n    d378 = 1 - d622\n    d608 = d378 / d622\n    return (1 + d608 * sphum) * temp\n</code></pre>"},{"location":"code/utils/numerical/","title":"Numerical","text":""},{"location":"code/utils/numerical/#isca_tools.utils.numerical.get_extrema_date_from_spline","title":"<code>get_extrema_date_from_spline(spline, type='max', thresh=None, n_extrema=2)</code>","text":"<p>Given a spline, this returns the dates (x variable) corresponding to the maxima or minima.</p> <p>Parameters:</p> Name Type Description Default <code>spline</code> <code>CubicSpline</code> <p>spline to find extrema of</p> required <code>type</code> <code>str</code> <p>which extrema to find ('max' or 'min')</p> <code>'max'</code> <code>thresh</code> <code>Optional[float]</code> <p>Only keep maxima (minima) with values above (below) this.</p> <code>None</code> <code>n_extrema</code> <code>int</code> <p>Keep at most this many extrema, if more than this then will only keep highest (lowest).</p> <code>2</code> Source code in <code>isca_tools/utils/numerical.py</code> <pre><code>def get_extrema_date_from_spline(spline: CubicSpline, type: str = 'max', thresh: Optional[float] = None,\n                                 n_extrema: int = 2) -&gt; np.ndarray:\n    \"\"\"\n    Given a spline, this returns the dates (x variable) corresponding to the maxima or minima.\n\n    Args:\n        spline: spline to find extrema of\n        type: which extrema to find ('max' or 'min')\n        thresh: Only keep maxima (minima) with values above (below) this.\n        n_extrema: Keep at most this many extrema, if more than this then will only keep highest (lowest).\n    \"\"\"\n    extrema_date = spline.derivative().roots(extrapolate=False)\n    if type == 'max':\n        extrema_date = extrema_date[spline(extrema_date, 2) &lt; 0]  # maxima have a negative second derivative\n    elif type == 'min':\n        extrema_date = extrema_date[spline(extrema_date, 2) &gt; 0]  # minima have a positive second derivative\n    else:\n        raise ValueError('type is not valid, it should be max or min')\n    extrema_values = spline(extrema_date)\n    if thresh is not None:\n        # Only keep maxima with value above threshold\n        if type == 'max':\n            keep = extrema_values &gt; thresh\n        elif type == 'min':\n            keep = extrema_values &lt; thresh\n        extrema_date = extrema_date[keep]\n        extrema_values = extrema_values[keep]\n    if len(extrema_date) &gt; n_extrema:\n        if type == 'max':\n            keep_ind = np.argsort(extrema_values)[-n_extrema:]\n        elif type == 'min':\n            keep_ind = np.argsort(extrema_values)[:n_extrema]\n        extrema_date = extrema_date[keep_ind]\n    return extrema_date\n</code></pre>"},{"location":"code/utils/numerical/#isca_tools.utils.numerical.get_var_extrema_date","title":"<code>get_var_extrema_date(time, var, smooth_window=1, type='max', thresh_extrema=None, max_extrema=2, smooth_method='convolve')</code>","text":"<p>Finds the dates of extrema of a variable, given some smoothing is performed first. Also returns the splines themselves.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>ndarray</code> <p><code>float [n_time]</code> Time in days (assumes periodic e.g. annual mean, so <code>time = np.arange(360)</code>)</p> required <code>var</code> <code>ndarray</code> <p><code>float [n_time]</code> Value of variable at each time. Again, assume periodic</p> required <code>smooth_window</code> <code>int</code> <p>Number of time steps to use to smooth <code>var</code> before finding extrema. Smaller equals more accurate fit. <code>1</code> is perfect fit.</p> <code>1</code> <code>type</code> <code>str</code> <p>which extrema to find ('max' or 'min')</p> <code>'max'</code> <code>thresh_extrema</code> <code>Optional[float]</code> <p>Only keep maxima (minima) with values above (below) this.</p> <code>None</code> <code>max_extrema</code> <code>int</code> <p>Keep at most this many extrema, if more than this then will only keep highest (lowest).</p> <code>2</code> <code>smooth_method</code> <code>str</code> <p><code>convolve</code> or <code>spline</code> If <code>convolve</code>, will smooth via convolution with window of length <code>smooth_window</code>. If <code>spline</code>, will fit a spline using every <code>smooth_window</code> values of <code>time</code>.</p> <code>'convolve'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>extrema_date</code>: <code>float [max_extrema]</code> Dates of extrema of var</p> <code>CubicSpline</code> <p><code>spline_var</code>: Spline fit to var to find the extrema.</p> Source code in <code>isca_tools/utils/numerical.py</code> <pre><code>def get_var_extrema_date(time: np.ndarray, var: np.ndarray, smooth_window: int = 1,\n                         type: str = 'max', thresh_extrema: Optional[float] = None,\n                         max_extrema: int = 2, smooth_method: str = 'convolve') -&gt; Tuple[np.ndarray, CubicSpline]:\n    \"\"\"\n    Finds the dates of extrema of a variable, given some smoothing is performed first.\n    Also returns the splines themselves.\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Time in days (assumes periodic e.g. annual mean, so `time = np.arange(360)`)\n        var: `float [n_time]`&lt;/br&gt;\n            Value of variable at each time. Again, assume periodic\n        smooth_window: Number of time steps to use to smooth `var` before finding extrema.\n            Smaller equals more accurate fit. `1` is perfect fit.\n        type: which extrema to find ('max' or 'min')\n        thresh_extrema: Only keep maxima (minima) with values above (below) this.\n        max_extrema: Keep at most this many extrema, if more than this then will only keep highest (lowest).\n        smooth_method: `convolve` or `spline`&lt;/br&gt;\n            If `convolve`, will smooth via convolution with window of length `smooth_window`.\n            If `spline`, will fit a spline using every `smooth_window` values of `time`.\n\n    Returns:\n        `extrema_date`: `float [max_extrema]`&lt;/br&gt;\n            Dates of extrema of var\n        `spline_var`: Spline fit to var to find the extrema.\n    \"\"\"\n    if smooth_method.lower() == 'spline':\n        # Make so last element of arrays equal first as periodic\n        time_smooth = np.append(time, time[-1]+1)[::smooth_window]\n        var_smooth = np.append(var, var[0])[::smooth_window]\n    elif smooth_method.lower() == 'convolve':\n        var_smooth = scipy.ndimage.convolve(var, np.ones(smooth_window) / smooth_window, mode='wrap')\n        time_smooth = np.append(time, time[-1] + 1)\n        var_smooth = np.append(var_smooth, var_smooth[0])\n    else:\n        raise  ValueError('smooth_method must be either spline or convolve')\n    # Spline var is the spline replicating var_smooth exactly i.e. spline_var(t) = var_smooth[t] if t in time_smooth\n    spline_var = CubicSpline(time_smooth, var_smooth, bc_type='periodic')\n    extrema_date = get_extrema_date_from_spline(spline_var, type, thresh_extrema, max_extrema)\n    return extrema_date, spline_var\n</code></pre>"},{"location":"code/utils/numerical/#isca_tools.utils.numerical.get_var_shift","title":"<code>get_var_shift(x, shift_time=None, shift_phase=None, time=None, time_start=None, time_end=None)</code>","text":"<p>Returns the periodic variable \\(x(t-t_{shift})\\) where \\(t\\)=<code>time</code>, and \\(t_{shift}=\\)<code>shift_time</code>. If <code>shift_phase</code> is provided, will set <code>shift_time = shift_phase * period</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p><code>float [n_x]</code> \\(x\\) variable such that <code>x[i]</code> is the value of \\(x\\) at time <code>time[i]</code>.</p> required <code>shift_time</code> <code>Optional[float]</code> <p>How much to shift \\(x\\) by in units of <code>time</code>.</p> <code>None</code> <code>shift_phase</code> <code>Optional[float]</code> <p>Fraction of period to shift \\(x\\) by.</p> <code>None</code> <code>time</code> <code>Optional[ndarray]</code> <p><code>float [n_x]</code> Time such that <code>x[i]</code> is \\(x\\) at time <code>time[i]</code>. If <code>time</code> provided, will use spline to apply shift to \\(x\\). If <code>time</code> not provided, assume time is <code>np.arange(n_x)</code>, and will use <code>np.roll</code> to apply shift to \\(x\\).</p> <code>None</code> <code>time_start</code> <code>Optional[float]</code> <p>Start time such that period is given by <code>time_end - time_start + 1</code>. If not provided, will set to min value in <code>time</code>.</p> <code>None</code> <code>time_end</code> <code>Optional[float]</code> <p>End time such that period is given by <code>time_end - time_start + 1</code>. If not provided, will set to max value in <code>time</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_shift</code> <code>ndarray</code> <p><code>float [n_x]</code> \\(x\\) variable shifted in time such that <code>x_shift[i]</code> is value of \\(x\\) at time <code>time[i] - shift_time</code>.</p> Source code in <code>isca_tools/utils/numerical.py</code> <pre><code>def get_var_shift(x: np.ndarray, shift_time: Optional[float]=None, shift_phase: Optional[float]=None,\n                time: Optional[np.ndarray] = None, time_start: Optional[float] = None,\n                time_end: Optional[float] = None) -&gt; np.ndarray:\n    \"\"\"\n    Returns the periodic variable $x(t-t_{shift})$ where $t$=`time`, and $t_{shift}=$`shift_time`.\n    If `shift_phase` is provided, will set `shift_time = shift_phase * period`.\n\n    Args:\n        x: `float [n_x]`&lt;/br&gt;\n            $x$ variable such that `x[i]` is the value of $x$ at time `time[i]`.\n        shift_time: How much to shift $x$ by in units of `time`.\n        shift_phase: Fraction of period to shift $x$ by.\n        time: `float [n_x]`&lt;/br&gt;\n            Time such that `x[i]` is $x$ at time `time[i]`.&lt;/n&gt;\n            If `time` provided, will use spline to apply shift to $x$.&lt;/n&gt;\n            If `time` not provided, assume time is `np.arange(n_x)`, and will use `np.roll` to apply shift to $x$.\n        time_start: Start time such that period is given by `time_end - time_start + 1`.\n            If not provided, will set to min value in `time`.\n        time_end: End time such that period is given by `time_end - time_start + 1`.\n            If not provided, will set to max value in `time`.\n\n    Returns:\n        x_shift: `float [n_x]`&lt;/br&gt;\n            $x$ variable shifted in time such that `x_shift[i]` is value of $x$ at time `time[i] - shift_time`.\n    \"\"\"\n    if time is not None:\n        ind = np.argsort(time)\n        if time_start is None:\n            time_start = time[ind][0]\n        if time_end is None:\n            time_end = time[ind][-1]\n        if time[ind][0] &lt; time_start:\n            raise ValueError(f'Min time={time[ind][0]} is less than time_start={time_start}')\n        if time[ind][-1] &gt; time_end:\n            raise ValueError(f'Max time={time[ind][-1]} is greater than time_end={time_end}')\n        x_spline_fit = CubicSpline(np.append(time[ind], time_end+time[ind][0]-time_start+1), np.append(x[ind], x[ind][0]),\n                                   bc_type='periodic')\n        period = time_end - time_start + 1\n        if shift_phase is not None:\n            shift_time = shift_phase * period\n        x_shift = x_spline_fit(time - shift_time)\n    else:\n        if shift_phase is not None:\n            shift_time = shift_phase * x.size\n        if int(np.round(shift_time)) != shift_time:\n            raise ValueError(f'shift_time={shift_time} is not a whole number - '\n                             f'may be better using spline by providing time.')\n        x_shift = np.roll(x, int(np.round(shift_time)))\n    return x_shift\n</code></pre>"},{"location":"code/utils/numerical/#isca_tools.utils.numerical.interp_nan","title":"<code>interp_nan(x, y)</code>","text":"<p>Set all nan values in <code>y</code> based on the two nearest values of <code>x</code> for which <code>y</code> is not nan.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p><code>[n_points]</code> Independent variable e.g. pressure</p> required <code>y</code> <code>ndarray</code> <p><code>[n_points]</code> Dependent variable e.g. temperature</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p><code>[n_points]</code> Same as input but with all nans replaced through interpolation.</p> <code>not_valid_idx</code> <code>ndarray</code> <p><code>[n_not_valid]</code> Indices of nans in <code>y</code> that were replaced through interpolation.</p> Source code in <code>isca_tools/utils/numerical.py</code> <pre><code>def interp_nan(x: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Set all nan values in `y` based on the two nearest values of `x` for which `y` is not nan.\n\n    Args:\n        x: `[n_points]` Independent variable e.g. pressure\n        y: `[n_points]` Dependent variable e.g. temperature\n\n    Returns:\n        y: `[n_points]` Same as input but with all nans replaced through interpolation.\n        not_valid_idx: `[n_not_valid]` Indices of nans in `y` that were replaced through interpolation.\n    \"\"\"\n    # Set all nan values in x based on the two nearest indices that are not nan\n    # using linear interpolation\n    not_valid = np.isnan(y)\n    if not_valid.sum() == 0:\n        return y, np.zeros(0)\n    else:\n        valid_idx = np.where(~not_valid)[0]\n        not_valid_idx = np.where(not_valid)[0]\n        for i in not_valid_idx:\n            if i &lt; valid_idx[0]:\n                # Case: before first valid point\n                j1, j2 = valid_idx[0], valid_idx[1]\n            elif i &gt; valid_idx[-1]:\n                # Case: after last valid point\n                j1, j2 = valid_idx[-2], valid_idx[-1]\n            else:\n                # Case: between valid points\n                # nearest valid indices around i\n                j2 = valid_idx[valid_idx &gt; i][0]\n                j1 = valid_idx[valid_idx &lt; i][-1]\n            # Linear interpolation between (x[j1], y[j1]) and (x[j2], y[j2])\n            slope = (y[j2] - y[j1]) / (x[j2] - x[j1])\n            y[i] = y[j1] + slope * (x[i] - x[j1])\n    return y, not_valid_idx\n</code></pre>"},{"location":"code/utils/numerical/#isca_tools.utils.numerical.polyfit_phase","title":"<code>polyfit_phase(x, y, deg, time=None, time_start=None, time_end=None, deg_phase_calc=10, resample=False, include_phase=True, fourier_harmonics=None, integ_method='spline')</code>","text":"<p>This fits a polynomial <code>y_approx(x) = p[0] * x**deg + ... + p[deg]</code> of degree <code>deg</code> to points (x, y) as <code>np.polyfit</code> but also includes additional phase shift term such that the total approximation for y is:</p> <p>\\(y_{approx} = \\frac{1}{2} \\lambda_{phase}(x(t-T/4) - x(t+T/4)) + \\sum_{n=0}^{n_{deg}} \\lambda_n x^n\\)</p> <p>where \\(\\lambda_n=\\)<code>poly_coefs[-1-n]</code> and \\(\\lambda_{phase}=\\)<code>poly_coefs[0]</code>. \\(x\\) is assumed periodic with period \\(T=\\)<code>time[-1]-time[0]+time_spacing</code>.</p> <p>The phase component, \\(y_{phase}=\\frac{1}{2} \\lambda_{phase}(x(t-T/4) - x(t+T/4))\\), is found first from the residual of \\(y-y_{best}\\), where \\(y_{best}\\) is the polynomial approximation of degree <code>deg_phase_calc</code>.</p> <p>\\(\\sum_{n=0}^{n_{deg}} \\lambda_n x^n\\) is then found by doing the normal polynomial approximation of degree <code>deg</code> to the residual \\(y-y_{phase}\\).</p> <p>If <code>fourier_harmonics</code> is provided, then a fourier series will also be added to \\(y_{approx}\\) containing all harmonics, \\(n\\), in <code>fourier_harmonics</code>:</p> <p>\\(y_{approx} + \\sum_{n} F_n\\cos(2n\\pi ft - \\Phi_n)\\)</p> <p>The idea behind this is to account for part of \\(y\\) not directly related to \\(x\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p><code>float [n_x]</code> \\(x\\) coordinates used to approximate \\(y\\). <code>x[i]</code> is value at time <code>time[i]</code>.</p> required <code>y</code> <code>ndarray</code> <p><code>float [n_x]</code> \\(y\\) coordinate correesponding to each \\(x\\).</p> required <code>deg</code> <code>int</code> <p>Degree of the fitting polynomial. If negative, will only do the phase fitting.</p> required <code>time</code> <code>Optional[ndarray]</code> <p><code>float [n_x]</code> Time such that <code>x[i]</code> is \\(x\\) and <code>y[i]</code> is \\(y\\) at time <code>time[i]</code>. If time provided, will use spline to apply phase shift to \\(x\\). If time not provided, assume time is <code>np.arange(n_x)</code>, and will use <code>np.roll</code> to apply phase shift to \\(x\\).</p> <code>None</code> <code>time_start</code> <code>Optional[float]</code> <p>Start time such that period is given by <code>time_end - time_start + 1</code>. If not provided, will set to min value in <code>time</code>.</p> <code>None</code> <code>time_end</code> <code>Optional[float]</code> <p>End time such that period is given by <code>time_end - time_start + 1</code>. If not provided, will set to max value in <code>time</code>.</p> <code>None</code> <code>deg_phase_calc</code> <code>int</code> <p>Degree of the fitting polynomial to use in the phase term calculation. Should be a large integer.</p> <code>10</code> <code>resample</code> <code>bool</code> <p>If <code>True</code>, will use <code>resample_data</code> to resample x and y before each calling of <code>np.polyfit</code>.</p> <code>False</code> <code>include_phase</code> <code>bool</code> <p>If <code>False</code>, will only call <code>np.polyfit</code>, but first return value will be 0 indicating no phase shift. Only makes sense to call this rather than <code>np.polyfit</code> if you want to use <code>resample</code>.</p> <code>True</code> <code>fourier_harmonics</code> <code>Optional[Union[int, ndarray]]</code> <p><code>int [n_harmonics_include]</code> After applied polynomial of degree <code>deg_phase_calc</code> and fit the phase factor, a residual is obtained <code>y_residual = y - y_best - y_phase</code>. If <code>fourier_harmonics</code> is provided, a fourier series will be directly fit to this residual including all the harmonics in <code>fourier_harmonics</code>. If <code>fourier_harmonics</code> is an integer, all harmonics up to and including the value will be fit. The final polynomial of degree <code>deg</code> will then be fit to <code>y - y_phase - y_fourier</code>. Idea behind this is to account for part of \\(y\\) not directly related to \\(x\\).</p> <code>None</code> <code>integ_method</code> <code>str</code> <p>How to perform the integration when calculating Fourier coefficients.. If <code>spline</code>, will fit a spline and then integrate the spline, otherwise will use <code>scipy.integrate.simpson</code>.</p> <code>'spline'</code> <p>Returns:</p> Name Type Description <code>poly_coefs</code> <code>Union[ndarray, Tuple[ndarray, ndarray, ndarray]]</code> <p><code>float [n_deg+2]</code> Polynomial coefficients, phase first and then normal output of <code>np.polyfit</code> with lowest power last.</p> <code>coefs_fourier_amp</code> <code>Union[ndarray, Tuple[ndarray, ndarray, ndarray]]</code> <p><code>float [fourier_harmonics.max()+1]</code> <code>coefs_fourier_amp[n]</code> is the amplitude fourier coefficient for the \\(n^{th}\\) harmonic. First value will be zero, because \\(0^{th}\\) harmonic is just a constant and so will be found in polynomial fitting. Only returned if <code>fourier_harmonics</code> is provided.</p> <code>coefs_fourier_phase</code> <code>Union[ndarray, Tuple[ndarray, ndarray, ndarray]]</code> <p><code>float [fourier_harmonics.max()+1]</code> <code>coefs_fourier_phase[n]</code> is the phase fourier coefficient for the \\((n+1)^{th}\\) harmonic. Only returned if <code>fourier_harmonics</code> is provided.</p> Source code in <code>isca_tools/utils/numerical.py</code> <pre><code>def polyfit_phase(x: np.ndarray, y: np.ndarray,\n                  deg: int, time: Optional[np.ndarray] = None, time_start: Optional[float] = None,\n                  time_end: Optional[float] = None,\n                  deg_phase_calc: int = 10, resample: bool = False,\n                  include_phase: bool = True, fourier_harmonics: Optional[Union[int, np.ndarray]] = None,\n                  integ_method: str = 'spline') -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"\n    This fits a polynomial `y_approx(x) = p[0] * x**deg + ... + p[deg]` of degree `deg` to points (x, y) as `np.polyfit`\n    but also includes additional phase shift term such that the total approximation for y is:\n\n    $y_{approx} = \\\\frac{1}{2} \\lambda_{phase}(x(t-T/4) - x(t+T/4)) + \\sum_{n=0}^{n_{deg}} \\lambda_n x^n$\n\n    where $\\lambda_n=$`poly_coefs[-1-n]` and $\\lambda_{phase}=$`poly_coefs[0]`.\n    $x$ is assumed periodic with period $T=$`time[-1]-time[0]+time_spacing`.\n\n    The phase component, $y_{phase}=\\\\frac{1}{2} \\lambda_{phase}(x(t-T/4) - x(t+T/4))$, is found first from the\n    residual of $y-y_{best}$, where $y_{best}$ is the polynomial approximation of degree `deg_phase_calc`.\n\n    $\\sum_{n=0}^{n_{deg}} \\lambda_n x^n$ is then found by doing the normal polynomial approximation of degree\n    `deg` to the residual $y-y_{phase}$.\n\n    If `fourier_harmonics` is provided, then a fourier series will also be added to $y_{approx}$ containing all\n    harmonics, $n$, in `fourier_harmonics`:\n\n    $y_{approx} + \\\\sum_{n} F_n\\\\cos(2n\\\\pi ft - \\\\Phi_n)$\n\n    The idea behind this is to account for part of $y$ not directly related to $x$.\n\n    Args:\n        x: `float [n_x]`&lt;/br&gt;\n            $x$ coordinates used to approximate $y$. `x[i]` is value at time `time[i]`.\n        y: `float [n_x]`&lt;/br&gt;\n            $y$ coordinate correesponding to each $x$.\n        deg: Degree of the fitting polynomial. If negative, will only do the phase fitting.\n        time: `float [n_x]`&lt;/br&gt;\n            Time such that `x[i]` is $x$ and `y[i]` is $y$ at time `time[i]`.&lt;/n&gt;\n            If time provided, will use spline to apply phase shift to $x$.&lt;/n&gt;\n            If time not provided, assume time is `np.arange(n_x)`, and will use `np.roll` to apply phase shift to $x$.\n        time_start: Start time such that period is given by `time_end - time_start + 1`.\n            If not provided, will set to min value in `time`.\n        time_end: End time such that period is given by `time_end - time_start + 1`.\n            If not provided, will set to max value in `time`.\n        deg_phase_calc: Degree of the fitting polynomial to use in the phase term calculation.\n            Should be a large integer.\n        resample: If `True`, will use `resample_data` to resample x and y before each calling of\n            `np.polyfit`.\n        include_phase: If `False`, will only call `np.polyfit`, but first return value will be 0 indicating\n            no phase shift. Only makes sense to call this rather than `np.polyfit` if you want to use `resample`.\n        fourier_harmonics: `int [n_harmonics_include]`&lt;/br&gt;\n            After applied polynomial of degree `deg_phase_calc` and fit the phase factor, a residual is obtained\n            `y_residual = y - y_best - y_phase`. If `fourier_harmonics` is provided, a fourier\n            series will be directly fit to this residual including all the harmonics in `fourier_harmonics`.\n            If `fourier_harmonics` is an integer, all harmonics up to and including the value will be fit.&lt;/br&gt;\n            The final polynomial of degree `deg` will then be fit to `y - y_phase - y_fourier`.&lt;/br&gt;\n            Idea behind this is to account for part of $y$ not directly related to $x$.\n        integ_method: How to perform the integration when calculating Fourier coefficients..&lt;/br&gt;\n            If `spline`, will fit a spline and then integrate the spline, otherwise will use `scipy.integrate.simpson`.\n\n    Returns:\n        poly_coefs: `float [n_deg+2]`\n            Polynomial coefficients, phase first and then normal output of `np.polyfit` with lowest power last.\n        coefs_fourier_amp: `float [fourier_harmonics.max()+1]`&lt;/br&gt;\n            `coefs_fourier_amp[n]` is the amplitude fourier coefficient for the $n^{th}$ harmonic.&lt;/br&gt;\n            First value will be zero, because $0^{th}$ harmonic is just a constant and so will be found in\n            polynomial fitting.&lt;/br&gt;\n            Only returned if `fourier_harmonics` is provided.\n        coefs_fourier_phase: `float [fourier_harmonics.max()+1]`&lt;/br&gt;\n            `coefs_fourier_phase[n]` is the phase fourier coefficient for the $(n+1)^{th}$ harmonic.&lt;/br&gt;\n            Only returned if `fourier_harmonics` is provided.\n    \"\"\"\n    coefs = np.zeros(np.clip(deg, 0, 1000) + 2)  # first coef is phase coef\n    if resample:\n        x_fit, y_fit = resample_data(time, x, y)[1:]\n    else:\n        x_fit = x\n        y_fit = y\n    if not include_phase:\n        coefs[1:] = np.polyfit(x_fit, y_fit, deg)       # don't do phase stuff so 1st value is 0\n    else:\n        y_best_polyfit = np.polyval(np.polyfit(x_fit, y_fit, deg_phase_calc), x)\n        x_shift = 0.5 * (get_var_shift(x, shift_phase=0.25, time=time, time_start=time_start, time_end=time_end) -\n                         get_var_shift(x, shift_phase=-0.25, time=time, time_start=time_start, time_end=time_end))\n        if resample:\n            x_shift_fit, y_residual_fit = resample_data(time, x_shift, y - y_best_polyfit)[1:]\n        else:\n            x_shift_fit = x_shift\n            y_residual_fit = y - y_best_polyfit\n        coefs[[0, -1]] = np.polyfit(x_shift_fit, y_residual_fit, 1)\n        y_no_phase = y - polyval_phase(coefs, x, time, time_start, time_end)  # residual after removing phase dependent term\n\n        if fourier_harmonics is not None:\n            time_use = np.arange(x.size) if time is None else time\n            if not all(np.ediff1d(time_use) == np.ediff1d(time_use)[0]):\n                raise ValueError('Can only include Fourier with evenly spaced data')\n            if isinstance(fourier_harmonics, int):\n                # if int, use all harmonics up to value indicated but without 0th harmonic\n                fourier_harmonics = np.arange(1, fourier_harmonics+1)\n            coefs_fourier_amp = np.zeros(np.max(fourier_harmonics)+1)\n            coefs_fourier_phase = np.zeros(np.max(fourier_harmonics))\n            for n in fourier_harmonics:\n                if n==0:\n                    warnings.warn('Will not fit 0th harmonic as constant will be fit in polynomial')\n                else:\n                    coefs_fourier_amp[n], coefs_fourier_phase[n-1] = \\\n                        get_fourier_coef(time_use, y_no_phase - y_best_polyfit, n, integ_method)\n            y_residual_fourier = fourier_series(time_use, coefs_fourier_amp, coefs_fourier_phase)\n            y_no_phase = y_no_phase - y_residual_fourier\n\n        if deg &gt;= 0:\n            if resample:\n                x_fit, y_no_phase_fit = resample_data(time, x, y_no_phase)[1:]\n            else:\n                x_fit = x\n                y_no_phase_fit = y_no_phase\n            coefs[1:] += np.polyfit(x_fit, y_no_phase_fit, deg)\n\n    if fourier_harmonics is None:\n        return coefs\n    else:\n        return coefs, coefs_fourier_amp, coefs_fourier_phase\n</code></pre>"},{"location":"code/utils/numerical/#isca_tools.utils.numerical.polyval_phase","title":"<code>polyval_phase(poly_coefs, x, time=None, time_start=None, time_end=None, coefs_fourier_amp=None, coefs_fourier_phase=None)</code>","text":"<p>Given the polynomial coefficients found by <code>polyfit_phase</code> for fitting a polynomial of degree <code>len(polyfit)-2</code> of \\(x\\) to \\(y\\), this will return the approximation of \\(y\\):</p> <p>\\(y_{approx} = \\frac{1}{2} \\lambda_{phase}(x(t-T/4) - x(t+T/4)) + \\sum_{n=0}^{n_{deg}} \\lambda_n x^n\\)</p> <p>where \\(\\lambda_n=\\)<code>poly_coefs[-1-n]</code>, \\(\\lambda_{phase}=\\)<code>poly_coefs[0]</code> and \\(x\\) is assumed periodic with period \\(T\\).</p> <p>If <code>coefs_fourier_amp</code> is provided, then a fourier series will also be added to \\(y_{approx}\\):</p> <p>\\(y_{approx} + \\frac{F_0}{2} + \\sum_{n=1}^{N} F_n\\cos(2n\\pi ft - \\Phi_n)\\)</p> <p>Parameters:</p> Name Type Description Default <code>poly_coefs</code> <code>ndarray</code> <p><code>float [n_deg+2]</code> Polynomial coefficients as output by <code>polyfit_phase</code>, lowest power last. \\(\\lambda_{phase}=\\)<code>poly_coefs[0]</code> and \\(\\lambda_n=\\)<code>poly_coefs[-1-n]</code>.</p> required <code>x</code> <code>ndarray</code> <p><code>float [n_x]</code> \\(x\\) coordinates used to approximate \\(y\\).</p> required <code>time</code> <code>Optional[ndarray]</code> <p><code>float [n_x]</code> Time such that <code>x[i]</code> is \\(x\\) at time <code>time[i]</code>. If <code>time</code> provided, will use spline to apply shift to \\(x\\). If <code>time</code> not provided, assume time is <code>np.arange(n_x)</code>, and will use <code>np.roll</code> to apply shift to \\(x\\).</p> <code>None</code> <code>time_start</code> <code>Optional[float]</code> <p>Start time such that period is given by <code>time_end - time_start + 1</code>. If not provided, will set to min value in <code>time</code>.</p> <code>None</code> <code>time_end</code> <code>Optional[float]</code> <p>End time such that period is given by <code>time_end - time_start + 1</code>. If not provided, will set to max value in <code>time</code>.</p> <code>None</code> <code>coefs_fourier_amp</code> <code>Optional[ndarray]</code> <p><code>float [n_harmonics+1]</code> The amplitude Fourier coefficients \\(F_n\\).</p> <code>None</code> <code>coefs_fourier_phase</code> <code>Optional[ndarray]</code> <p><code>float [n_harmonics]</code> The phase Fourier coefficients \\(\\Phi_n\\).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y_approx</code> <code>ndarray</code> <p><code>float [n_x]</code> Polynomial approximation to \\(y\\), possibly including phase and Fourier terms.</p> Source code in <code>isca_tools/utils/numerical.py</code> <pre><code>def polyval_phase(poly_coefs: np.ndarray, x: np.ndarray, time: Optional[np.ndarray] = None,\n                  time_start: Optional[float] = None, time_end: Optional[float] = None,\n                  coefs_fourier_amp: Optional[np.ndarray] = None,\n                  coefs_fourier_phase: Optional[np.ndarray] = None) -&gt; np.ndarray:\n    \"\"\"\n    Given the polynomial coefficients found by `polyfit_phase` for fitting a polynomial\n    of degree `len(polyfit)-2` of $x$ to $y$, this will return the approximation of $y$:\n\n    $y_{approx} = \\\\frac{1}{2} \\lambda_{phase}(x(t-T/4) - x(t+T/4)) + \\sum_{n=0}^{n_{deg}} \\lambda_n x^n$\n\n    where $\\lambda_n=$`poly_coefs[-1-n]`, $\\lambda_{phase}=$`poly_coefs[0]` and\n    $x$ is assumed periodic with period $T$.\n\n    If `coefs_fourier_amp` is provided, then a fourier series will also be added to $y_{approx}$:\n\n    $y_{approx} + \\\\frac{F_0}{2} + \\\\sum_{n=1}^{N} F_n\\\\cos(2n\\\\pi ft - \\\\Phi_n)$\n\n    Args:\n        poly_coefs: `float [n_deg+2]`&lt;/br&gt;\n            Polynomial coefficients as output by `polyfit_phase`, lowest power last.&lt;/br&gt;\n            $\\lambda_{phase}=$`poly_coefs[0]` and $\\lambda_n=$`poly_coefs[-1-n]`.\n        x: `float [n_x]`&lt;/br&gt;\n            $x$ coordinates used to approximate $y$.\n        time: `float [n_x]`&lt;/br&gt;\n            Time such that `x[i]` is $x$ at time `time[i]`.&lt;/n&gt;\n            If `time` provided, will use spline to apply shift to $x$.&lt;/n&gt;\n            If `time` not provided, assume time is `np.arange(n_x)`, and will use `np.roll` to apply shift to $x$.\n        time_start: Start time such that period is given by `time_end - time_start + 1`.\n            If not provided, will set to min value in `time`.\n        time_end: End time such that period is given by `time_end - time_start + 1`.\n            If not provided, will set to max value in `time`.\n        coefs_fourier_amp: `float [n_harmonics+1]`&lt;/br&gt;\n            The amplitude Fourier coefficients $F_n$.\n        coefs_fourier_phase: `float [n_harmonics]`&lt;/br&gt;\n            The phase Fourier coefficients $\\\\Phi_n$.\n\n    Returns:\n        y_approx: `float [n_x]`&lt;/br&gt;\n            Polynomial approximation to $y$, possibly including phase and Fourier terms.\n    \"\"\"\n    # In this case, poly_coefs are output of polyfit_with_phase so first coefficient is the phase coefficient\n    y_approx = np.polyval(poly_coefs[1:], x)\n    x_shift = 0.5 * (get_var_shift(x, shift_phase=0.25, time=time, time_start=time_start, time_end=time_end) -\n                     get_var_shift(x, shift_phase=-0.25, time=time, time_start=time_start, time_end=time_end))\n    if coefs_fourier_amp is not None:\n        time_use = np.arange(x.size) if time is None else time\n        y_residual_fourier = fourier_series(time_use, coefs_fourier_amp, coefs_fourier_phase)\n    else:\n        y_residual_fourier = 0\n    return y_approx + poly_coefs[0] * x_shift + y_residual_fourier\n</code></pre>"},{"location":"code/utils/numerical/#isca_tools.utils.numerical.resample_data","title":"<code>resample_data(time, x, y, x_return=None, n_return=None, bc_type='periodic', extrapolate=False)</code>","text":"<p>Given that <code>x[i]</code> and <code>y[i]</code> both occur at time <code>time[i]</code>, this resamples data to return values of <code>y</code> corresponding to <code>x_return</code>.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>Optional[ndarray]</code> <p><code>float [n_time]</code> Times such that <code>x[i]</code> and <code>y[i]</code> correspond to time <code>time[i]</code>. It assumes time has a spacing of 1, and starts with 0, so for a 360-day year, it would be <code>np.arange(360)</code>.</p> required <code>x</code> <code>ndarray</code> <p><code>float [n_time]</code> Value of variable \\(x\\) at each time.</p> required <code>y</code> <code>ndarray</code> <p><code>float [n_time]</code> Value of variable \\(y\\) at each time.</p> required <code>x_return</code> <code>Optional[ndarray]</code> <p><code>float [n_return]</code> Values of \\(x\\) for the resampled \\(y\\) data to be returned. If not provided, will use <code>np.linspace(x.min(), x.max(), n_return)</code>.</p> <code>None</code> <code>n_return</code> <code>Optional[int]</code> <p>Number of resampled data if <code>x_return</code> is not provided. Will set to <code>n_time</code> neither this nor <code>x_return</code> provided.</p> <code>None</code> <code>bc_type</code> <code>str</code> <p>Boundary condition type in <code>scipy.interpolate.CubicSpline</code>.</p> <code>'periodic'</code> <code>extrapolate</code> <code>bool</code> <p>Whether to extrapolate if any <code>x_return</code> outside<code>x</code> is provided.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>times_return</code> <code>ndarray</code> <p><code>float [n_return_out]</code> Times corresponding to <code>x_return</code>. Not necessarily <code>n_return</code> values because can have multiple \\(y\\) values for each \\(x\\).</p> <code>x_return_out</code> <code>ndarray</code> <p><code>float [n_return_out]</code> \\(x\\) values corresponding to <code>times_return</code>. Will only contain values in <code>x_return</code>, but may contain multiple of each.</p> <code>y_return</code> <code>ndarray</code> <p><code>float [n_return_out]</code> \\(y\\) values corresponding to <code>times_return</code> and <code>x_return_out</code>.</p> Source code in <code>isca_tools/utils/numerical.py</code> <pre><code>def resample_data(time: Optional[np.ndarray], x: np.ndarray, y: np.ndarray, x_return: Optional[np.ndarray] = None,\n                  n_return: Optional[int] = None, bc_type: str = 'periodic',\n                  extrapolate: bool=False) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Given that `x[i]` and `y[i]` both occur at time `time[i]`, this resamples data to return values of `y`\n    corresponding to `x_return`.\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Times such that `x[i]` and `y[i]` correspond to time `time[i]`. It assumes time has a spacing of 1, and\n            starts with 0, so for a 360-day year, it would be `np.arange(360)`.\n        x: `float [n_time]`&lt;/br&gt;\n            Value of variable $x$ at each time.\n        y: `float [n_time]`&lt;/br&gt;\n            Value of variable $y$ at each time.\n        x_return: `float [n_return]`&lt;/br&gt;\n            Values of $x$ for the resampled $y$ data to be returned. If not provided, will use\n            `np.linspace(x.min(), x.max(), n_return)`.\n        n_return: Number of resampled data if `x_return` is not provided. Will set to `n_time` neither this\n            nor `x_return` provided.\n        bc_type: Boundary condition type in `scipy.interpolate.CubicSpline`.\n        extrapolate: Whether to extrapolate if any `x_return` outside`x` is provided.\n\n    Returns:\n        times_return: `float [n_return_out]`&lt;/br&gt;\n            Times corresponding to `x_return`. Not necessarily `n_return` values because can have multiple $y$\n            values for each $x$.\n        x_return_out: `float [n_return_out]`&lt;/br&gt;\n            $x$ values corresponding to `times_return`. Will only contain values in `x_return`, but may contain\n            multiple of each.\n        y_return: `float [n_return_out]`&lt;/br&gt;\n            $y$ values corresponding to `times_return` and `x_return_out`.\n    \"\"\"\n    if n_return is None:\n        n_return = x.size\n    if time is None:\n        time = np.arange(x.size)\n    time_spacing = np.median(np.ediff1d(time))\n    if 'periodic' in bc_type:\n        x_spline = CubicSpline(np.append(time, [time[-1]+time_spacing]), np.append(x, x[0]),\n                               bc_type=bc_type)\n        y_spline = CubicSpline(np.append(time, [time[-1]+time_spacing]), np.append(y, y[0]),\n                               bc_type=bc_type)\n    else:\n        x_spline = CubicSpline(time, x, bc_type=bc_type)\n        y_spline = CubicSpline(time, y, bc_type=bc_type)\n    if x_return is None:\n        x_return = np.linspace(x.min(), x.max(), n_return)\n    times_return = []\n    for i in range(x_return.size):\n        times_return += [*x_spline.solve(x_return[i], extrapolate=extrapolate)]\n    times_return = np.asarray(times_return) % time[-1]      # make return times between 0 and time[-1]\n    return times_return, x_spline(times_return), y_spline(times_return)\n</code></pre>"},{"location":"code/utils/numerical/#isca_tools.utils.numerical.resample_data_distance","title":"<code>resample_data_distance(time, x, y, n_return=None, bc_type='periodic', norm=False)</code>","text":"<p>Given that <code>x[i]</code> and <code>y[i]</code> both occur at time <code>time[i]</code>, this resamples data to return <code>n_return</code> values of \\(x\\) and \\(y\\) evenly spaced along the line connecting all (x, y) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>Optional[ndarray]</code> <p><code>float [n_time]</code> Times such that <code>x[i]</code> and <code>y[i]</code> correspond to time <code>time[i]</code>. If time not provided, assume time is <code>np.arange(n_x)</code>.</p> required <code>x</code> <code>ndarray</code> <p><code>float [n_time]</code> Value of variable \\(x\\) at each time.</p> required <code>y</code> <code>ndarray</code> <p><code>float [n_time]</code> Value of variable \\(y\\) at each time.</p> required <code>n_return</code> <code>Optional[int]</code> <p>Number of resampled data, will set to <code>n_time</code> if not provided.</p> <code>None</code> <code>bc_type</code> <code>str</code> <p>Boundary condition type in <code>scipy.interpolate.CubicSpline</code> for <code>x</code> and <code>y</code>.</p> <code>'periodic'</code> <code>norm</code> <code>bool</code> <p>If <code>True</code> will normalize <code>x</code> and <code>y</code> so both have a range of 1, before calculating distance along line.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>times_return</code> <code>ndarray</code> <p><code>float [n_return]</code> Times of returned \\(x\\) and \\(y\\) such that they are evenly spaced along line connecting all input (x, y) coordinates.</p> <code>x_return_out</code> <code>ndarray</code> <p><code>float [n_return_out]</code> \\(x\\) values corresponding to <code>times_return</code>.</p> <code>y_return</code> <code>ndarray</code> <p><code>float [n_return_out]</code> \\(y\\) values corresponding to <code>times_return</code> and <code>x_return</code>.</p> Source code in <code>isca_tools/utils/numerical.py</code> <pre><code>def resample_data_distance(time: Optional[np.ndarray], x: np.ndarray, y: np.ndarray, n_return: Optional[int] = None,\n                           bc_type: str = 'periodic', norm: bool = False) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Given that `x[i]` and `y[i]` both occur at time `time[i]`, this resamples data to return `n_return` values of\n    $x$ and $y$ evenly spaced along the line connecting all (x, y) coordinates.\n\n    Args:\n        time: `float [n_time]`&lt;/br&gt;\n            Times such that `x[i]` and `y[i]` correspond to time `time[i]`.&lt;/br&gt;\n            If time not provided, assume time is `np.arange(n_x)`.\n        x: `float [n_time]`&lt;/br&gt;\n            Value of variable $x$ at each time.\n        y: `float [n_time]`&lt;/br&gt;\n            Value of variable $y$ at each time.\n        n_return:  Number of resampled data, will set to `n_time` if not provided.\n        bc_type: Boundary condition type in `scipy.interpolate.CubicSpline` for `x` and `y`.\n        norm: If `True` will normalize `x` and `y` so both have a range of 1, before calculating distance along line.\n\n    Returns:\n        times_return: `float [n_return]`&lt;/br&gt;\n            Times of returned $x$ and $y$ such that they are evenly spaced along line connecting all input\n            (x, y) coordinates.\n        x_return_out: `float [n_return_out]`&lt;/br&gt;\n            $x$ values corresponding to `times_return`.\n        y_return: `float [n_return_out]`&lt;/br&gt;\n            $y$ values corresponding to `times_return` and `x_return`.\n    \"\"\"\n    if n_return is None:\n        n_return = x.size\n    if time is None:\n        time = np.arange(x.size)\n    time_spacing = np.median(np.ediff1d(time))\n    # dist[i] is distance along line from (x[0], y[0]) at time=time[i]\n    if norm:\n        coords_dist_calc = np.vstack((x / (np.max(x) - np.min(x)), y / (np.max(y) - np.min(y))))\n    else:\n        coords_dist_calc = np.vstack((x, y))\n    dist = np.append(0, np.cumsum(np.sqrt(np.sum(np.diff(coords_dist_calc, axis=1) ** 2, axis=0))))\n    x_spline = CubicSpline(np.append(time, [time[-1] + time_spacing]), np.append(x, x[0]),\n                                             bc_type=bc_type)\n    y_spline = CubicSpline(np.append(time, [time[-1] + time_spacing]), np.append(y, y[0]),\n                                             bc_type=bc_type)\n    dist_spline = CubicSpline(time, dist)\n    dist_return = np.linspace(dist[0], dist[-1], n_return)\n    # Adjust first and last values by tiny amount, to ensure that within the range when trying to solve\n    small = 0.0001 * (dist_return[1]-dist_return[0])\n    dist_return[0] += small\n    dist_return[-1] -= small\n    time_resample = np.zeros(n_return)\n    for i in range(n_return):\n        time_resample[i] = dist_spline.solve(dist_return[i], extrapolate=False)[0]\n    return time_resample, x_spline(time_resample), y_spline(time_resample)\n</code></pre>"},{"location":"code/utils/numerical/#isca_tools.utils.numerical.spline_integral","title":"<code>spline_integral(x, dy_dx, y0=0, x0=None, x_return=None, periodic=False)</code>","text":"<p>Uses spline integration to solve for \\(y\\) given \\(\\frac{dy}{dx}\\) such that \\(y=y_0\\) at \\(x=x_0\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p><code>float [n_x]</code> Values of \\(x\\) where \\(\\frac{dy}{dx}\\) given, and used to fit the spline.</p> required <code>dy_dx</code> <code>ndarray</code> <p><code>float [n_x]</code> Values of \\(\\frac{dy}{dx}\\) corresponding to \\(x\\), and used to fit the spline.</p> required <code>y0</code> <code>float</code> <p>Boundary condition, \\(y(x_0)=y_0\\).</p> <code>0</code> <code>x0</code> <code>Optional[float]</code> <p>Boundary condition, \\(y(x_0)=y_0\\). If not given, will assume <code>x0=x_return[0]</code>.</p> <code>None</code> <code>x_return</code> <code>Optional[ndarray]</code> <p><code>float [n_x_return]</code> Values of \\(y\\) are returned for these \\(x\\) values. If not given, will set to <code>x</code>.</p> <code>None</code> <code>periodic</code> <code>bool</code> <p>Whether to use periodic boundary condition. If periodic expect \\(\\frac{dy}{dx}\\) at \\(x=\\)<code>x[-1]+x_spacing</code> is equal to <code>dy_dx[0]</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>y_return</code> <code>ndarray</code> <p><code>float [n_x_return]</code> Values of \\(y\\) corresponding to `x_return.</p> Source code in <code>isca_tools/utils/numerical.py</code> <pre><code>def spline_integral(x: np.ndarray, dy_dx: np.ndarray, y0: float = 0, x0: Optional[float] = None,\n                    x_return: Optional[np.ndarray] = None,\n                    periodic: bool = False) -&gt; np.ndarray:\n    \"\"\"\n    Uses spline integration to solve for $y$ given $\\\\frac{dy}{dx}$ such that $y=y_0$ at $x=x_0$.\n\n    Args:\n        x: `float [n_x]`&lt;/br&gt;\n            Values of $x$ where $\\\\frac{dy}{dx}$ given, and used to fit the spline.\n        dy_dx: `float [n_x]`&lt;/br&gt;\n            Values of $\\\\frac{dy}{dx}$ corresponding to $x$, and used to fit the spline.\n        y0: Boundary condition, $y(x_0)=y_0$.\n        x0: Boundary condition, $y(x_0)=y_0$.&lt;/br&gt;\n            If not given, will assume `x0=x_return[0]`.\n        x_return: `float [n_x_return]`&lt;/br&gt;\n            Values of $y$ are returned for these $x$ values. If not given, will set to `x`.\n        periodic: Whether to use periodic boundary condition.&lt;/br&gt;\n            If periodic expect $\\\\frac{dy}{dx}$ at $x=$`x[-1]+x_spacing` is equal to `dy_dx[0]`.\n\n    Returns:\n        y_return: `float [n_x_return]`&lt;/br&gt;\n            Values of $y$ corresponding to `x_return.\n    \"\"\"\n    if periodic:\n        x_spacing = np.median(np.ediff1d(x))\n        spline_use = CubicSpline(np.append(x, x[-1] + x_spacing), np.append(dy_dx, dy_dx[0]), bc_type='periodic')\n    else:\n        spline_use = CubicSpline(x, dy_dx)\n    if x_return is None:\n        x_return = x\n    if x0 is None:\n        x0 = x_return[0]\n    y = np.full_like(x_return, y0, dtype=float)\n    for i in range(x_return.size):\n        y[i] += spline_use.integrate(x0, x_return[i], extrapolate='periodic' if periodic else None)\n    return y\n</code></pre>"},{"location":"code/utils/radiation/","title":"Radiation","text":""},{"location":"code/utils/radiation/#isca_tools.utils.radiation.frierson_atmospheric_heating","title":"<code>frierson_atmospheric_heating(ds, albedo=0)</code>","text":"<p>Returns the atmospheric radiative heating rate from the surface and top of atmosphere energy fluxes. A negative value indicates that the atmosphere is cooling.</p> <p>This takes into account any radiation that is absorbed by the atmosphere on its way down from space to the surface, as specified through <code>tau_equator</code> and the amount reflected at the surface through the <code>albedo</code>.</p> <p>In Isca, there is no absorption of the shortwave radiation as it moves back up through the atmosphere to space after being reflected at the surface.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset for particular experiment, must contain:</p> <ul> <li><code>swdn_toa</code> - Incident shortwave radiation at the top of atmosphere.     This is saved by Isca if the variable <code>swdn_toa</code> in the <code>two_stream</code> module is specified in the     diagnostic table.</li> <li><code>swdn_sfc</code> - Net shortwave radiation absorbed at the surface i.e. incident - reflected.     This is the negative of net upward shortwave radiation at the surface.     This is saved by Isca if the variable <code>swdn_sfc</code> in the <code>two_stream</code> module is specified in the     diagnostic table.</li> <li><code>lwup_sfc</code> - Upward longwave flux at the surface.     This is saved by Isca if the variable <code>lwdn_sfc</code> in the <code>two_stream</code> module is specified in the     diagnostic table.</li> <li><code>lwdn_sfc</code> - Downward longwave flux at the surface.     This is saved by Isca if the variable <code>lwdn_sfc</code> in the <code>two_stream</code> module is specified in the     diagnostic table.</li> <li><code>olr</code> - Outgoing longwave radiation at the top of atmosphere.     This is saved by Isca if the variable <code>lwdn_sfc</code> in the <code>two_stream</code> module is specified in the     diagnostic table.</li> </ul> required <code>albedo</code> <code>float</code> <p>Fraction of incident shortwave radiation reflected by the surface. It is specified through the option <code>albedo_value</code> in the <code>mixed_layer_nml</code> namelist.</p> <code>0</code> <p>Returns:     Atmospheric radiative heating rate in \\(W/m^2\\).</p> Source code in <code>isca_tools/utils/radiation.py</code> <pre><code>def frierson_atmospheric_heating(ds: Dataset, albedo: float = 0) -&gt; xr.DataArray:\n    \"\"\"\n    Returns the atmospheric radiative heating rate from the surface and top of atmosphere energy fluxes. A negative\n    value indicates that the atmosphere is cooling.\n\n    This takes into account any radiation that is absorbed by the atmosphere on its way down from space to the surface,\n    as specified through `tau_equator` and the amount reflected at the surface through the `albedo`.\n\n    In *Isca*, there is no absorption of the shortwave radiation as it moves back up through the atmosphere to space\n    after being reflected at the surface.\n\n    Args:\n        ds: Dataset for particular experiment, must contain:\n\n            * `swdn_toa` - Incident shortwave radiation at the top of atmosphere.\n                This is saved by *Isca* if the variable `swdn_toa` in the `two_stream` module is specified in the\n                diagnostic table.\n            * `swdn_sfc` - Net shortwave radiation absorbed at the surface i.e. incident - reflected.\n                This is the negative of net upward shortwave radiation at the surface.\n                This is saved by *Isca* if the variable `swdn_sfc` in the `two_stream` module is specified in the\n                diagnostic table.\n            * `lwup_sfc` - Upward longwave flux at the surface.\n                This is saved by *Isca* if the variable `lwdn_sfc` in the `two_stream` module is specified in the\n                diagnostic table.\n            * `lwdn_sfc` - Downward longwave flux at the surface.\n                This is saved by *Isca* if the variable `lwdn_sfc` in the `two_stream` module is specified in the\n                diagnostic table.\n            * `olr` - Outgoing longwave radiation at the top of atmosphere.\n                This is saved by *Isca* if the variable `lwdn_sfc` in the `two_stream` module is specified in the\n                diagnostic table.\n        albedo: Fraction of incident shortwave radiation reflected by the surface.\n            It is specified through the option `albedo_value` in the `mixed_layer_nml` namelist.\n    Returns:\n        Atmospheric radiative heating rate in $W/m^2$.\n\n    \"\"\"\n    # #This is the full method of doing it, but we can do it simpler without the sw absorption stuff\n    # swup_net_sfc = -ds.swdn_sfc\n    # # need to account for SW absorbed by atmosphere and reflected at surface\n    # swup_net_toa = -frierson_net_toa_sw_dwn(ds.swdn_toa, ds.ps, albedo, tau_equator, tau_lat_var, pressure_exponent,\n    #                                         ref_pressure)\n    # flux_surf = swup_net_sfc + ds.lwup_sfc - ds.lwdn_sfc\n    # flux_toa = swup_net_toa + ds.olr\n    # return ds.swdn_toa - ds.swdn_sfc / (1 - albedo) + ds.lwup_sfc - ds.lwdn_sfc - ds.olr\n\n    swup_toa = albedo/(1-albedo) * ds.swdn_sfc\n    flux_toa = swup_toa - ds.swdn_toa + ds.olr\n    flux_surf = -ds.swdn_sfc + ds.lwup_sfc - ds.lwdn_sfc\n    return flux_surf - flux_toa\n</code></pre>"},{"location":"code/utils/radiation/#isca_tools.utils.radiation.frierson_net_toa_sw_dwn","title":"<code>frierson_net_toa_sw_dwn(insolation, surface_pressure, albedo=0, tau_equator=0, tau_lat_var=0, pressure_exponent=4, ref_pressure=101325)</code>","text":"<p>Function to calculate the net downward shortwave radiation at the top of atmosphere for the Frierson and Byrne radiation schemes.</p> <p>This takes into account any radiation that is absorbed by the atmosphere on its way down from space to the surface, as specified through <code>tau_equator</code> and the amount reflected at the surface through the <code>albedo</code>.</p> <p>In Isca, there is no absorption of the shortwave radiation as it moves back up through the atmosphere to space after being reflected at the surface.</p> <p>All default values are the default values in Isca if the option is not specified in the relavent <code>namelist</code>.</p> <p>Parameters:</p> Name Type Description Default <code>insolation</code> <code>DataArray</code> <p>Incident shortwave radiation at the top of atmosphere with dimensions of latitude, longitude and time. This is saved by Isca if the variable <code>swdn_toa</code> in the <code>two_stream</code> module is specified in the diagnostic table.</p> required <code>surface_pressure</code> <code>DataArray</code> <p>Surface pressure in Pa with dimensions of latitude, longitude and time, \\(p_s\\). This is saved by Isca if the variable <code>ps</code> in the <code>dynamics</code> module is specified in the diagnostic table.</p> required <code>albedo</code> <code>float</code> <p>Fraction of incident shortwave radiation reflected by the surface. It is specified through the option <code>albedo_value</code> in the <code>mixed_layer_nml</code> namelist.</p> <code>0</code> <code>tau_equator</code> <code>float</code> <p>Surface optical depth at the equator, \\(\\tau_{0e}^*\\). It is specified through the option <code>atm_abs</code> in the <code>two_stream_gray_rad_nml</code> namelist.</p> <code>0</code> <code>tau_lat_var</code> <code>float</code> <p>Variation of optical depth with latitude, \\(\\Delta \\tau^*\\). It is specified through the option <code>sw_diff</code> in the <code>two_stream_gray_rad_nml</code> namelist.</p> <code>0</code> <code>pressure_exponent</code> <code>float</code> <p>Determines the variation of optical depth with pressure, \\(\\kappa^*\\). It is specified through the option <code>solar_exponent</code> in the <code>two_stream_gray_rad_nml</code> namelist.</p> <code>4</code> <code>ref_pressure</code> <code>float</code> <p>Reference pressure used by Isca in Pa. It is specified through the option <code>pstd_mks</code> in the <code>constants_nml</code> namelist.</p> <code>101325</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>Net downward shortwave radiation at the top of atmosphere with dimensions of latitude, longitude and time.</p> Source code in <code>isca_tools/utils/radiation.py</code> <pre><code>def frierson_net_toa_sw_dwn(insolation: xr.DataArray, surface_pressure: xr.DataArray, albedo: float = 0,\n                            tau_equator: float = 0, tau_lat_var: float = 0, pressure_exponent: float = 4,\n                            ref_pressure: float = 101325) -&gt; xr.DataArray:\n    \"\"\"\n    Function to calculate the net downward shortwave radiation at the top of atmosphere for the *Frierson*\n    and *Byrne* radiation schemes.\n\n    This takes into account any radiation that is absorbed by the atmosphere on its way down from space to the surface,\n    as specified through `tau_equator` and the amount reflected at the surface through the `albedo`.\n\n    In *Isca*, there is no absorption of the shortwave radiation as it moves back up through the atmosphere to space\n    after being reflected at the surface.\n\n    All default values are the default values in *Isca* if the option is not specified in the relavent `namelist`.\n\n    Args:\n        insolation: Incident shortwave radiation at the top of atmosphere\n            with dimensions of latitude, longitude and time.\n            This is saved by *Isca* if the variable `swdn_toa` in the `two_stream` module is specified in the\n            diagnostic table.\n        surface_pressure: Surface pressure in *Pa* with dimensions of latitude, longitude and time, $p_s$.\n            This is saved by *Isca* if the variable `ps` in the `dynamics` module is specified in the diagnostic table.\n        albedo: Fraction of incident shortwave radiation reflected by the surface.\n            It is specified through the option `albedo_value` in the `mixed_layer_nml` namelist.\n        tau_equator: Surface optical depth at the equator, $\\\\tau_{0e}^*$.\n            It is specified through the option `atm_abs` in the `two_stream_gray_rad_nml` namelist.\n        tau_lat_var: Variation of optical depth with latitude, $\\Delta \\\\tau^*$.\n            It is specified through the option `sw_diff` in the `two_stream_gray_rad_nml` namelist.\n        pressure_exponent: Determines the variation of optical depth with pressure, $\\\\kappa^*$.\n            It is specified through the option `solar_exponent` in the `two_stream_gray_rad_nml` namelist.\n        ref_pressure: Reference pressure used by Isca in *Pa*.\n            It is specified through the option `pstd_mks` in the `constants_nml` namelist.\n\n    Returns:\n        Net downward shortwave radiation at the top of atmosphere with dimensions of latitude, longitude and time.\n    \"\"\"\n    tau = frierson_sw_optical_depth(surface_pressure, tau_equator, tau_lat_var, pressure_exponent, ref_pressure)\n    return insolation*(1-albedo*np.exp(-tau))\n</code></pre>"},{"location":"code/utils/radiation/#isca_tools.utils.radiation.frierson_sw_optical_depth","title":"<code>frierson_sw_optical_depth(surface_pressure, tau_equator=0, tau_lat_var=0, pressure_exponent=4, ref_pressure=101325)</code>","text":"<p>Function to calculate shortwave surface optical depth, \\(\\tau_s\\), as a function of latitude, \\(\\phi\\), as performed in Isca for the Frierson and Byrne radiation schemes:</p> \\[ \\tau_s(\\phi) = (1-\\Delta \\tau^* \\sin^2 \\phi)\\tau_{0e}^*\\big(\\frac{p_s}{p_0}\\big)^{\\kappa^*} \\] <p>All default values are the default values in Isca if the option is not specified in the relavent <code>namelist</code>.</p> <p>Parameters:</p> Name Type Description Default <code>surface_pressure</code> <code>DataArray</code> <p>Surface pressure in Pa with dimensions of latitude, longitude and time, \\(p_s\\). This is saved by Isca if the variable <code>ps</code> in the <code>dynamics</code> module is specified in the diagnostic table.</p> required <code>tau_equator</code> <code>float</code> <p>Surface optical depth at the equator, \\(\\tau_{0e}^*\\). It is specified through the option <code>atm_abs</code> in the <code>two_stream_gray_rad_nml</code> namelist.</p> <code>0</code> <code>tau_lat_var</code> <code>float</code> <p>Variation of optical depth with latitude, \\(\\Delta \\tau^*\\). It is specified through the option <code>sw_diff</code> in the <code>two_stream_gray_rad_nml</code> namelist.</p> <code>0</code> <code>pressure_exponent</code> <code>float</code> <p>Determines the variation of optical depth with pressure, \\(\\kappa^*\\). It is specified through the option <code>solar_exponent</code> in the <code>two_stream_gray_rad_nml</code> namelist.</p> <code>4</code> <code>ref_pressure</code> <code>float</code> <p>Reference pressure used by Isca in Pa. It is specified through the option <code>pstd_mks</code> in the <code>constants_nml</code> namelist.</p> <code>101325</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>Shortwave surface optical depth with dimensions of latitude, longitude and time.</p> Source code in <code>isca_tools/utils/radiation.py</code> <pre><code>def frierson_sw_optical_depth(surface_pressure: xr.DataArray, tau_equator: float = 0, tau_lat_var: float = 0,\n                              pressure_exponent: float = 4, ref_pressure: float = 101325) -&gt; xr.DataArray:\n    \"\"\"\n    Function to calculate shortwave surface optical depth, $\\\\tau_s$, as a function of latitude, $\\\\phi$, as performed in\n    [Isca](https://execlim.github.io/Isca/modules/two_stream_gray_rad.html#frierson-byrne-schemes) for the *Frierson*\n    and *Byrne* radiation schemes:\n\n    $$\n    \\\\tau_s(\\\\phi) = (1-\\Delta \\\\tau^* \\sin^2 \\\\phi)\\\\tau_{0e}^*\\\\big(\\\\frac{p_s}{p_0}\\\\big)^{\\\\kappa^*}\n    $$\n\n    All default values are the default values in *Isca* if the option is not specified in the relavent `namelist`.\n\n    Args:\n        surface_pressure: Surface pressure in *Pa* with dimensions of latitude, longitude and time, $p_s$.\n            This is saved by *Isca* if the variable `ps` in the `dynamics` module is specified in the diagnostic table.\n        tau_equator: Surface optical depth at the equator, $\\\\tau_{0e}^*$.\n            It is specified through the option `atm_abs` in the `two_stream_gray_rad_nml` namelist.\n        tau_lat_var: Variation of optical depth with latitude, $\\Delta \\\\tau^*$.\n            It is specified through the option `sw_diff` in the `two_stream_gray_rad_nml` namelist.\n        pressure_exponent: Determines the variation of optical depth with pressure, $\\\\kappa^*$.\n            It is specified through the option `solar_exponent` in the `two_stream_gray_rad_nml` namelist.\n        ref_pressure: Reference pressure used by Isca in *Pa*.\n            It is specified through the option `pstd_mks` in the `constants_nml` namelist.\n\n    Returns:\n        Shortwave surface optical depth with dimensions of latitude, longitude and time.\n    \"\"\"\n    tau_surface = (1-tau_lat_var * np.sin(np.deg2rad(surface_pressure.lat))**2) * tau_equator\n    return tau_surface * (surface_pressure/ref_pressure)**pressure_exponent\n</code></pre>"},{"location":"code/utils/radiation/#isca_tools.utils.radiation.get_heat_capacity","title":"<code>get_heat_capacity(c_p, density, layer_depth)</code>","text":"<p>Given heat capacity un units of \\(JK^{-1}kg^{-1}\\), this returns heat capacity in units of \\(JK^{-1}m^{-2}\\).</p> <p>Parameters:</p> Name Type Description Default <code>c_p</code> <code>float</code> <p>Specific heat at constant pressure. Units: \\(JK^{-1}kg^{-1}\\)</p> required <code>density</code> <code>float</code> <p>Density of substance (usually air or water). Units: \\(kgm^{-3}\\)</p> required <code>layer_depth</code> <code>float</code> <p>Depth of layer. Units: \\(m\\)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Heat capacity in units of \\(JK^{-1}m^{-2}\\).</p> Source code in <code>isca_tools/utils/radiation.py</code> <pre><code>def get_heat_capacity(c_p: float, density: float, layer_depth: float) -&gt; float:\n    \"\"\"\n    Given heat capacity un units of $JK^{-1}kg^{-1}$, this returns heat capacity in units of $JK^{-1}m^{-2}$.\n\n    Args:\n        c_p: Specific heat at constant pressure.&lt;/br&gt;\n            Units: $JK^{-1}kg^{-1}$\n        density: Density of substance (usually air or water).&lt;/br&gt;\n            Units: $kgm^{-3}$\n        layer_depth: Depth of layer.&lt;/br&gt;\n            Units: $m$\n\n    Returns:\n        Heat capacity in units of $JK^{-1}m^{-2}$.\n    \"\"\"\n    return c_p * density * layer_depth\n</code></pre>"},{"location":"code/utils/radiation/#isca_tools.utils.radiation.opd_lw_gray","title":"<code>opd_lw_gray(lat, pressure=None, kappa=1, tau_eq=6, tau_pole=1.5, pressure_ref=10 ** 5, frac_linear=0.1, k_exponent=4)</code>","text":"<p>Returns the longwave optical depth used in the Frierson Isca <code>rad_scheme</code>.</p> <p>If <code>pressure</code> not provided, will return surface value. Otherwise, will return value at give <code>pressure</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>ndarray</code> <p><code>float [n_lat]</code> Latitude in degrees.</p> required <code>pressure</code> <code>Optional[float]</code> <p>Pressure in Pa.</p> <code>None</code> <code>kappa</code> <code>float</code> <p>Frierson optical depth scaling parameter. <code>opd</code> in <code>two_stream_gray_rad_nml</code> namelist.</p> <code>1</code> <code>tau_eq</code> <code>float</code> <p>Surface longwave optical depth at equator. <code>ir_tau_eq</code> in <code>two_stream_gray_rad_nml</code> namelist.</p> <code>6</code> <code>tau_pole</code> <code>float</code> <p>Surface longwave optical depth at pole. <code>ir_tau_pole</code> in <code>two_stream_gray_rad_nml</code> namelist.</p> <code>1.5</code> <code>pressure_ref</code> <code>float</code> <p>Reference pressure in Pa.</p> <code>10 ** 5</code> <code>frac_linear</code> <code>float</code> <p>Determines partitioning between linear term and \\(p^k\\) term. <code>linear_tau</code> in <code>two_stream_gray_rad_nml</code> namelist.</p> <code>0.1</code> <code>k_exponent</code> <code>float</code> <p>Pressure exponent. <code>wv_exponent</code> in <code>two_stream_gray_rad_nml</code> namelist.</p> <code>4</code> <p>Returns:</p> Name Type Description <code>opd</code> <code>ndarray</code> <p><code>float [n_lat]</code> Longwave optical depth</p> Source code in <code>isca_tools/utils/radiation.py</code> <pre><code>def opd_lw_gray(lat: np.ndarray, pressure: Optional[float] = None,\n                kappa: float = 1, tau_eq: float = 6, tau_pole: float = 1.5,\n                pressure_ref: float = 10**5, frac_linear: float=0.1, k_exponent: float=4) -&gt; np.ndarray:\n    \"\"\"\n    Returns the longwave optical depth used in the\n    [Frierson](https://execlim.github.io/Isca/modules/two_stream_gray_rad.html#frierson-byrne-schemes)\n    Isca `rad_scheme`.\n\n    If `pressure` not provided, will return surface value. Otherwise, will return value at give `pressure`.\n\n    Args:\n        lat: `float [n_lat]`&lt;/br&gt;\n            Latitude in degrees.\n        pressure: Pressure in Pa.\n        kappa: Frierson optical depth scaling parameter.&lt;/br&gt;\n            `opd` in `two_stream_gray_rad_nml` namelist.\n        tau_eq: Surface longwave optical depth at equator.&lt;/br&gt;\n            `ir_tau_eq` in `two_stream_gray_rad_nml` namelist.\n        tau_pole: Surface longwave optical depth at pole.&lt;/br&gt;\n            `ir_tau_pole` in `two_stream_gray_rad_nml` namelist.\n        pressure_ref: Reference pressure in Pa.\n        frac_linear: Determines partitioning between linear term and $p^k$ term.&lt;/br&gt;\n            `linear_tau` in `two_stream_gray_rad_nml` namelist.\n        k_exponent: Pressure exponent.&lt;/br&gt;\n            `wv_exponent` in `two_stream_gray_rad_nml` namelist.\n\n    Returns:\n        opd: `float [n_lat]`&lt;/br&gt;\n            Longwave optical depth\n    \"\"\"\n    opd_surf = kappa * (tau_eq + (tau_pole - tau_eq) * np.sin(np.deg2rad(lat)) ** 2)\n    if pressure is None:\n        return opd_surf\n    else:\n        pressure_factor = frac_linear * (pressure/pressure_ref) + (1-frac_linear) * (pressure/pressure_ref)**k_exponent\n        return opd_surf * pressure_factor\n</code></pre>"},{"location":"code/utils/stats/","title":"Statistics","text":""},{"location":"code/utils/stats/#isca_tools.utils.stats.z_score_from_confidence_interval","title":"<code>z_score_from_confidence_interval(confidence)</code>","text":"<p>Given a confidence interval, this returns the z-score, \\(Z\\). Similar to obtaining z-score from P-value with a double tail distribution.</p> <p>If variable, \\(x\\), has standard deviation, \\(\\sigma\\), \\(x \\pm Z \\sigma\\) is the value of \\(x\\) with the desired uncertainty.</p> <p>Parameters:</p> Name Type Description Default <code>confidence</code> <code>Union[array, float]</code> <p>The confidence interval (between 0 and 1) required, e.g. 0.95 for 95% confidence, will return a z-score of 1.96.</p> required <p>Returns:</p> Type Description <code>Union[array, float]</code> <p>Z-score corresponding to confidence interval</p> Source code in <code>isca_tools/utils/stats.py</code> <pre><code>def z_score_from_confidence_interval(confidence: Union[np.array, float]) -&gt; Union[np.array, float]:\n    \"\"\"\n    Given a confidence interval, this returns the z-score, $Z$. Similar to obtaining z-score from P-value with a double\n    tail distribution.\n\n    If variable, $x$, has standard deviation, $\\sigma$, $x \\pm Z \\sigma$ is the value of $x$ with the\n    desired uncertainty.\n\n    Args:\n        confidence: The confidence interval (between 0 and 1) required,\n            e.g. 0.95 for 95% confidence, will return a z-score of 1.96.\n\n    Returns:\n        Z-score corresponding to confidence interval\n    \"\"\"\n    return scipy.stats.norm.ppf(1-(1-confidence)/2)\n</code></pre>"},{"location":"code/utils/xarray/","title":"Xarray","text":""},{"location":"code/utils/xarray/#isca_tools.utils.xarray.convert_ds_dtypes","title":"<code>convert_ds_dtypes(ds, verbose=False)</code>","text":"<p>Convert all float variables to float32 and all int variables to int32 in an xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Input xarray Dataset.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print out variables converted</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ds_out</code> <code>Dataset</code> <p>Dataset with all float variables converted to float32 and all int variables to int32.</p> Source code in <code>isca_tools/utils/xarray.py</code> <pre><code>def convert_ds_dtypes(ds: xr.Dataset, verbose: bool = False) -&gt; xr.Dataset:\n    \"\"\"\n    Convert all float variables to float32 and all int variables to int32 in an xarray Dataset.\n\n    Args:\n        ds: Input xarray Dataset.\n        verbose: Whether to print out variables converted\n\n    Returns:\n        ds_out: Dataset with all float variables converted to float32 and all int variables to int32.\n    \"\"\"\n    converted = {}\n    float_conv = []\n    int_conv = []\n    for var_name, da in ds.data_vars.items():\n        if np.issubdtype(da.dtype, np.floating) and da.dtype != np.float32:\n            converted[var_name] = da.astype(np.float32)\n            float_conv.append(var_name)\n        elif np.issubdtype(da.dtype, np.integer) and da.dtype != np.int32:\n            converted[var_name] = da.astype(np.int32)\n            int_conv.append(var_name)\n        else:\n            converted[var_name] = da\n    if verbose:\n        if len(float_conv) &gt; 0:\n            print(f\"Converted the following float variables:\\n{float_conv}\")\n        if len(int_conv) &gt; 0:\n            print(f\"Converted the following integer variables:\\n{int_conv}\")\n    return ds.assign(**converted)\n</code></pre>"},{"location":"code/utils/xarray/#isca_tools.utils.xarray.flatten_to_numpy","title":"<code>flatten_to_numpy(var, keep_dim=None)</code>","text":"<p>Flattens <code>var</code> to a numpy array with at most 2 dimensions.</p> <p>Examples:</p> <p>If <code>var</code> has <code>dims=(lat, lon, lev)</code> and <code>keep_dim=lev</code>, it will return a numpy array of     size <code>[n_lat*n_lon, n_lev]</code>.</p> <p>If <code>var</code> has <code>dims=(lat, lon)</code> and <code>keep_dim=None</code>, it will return a numpy array of     size <code>[n_lat*n_lon]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>DataArray</code> <p>Variable to flatten.</p> required <code>keep_dim</code> <code>Optional[str]</code> <p>Dimension along which not to flatten.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>var_flatten</code> <code>ndarray</code> <p>Numpy array with flattened dimension first, and <code>keep_dim</code> dimension second.</p> Source code in <code>isca_tools/utils/xarray.py</code> <pre><code>def flatten_to_numpy(var: xr.DataArray, keep_dim: Optional[str] = None) -&gt; np.ndarray:\n    \"\"\"\n    Flattens `var` to a numpy array with at most 2 dimensions.\n\n    Examples:\n        If `var` has `dims=(lat, lon, lev)` and `keep_dim=lev`, it will return a numpy array of\n            size `[n_lat*n_lon, n_lev]`.\n\n        If `var` has `dims=(lat, lon)` and `keep_dim=None`, it will return a numpy array of\n            size `[n_lat*n_lon]`.\n\n    Args:\n        var: Variable to flatten.\n        keep_dim: Dimension along which not to flatten.\n\n    Returns:\n        var_flatten: Numpy array with flattened dimension first, and `keep_dim` dimension second.\n    \"\"\"\n    if (keep_dim is not None) and (keep_dim not in var.dims):\n        raise ValueError(f\"var must have a '{keep_dim}' dimension\")\n\n    # dims except vertical\n    flatten_dims = [d for d in var.dims if d != keep_dim]\n\n    # stack all flatten_dims into a single \"points\" axis\n    stacked = var.stack(points=flatten_dims)  # dims (..., lev_name) -&gt; (points, lev_name) after transpose\n    if keep_dim is not None:\n        stacked = stacked.transpose(\"points\", keep_dim)  # ensure order is (points, lev)\n    return stacked.values\n</code></pre>"},{"location":"code/utils/xarray/#isca_tools.utils.xarray.print_ds_var_list","title":"<code>print_ds_var_list(ds, phrase=None)</code>","text":"<p>Prints all variables in <code>ds</code> which contain <code>phrase</code> in the variable name or variable <code>long_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset to investigate variables of.</p> required <code>phrase</code> <code>Optional[str]</code> <p>Key phrase to search for in variable info.</p> <code>None</code> Source code in <code>isca_tools/utils/xarray.py</code> <pre><code>def print_ds_var_list(ds: xr.Dataset, phrase: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Prints all variables in `ds` which contain `phrase` in the variable name or variable `long_name`.\n\n    Args:\n        ds: Dataset to investigate variables of.\n        phrase: Key phrase to search for in variable info.\n\n    \"\"\"\n    # All the exceptions to deal with case when var does not have a long_name\n    var_list = list(ds.keys())\n    if phrase is None:\n        for var in var_list:\n            try:\n                print(f'{var}: {ds[var].long_name}')\n            except AttributeError:\n                print(f'{var}')\n    else:\n        for var in var_list:\n            if phrase.lower() in var.lower():\n                try:\n                    print(f'{var}: {ds[var].long_name}')\n                except AttributeError:\n                    print(f'{var}')\n                continue\n            try:\n                if phrase.lower() in ds[var].long_name.lower():\n                    print(f'{var}: {ds[var].long_name}')\n                    continue\n            except AttributeError:\n                continue\n    return None\n</code></pre>"},{"location":"code/utils/xarray/#isca_tools.utils.xarray.set_attrs","title":"<code>set_attrs(var, overwrite=True, **kwargs)</code>","text":"<p>Set attributes of a given variable.</p> <p>Examples:</p> <p><code>set_attrs(ds.plev, long_name='pressure', units='Pa')</code></p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>DataArray</code> <p>Variable to set attributes of.</p> required <code>overwrite</code> <code>bool</code> <p>If <code>True</code>, overwrite existing attributes, otherwise leave unchanged.</p> <code>True</code> <code>**kwargs</code> <code>str</code> <p>Attributes to set. Common ones include <code>long_name</code> and <code>units</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>DataArray</code> <p><code>var</code> with attributes set.</p> Source code in <code>isca_tools/utils/xarray.py</code> <pre><code>def set_attrs(var: xr.DataArray, overwrite: bool = True, **kwargs: str) -&gt; xr.DataArray:\n    \"\"\"\n    Set attributes of a given variable.\n\n    Examples:\n        `set_attrs(ds.plev, long_name='pressure', units='Pa')`\n\n    Args:\n        var: Variable to set attributes of.\n        overwrite: If `True`, overwrite existing attributes, otherwise leave unchanged.\n        **kwargs: Attributes to set. Common ones include `long_name` and `units`\n\n    Returns:\n        `var` with attributes set.\n    \"\"\"\n    # Function to set main attributes of given variable\n    for key in kwargs:\n        if (key in var.attrs) and not overwrite:\n            continue\n        var.attrs[key] = kwargs[key]\n    return var\n</code></pre>"},{"location":"code/utils/xarray/#isca_tools.utils.xarray.unflatten_from_numpy","title":"<code>unflatten_from_numpy(arr, var, keep_dim=None)</code>","text":"<p>Reconstructs an xarray.DataArray from a flattened NumPy array created by <code>flatten_to_numpy</code>.</p> <p>Examples:</p> <p>If <code>var</code> had dims=(lat, lon, lev) and <code>keep_dim='lev'</code>, and <code>arr</code> has shape (n_lat*n_lon, n_lev), this will return a DataArray with dims (lat, lon, lev).</p> <p>If <code>var</code> had dims=(lat, lon)<code>and</code>keep_dim=None<code>, and</code>arr` has shape (n_lat*n_lon), this will return a DataArray with dims (lat, lon).</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Flattened NumPy array from <code>flatten_to_numpy</code>.</p> required <code>var</code> <code>DataArray</code> <p>The original DataArray used to determine dimension order, shape, and coordinates.</p> required <code>keep_dim</code> <code>Optional[str]</code> <p>Dimension that was kept unflattened in <code>flatten_to_numpy</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>xr.DataArray: DataArray with the original dimensions and coordinates restored.</p> Source code in <code>isca_tools/utils/xarray.py</code> <pre><code>def unflatten_from_numpy(arr: np.ndarray, var: xr.DataArray, keep_dim: Optional[str] = None) -&gt; xr.DataArray:\n    \"\"\"\n    Reconstructs an xarray.DataArray from a flattened NumPy array created by `flatten_to_numpy`.\n\n    Examples:\n        If `var` had dims=(lat, lon, lev) and `keep_dim='lev'`, and `arr` has shape (n_lat*n_lon, n_lev),\n        this will return a DataArray with dims (lat, lon, lev).\n\n        If `var` had dims=(lat, lon)` and `keep_dim=None`, and `arr` has shape (n_lat*n_lon),\n        this will return a DataArray with dims (lat, lon).\n\n    Args:\n        arr: Flattened NumPy array from `flatten_to_numpy`.\n        var: The original DataArray used to determine dimension order, shape, and coordinates.\n        keep_dim: Dimension that was kept unflattened in `flatten_to_numpy`.\n\n    Returns:\n        xr.DataArray: DataArray with the original dimensions and coordinates restored.\n    \"\"\"\n    # Validate keep_dim\n    if (keep_dim is not None) and (keep_dim not in var.dims):\n        raise ValueError(f\"var must have a '{keep_dim}' dimension\")\n\n    # Identify flattened dims\n    flatten_dims = [d for d in var.dims if d != keep_dim]\n\n    # Compute target shape\n    target_shape = [var.sizes[d] for d in flatten_dims]\n    if keep_dim is not None:\n        target_shape.append(var.sizes[keep_dim])\n\n    # Reshape numpy array\n    reshaped = arr.reshape(target_shape)\n\n    # Reconstruct DataArray with original dimension order\n    if keep_dim is not None:\n        dims = flatten_dims + [keep_dim]\n    else:\n        dims = flatten_dims\n\n    # Unstack the flattened dims\n    da_flat = xr.DataArray(reshaped, dims=dims, coords={d: var.coords[d] for d in dims if d in var.coords},\n                           attrs=var.attrs)\n\n    # Reverse the stacking by unstacking the combined \"points\" dimension\n    if keep_dim is not None:\n        da_flat = da_flat.transpose(*var.dims)\n    else:\n        da_flat = da_flat.transpose(*var.dims)\n\n    return da_flat\n</code></pre>"},{"location":"hpc_basics/kennedy/","title":"kennedy","text":"<p>The following gives some instructions on how to get an account on kennedy and then login.</p>"},{"location":"hpc_basics/kennedy/#getting-an-account","title":"Getting an account","text":"<ul> <li>Register for an account \ud83d\udd17.</li> <li>Create a public ssh key.<ul> <li>Follow these instructions.</li> <li>I would not enter a password and leave the folder as the default.</li> <li>You should have created a private key (id_rsa) and a public key (id_rsa.pub). For me, these were saved to  /Users/joshduffield/.ssh/.</li> </ul> </li> <li>Email the public key to Herbert (herbert.fruchtl@st-andrews.ac.uk) so he can make you an account.<ul> <li>He should then email you a username and password.</li> </ul> </li> <li>If on Mac, install XQuartz on your local computer.</li> </ul>"},{"location":"hpc_basics/kennedy/#login","title":"Login","text":"<p>Username</p> <p>Your kennedy username will probably be the same as your normal St Andrews one. In the following, I have left  my username, jamd1, so just replace this wherever it appears with your username.</p> <ul> <li>To login, run the following in terminal <pre><code>ssh jamd1@kennedy.st-andrews.ac.uk\n</code></pre></li> <li>You may get the following message: <pre><code>The authenticity of host 'kennedy.st-andrews.ac.uk (138.251.14.67)' can't be established.\nED25519 key fingerprint is SHA256:eZyafolEFuPQuBBSJtBr55VbWI9Hmuko3GUkuV0vfaw.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> If so, just say <code>yes</code></li> <li>Then it will call for the password:       <pre><code>jamd1@kennedy.st-andrews.ac.uk's password:\n</code></pre></li> <li>If logged in successfully, the following should appear: <pre><code>Last login: Fri Sep 30 16:07:23 2022\nWelcome to kennedy\nPlease find information about using it at the HPC website:\nhttps://www.st-andrews.ac.uk/high-performance-computing/\n[jamd1@kennedy10 ~]$\n</code></pre></li> </ul>"},{"location":"hpc_basics/kennedy/#change-password","title":"Change Password","text":"<p>On first login, the password should be changed using the command <code>passwd</code>: <pre><code>[jamd1@kennedy10 ~]$ passwd\nChanging password for user jamd1.\nCurrent Password: \nNew password: \nRetype new password: \npasswd: all authentication tokens updated successfully.\n[jamd1@kennedy10 ~]$ \n</code></pre></p>"},{"location":"hpc_basics/kennedy/#file-transfer","title":"File Transfer","text":"Using Terminal <p>You can exchange files between your local computer and kennedy using terminal: <pre><code>sftp jamd1@kennedy.st-andrews.ac.uk\njamd1@kennedy.st-andrews.ac.uk's password: \nConnected to kennedy.st-andrews.ac.uk.\nsftp&gt; \n</code></pre></p> <ul> <li>Then use <code>cd &lt;dir&gt;</code> to change directory</li> <li><code>put &lt;file&gt;</code> to move file from local computer to kennedy.</li> <li><code>get &lt;file&gt;</code> to move file from kennedy to local computer.</li> </ul> <p>I find it easiest to exchange files using FileZilla which has a graphical interface.</p> <ul> <li>Download it from this website</li> <li> <p>Next, you need to specify the private key which was created earlier.</p> <ul> <li>In FileZilla, in the top bar, click Edit and then Settings.</li> <li>Then click Connection/SFTP and then Add key file.</li> <li>Select the private key created earlier i.e. it should be called id_rsa.  </li> <li>Then press OK </li> <li>This comes from the On a Mac section on the wiki. </li> </ul> </li> <li> <p>Now, in the top bar, click File and then Site Manager</p> <ul> <li>Create a New Site for kennedy.</li> <li>Protocol: SFTP</li> <li>Host: kennedy.st-andrews.ac.uk</li> <li>Username and password is the same as used to login.  </li> <li>Then click Connect and it should hopefully work with a screen like the following appearing.  </li> </ul> </li> <li>You should now be able to drag and drop files from the local computer (left) to kennedy (right) and create new  directories etc.</li> </ul>"},{"location":"hpc_basics/kennedy/#conda","title":"CONDA","text":""},{"location":"hpc_basics/kennedy/#installation","title":"Installation","text":"<p>To install CONDA, login and then run <code>install-conda</code> in terminal. This should then produce some files in the location /gpfs1/apps/conda/jamd1/conda:  </p>"},{"location":"hpc_basics/kennedy/#create-environment","title":"Create Environment","text":"<p>To create a <code>python 3.9</code> conda environment called <code>test_env</code> run: <pre><code>conda create -n test_env python=3.9\n</code></pre> Then to activate it, run: <pre><code>conda activate test_env\n</code></pre> Terminal should then look something like this: <pre><code>(test_env) [jamd1@kennedy10 ~]$\n</code></pre></p>"},{"location":"hpc_basics/kennedy/#error-wrong-python-version","title":"Error - Wrong Python Version","text":"Error <p>If you now run <code>python -V</code> to check the python version, it will print <code>Python 2.7.5</code> even though the conda  environment is <code>python 3.9</code>.</p> <p>This is because it is using the wrong python. If you run <code>which python</code>, it will print <code>/usr/bin/python</code> which  has nothing to do with the <code>test_env</code> CONDA environment.</p> <p>The problem is that the python installed using CONDA does not have execution permissions, so it reverts to  a python version which does. To give execution permissions, you can run the following line (The <code>$USER</code> will  automatically be your username so you don't need to change it): <pre><code>chmod u+x /gpfs1/apps/conda/$USER/conda/envs/*/bin/*\n</code></pre></p> <p>If you now run <code>conda deactivate</code> and then <code>conda activate test_env</code> to log out and then back into the CONDA  environment, <code>python -V</code> should now print <code>Python 3.9.13</code> and <code>which python</code> should print  <code>/gpfs1/apps/conda/jamd1/conda/envs/test_env/bin/python</code>.</p> <p>In general, whenever you hit a <code>Permission Denied</code> error when using a CONDA environment, I would run  <code>chmod u+x /gpfs1/apps/conda/$USER/conda/envs/*/bin/*</code> as a first attempt at fixing it.</p>"},{"location":"hpc_basics/kennedy/#resources","title":"Resources","text":"<p>There is a website for kennedy. There is also a recorded  lecture and the corresponding  lecture notes.</p> <p>These may be useful, especially if using an operating system other than Mac.</p>"},{"location":"hpc_basics/pycharm/","title":"Pycharm","text":"<p>My preferred IDE for python is Pycharm. The following gives instructions on how files can be synced between your local computer and a high performance computer e.g. kennedy  using Pycharm.</p> <p>Then, you basically code as you normally would on your local computer but everything happens on the high performance computer.</p>"},{"location":"hpc_basics/pycharm/#syncing-a-pycharm-project","title":"Syncing a Pycharm Project","text":"<p>Pycharm also provides a useful video and instructions.</p> <ul> <li> <p>First, login to kennedy and create a CONDA environment. I will be using an environment called <code>test_env</code>.</p> </li> <li> <p>Next, create a Pycharm project. I wouldn't worry too much about the python interpreter as we will later  change this to use the CONDA environment we just set up on the high performance computer. I would probably just use  set up a CONDA environment or use one that is already been set up.</p> </li> <li> <p>Create a couple of files in the project so we can sync it to the high performance computer e.g. </p> </li> <li> <p>Next choose the remote CONDA environment as the python interpreter by following the 5 steps indicated below.</p> 12345 <p></p> <p></p> <p></p> <p></p> <p></p> <p>1.Click on your current python interpreter in the bottom right (Python 3.9 (Isca) for me).    Then add a new SSH interpreter. 2.Enter your kennedy login details. 3.Enter the corresponding password. 4.You should get a confirmation message indicating that you connected successfully. 5.In the next screen, select an existing environment.In the Interpreter section, select the python file from    the CONDA environment you want to use.In the Sync folders section, enter the address on kennedy where you    would like the project to be saved (The project name must be the same as it is on your local computer though    i.e. pythonProject for me).</p> </li> <li> <p>After confirming this, you should get a File Transfer tab in the bottom toolbar, indicating the files  in the project have been transferred. When changes are made to these files locally, they will also be changed  remotely. </p> Remote Files don't get deleted when deleted locally <p>By default, when you delete a file locally, the remote equivalent does not get deleted.  To change this, go to Tools \u2192 Deployment \u2192 Options.  Then tick the Delete remote files when local are deleted box:</p> Tools \u2192 Deployment \u2192 OptionsDelete remote files when local are deleted <p></p> <p></p> </li> </ul>"},{"location":"hpc_basics/pycharm/#python-console-and-terminal","title":"Python Console and Terminal","text":"<p>The Python Console in the bottom toolbar should now be using the remote CONDA version of python as indicated by the first line in blue and the current path is the remote project as indicated by the last blue line, starting <code>sys.path.extend</code>: </p> <p>However, this CONDA version of python may not have anything installed yet, hence I get the error when trying  to import <code>numpy</code>. To install a package on the remote CONDA, go to the Terminal tab in the bottom toolbar and  start a SSH terminal session by clicking the downward arrow and selecting the correct Remote Python option:</p> <p></p> <p>This will log you into kennedy without asking for login details seen as you have already provided them. Then, activate the CONDA environment and install the package (<code>pip install numpy</code>). If you then restart the Python Console, you should now be able to import <code>numpy</code>.</p>"},{"location":"hpc_basics/pycharm/#debugging","title":"Debugging","text":"<p>I think that one of the main advantages of using Pycharm is the debugging feature, so you can pause a function  in real time to see the variables or see why it is hitting an error. This feature can also be used in this remote  setup.</p> <p>First, create a run configuration for the  script that you want to run. I will be using <code>script1.py</code>:</p> <p></p> <p>Then, to run in debug mode, click the little beetle in the top right. If you then add a breakpoint somewhere,  the code should stop at that point, so you can see the value of all the variables:</p> <p></p>"},{"location":"hpc_basics/pycharm/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>The following comes from this website.</p> <ul> <li>On the remote computer, install <code>jupyterlab</code> in your CONDA environment:  <code>conda install -c conda-forge jupyterlab</code>.</li> <li>Next, start a screen session, so you  can drop in and out of it and leave it running for ages: <code>screen -S jupyter_test</code></li> <li> <p>Then, run the following to enable a jupyter connection:</p> <pre><code>conda activate test_env\nchmod u+x /gpfs1/apps/conda/$USER/conda/envs/*/bin/*\njupyter lab --no-browser --port=1111\n</code></pre> <p>The second line is just to ensure you have permission to use jupyter.  I get a Permission Denied error without doing this. The port in the last line can be any 4 digit number.</p> </li> <li> <p>You should then get a list of links: </p> </li> <li> <p>Next, open a local terminal window and type:</p> <p><pre><code>ssh -N -f -L localhost:1111:localhost:1111 jamd1@kennedy.st-andrews.ac.uk\n</code></pre> You will have to enter your kennedy password.</p> </li> <li> <p>Next, create a jupyter notebook in Pycharm as indicated by the first image below:</p> Create NotebookConfigure NotebookExample Notebook <p></p> <p></p> <p></p> <p>Then configure the notebook (button in top right of notebook), by specifying the Configured Server to be  the last link that was printed out when creating the jupyter connection (second image. You should now be able to create a notebook, with access to the environmental variables on the remote computer as indicated by the third image.</p> </li> <li> <p>If you prefer the normal browser interface for the jupyter notebook, you can click the globe in the rop right corner.</p> </li> <li>If you keep the screen session going, you should only have to re-run the line  <code>ssh -N -f -L localhost:1111:localhost:1111 jamd1@kennedy.st-andrews.ac.uk</code>  everytime you want to use the notebook again.</li> </ul>  Error - <code>Could not request local forwarding</code> <p>You may need to restart the screen session every now and again with a new port number. I had to do this  because I recieved the following error:</p> <p></p> <p>If you get this error, just terminal the screen session (<code>pkill screen</code> kills all screen sessions and <code>screen -S 13780 -X quit</code> kills screen session with number <code>13780</code>) and then follow the above instructions again with a new port number.</p> Changes to jupyter notebook files do not sync to remote computer <p>To upload the changes you made to the jupyter notebook file locally, you can use: Tools \u2192 Deployment \u2192 Sync with Deployed to jamd1@kennedy... Make sure you are using the blue arrow to sync from your local to the remote computer.</p>"},{"location":"hpc_basics/shell_scripting/","title":"Shell Scripting","text":"<p>A shell script (<code>.sh</code>) can be used to submit a sequence of commands to terminal.</p>"},{"location":"hpc_basics/shell_scripting/#video","title":"Video","text":"<p>The following video outlines the basics of shell scripting. </p> <p>08:32: The first line of a shell script must be the shebang line, telling system which shell interpreter to use. The  default shell interpreter can be found  by running <code>echo $SHELL</code> in terminal. This will return something like <code>/bin/bash</code>.</p> <p>10:36: Before running a shell script, you may need to give it execution permissions (this is similar to the CONDA python issue with kennedy).</p> <p>13:00: A shell script can accept any number of parameters.</p>"},{"location":"hpc_basics/shell_scripting/#example-script","title":"Example Script","text":"<p>Let's create an example script which accepts 3 parameters and prints them, as well as the file name, called example.sh (execution permission can be given through <code>chmod u+x example.sh</code>):</p> <pre><code>#!/bin/bash\n\n#This program accepts 3 parameters and prints them\necho File Name: $0\necho Param 1  : $1\necho Param 2  : $2\necho Param 3  : $3\n</code></pre> <p>Running this script through <code>./example.sh p1 p2 p3</code> prints: <pre><code>Exec Name: ./example.sh\nParam 1  : p1\nParam 2  : p2\nParam 3  : p3\n</code></pre></p>"},{"location":"hpc_basics/shell_scripting/#useful-commands","title":"Useful Commands","text":"<ul> <li><code>echo</code> - Use to display the value of a variable e.g. <code>echo $SHELL</code> will display the value of <code>SHELL</code>. </li> <li><code>export var=5</code> - this will mean that an environmental variable called <code>var</code> will be created and given the value <code>5</code>. It can then be accessed in terminal e.g. it can be printed through <code>echo $var</code>. </li> <li><code>printenv</code> - This displays all the environmental variables e.g. if the parameter <code>SHELL</code> shows up in the list,  then its value can be accessed through <code>$SHELL</code> </li> <li><code>source shell_script.sh</code> - This will make all variables defined through <code>export</code> in the shell script shell_script.sh  be available in the current terminal. </li> </ul>"},{"location":"hpc_basics/slurm/","title":"Slurm","text":"<p>To run a job on kennedy, it must be submitted to a Slurm queue. To do this, you basically modify a .sh script with some headers indicating which node to submit the job to.</p>"},{"location":"hpc_basics/slurm/#useful-commands","title":"Useful commands \ud83d\udd17","text":""},{"location":"hpc_basics/slurm/#monitoring-jobs","title":"Monitoring jobs","text":"<p>These can be used to monitor the current state of the queues and your jobs.</p> <ul> <li><code>squeue -u jamd1</code> - shows all jobs submitted by the username <code>jamd1</code> </li> <li><code>scancel 158329</code> - cancels job with id <code>158329</code>. Job ID is shown with the above command or <code>squeue</code> </li> <li><code>scancel -u jamd1</code> - cancels all jobs by the username <code>jamd1</code> </li> <li><code>squeue -t RUNNING</code> - shows all jobs that are currently running  </li> <li><code>squeue -p debug</code> - shows all jobs queued and running for the partition called <code>debug</code> </li> </ul>"},{"location":"hpc_basics/slurm/#sbatch","title":"SBATCH","text":"<p>These must be added to top of .sh script, just below the shebang line. \ud83d\udd17</p> <ul> <li><code>#SBATCH --job-name=</code> - name of job e.g. <code>test</code> </li> <li><code>#SBATCH --output=</code> - file where things printed to console are saved e.g. <code>output.txt</code> </li> <li><code>#SBATCH --error=</code> - file where any errors are saved e.g. <code>error.txt</code> </li> <li><code>#SBATCH --time=</code> - maximum walltime for the job e.g. <code>01:00:00</code> for 1 hour or <code>01:00</code> for 1 minute.  </li> <li><code>#SBATCH --nodes=</code> - number of nodes used e.g. <code>1</code> </li> <li><code>#SBATCH --ntasks-per-node=</code> - Number of processors per node e.g. <code>16</code>.</li> <li><code>#SBATCH --partition=</code> - the queue to submit a job to. </li> </ul> Options for kennedy <ul> <li>singlenode  Jobs requiring one node, with a maximum of 32 cores. Maximum run-time 30 days.</li> <li>parallel  Jobs running parallel across multiple nodes.    There is no upper limit on the number of nodes (other than how many there are), but remember that you are using a shared    resource. Maximum run-time 30 days.</li> <li>gpu  Jobs requesting one or both of kennedy's GPU nodes.   Maximum run-time 30 days.    Note that you still need to request the GPUs with the <code>--gres</code> flag.    The following line would request both GPUs on a node:   <code>#SBATCH --gres=gpu:2</code></li> <li>debug  Small and short jobs, usually meant for tests or debugging.    This partition is limited to one node and a maximum of two hours run-time.  I would use this one for starting off with and running short scripts.</li> </ul> <ul> <li><code>#SBATCH --mail-type=</code> - indicates when you would like to receive email notifications, <code>NONE</code>, <code>FAIL</code>, <code>ALL</code> or <code>END</code>.  </li> <li><code>#SBATCH --mail-user=</code> - email address e.g. <code>jamd1@st-andrews.ac.uk</code> </li> </ul>"},{"location":"hpc_basics/slurm/#submitting-a-job","title":"Submitting a job","text":"<p>To submit a script which prints 'hello world' to the <code>debug</code> queue with 16 tasks per node with the output saved to the file <code>example_output.txt</code>, the <code>example.sh</code> script would look like this:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=example\n#SBATCH --output=\"example_output.txt\"\n#SBATCH --error=\"example_error.txt\"\n#SBATCH --time=01:00 # maximum walltime for the job\n#SBATCH --nodes=1 # specify number of nodes\n#SBATCH --ntasks-per-node=16 # specify number of processors per node\n#SBATCH --mail-type=END # send email at job completion\n#SBATCH --mail-user=$USER@st-andrews.ac.uk # email address\n#SBATCH --partition=debug # queue to run on\n\necho hello world\n</code></pre> <p>To submit this job, you then need to login to kennedy and  transfer <code>example.sh</code> from your local computer to kennedy.</p> <p>Then make sure your current directory contains the <code>example.sh</code> file and run <code>sbatch example.sh</code>.  This should produce the files <code>example_output.txt</code> and <code>example_error.txt</code> in your current directory when it is finished running (use <code>squeue -p debug</code> to see its place in the queue).</p>"},{"location":"hpc_basics/slurm/#multiple-jobs-with-environmental-variables","title":"Multiple jobs with environmental variables","text":"<p>The submission of multiple instances of the same job but using different numbers of cores can be achieved  through an overarching python script which then runs the shell (<code>.sh</code>) script with different input  parameters.</p> <p>For example, if the three files below are all in the current directory then running <code>python example_run.py</code> will send two jobs to the <code>debug</code> queue, both with job name <code>example</code> but one on 8 cores per node and one on 16.</p> Using CONDA <p>Because this uses python, you probably want to run it with a relatively modern version of python. To do this, activate a CONDA environment before running <code>python example_run.py</code>.</p> example_run.pyexample.shexample_print.py <pre><code>from os import system\njob_name = 'example'\nshell_script = 'example.sh'\nn_nodes = 1\noutput_text = 'hello world'\nfor n_cores in [8, 16]:\n    system(f'bash {shell_script} {job_name} {n_nodes} {n_cores} {output_text')\n</code></pre> <pre><code>#!/bin/bash\nsbatch &lt;&lt;EOT\n#!/bin/bash\n#SBATCH --job-name=$1\n#SBATCH --output=\"outFile\"$3\".txt\"\n#SBATCH --error=\"errFile\"$3\".txt\"\n#SBATCH --time=01:00 # maximum walltime for the job\n#SBATCH --nodes=$2 # specify number of nodes\n#SBATCH --ntasks-per-node=$3 # specify number of processors per node\n#SBATCH --mail-type=END # send email at job completion\n#SBATCH --mail-user=$USER@st-andrews.ac.uk # email address\n#SBATCH --partition=debug # queue to run on\n\nexport OUTPUT_TEXT=$4   # export so can be used by python script\npython example_print.py\nexit 0\nEOT\n</code></pre> <pre><code>import os\nprint(f'Job Name: {os.environ['SLURM_JOB_NAME']}')\nprint(f'Number of nodes: {int(os.environ['SLURM_NNODES'])}')\nprint(f'Number of tasks per node: {int(os.environ['SLURM_NTASKS_PER_NODE'])}')\nprint(f'Output text: {os.environ['OUTPUT_TEXT']}')\n</code></pre> Wrapper in <code>.sh</code> script <p>The <code>example.sh</code> script is slightly different when it is called from a python script. It needs a wrapper which is what the <code>EOT</code> stuff is.</p> <p>When both jobs have been completed, this will then produce 4 files in the current directory,  <code>outFile8.txt</code>, <code>outFile16.txt</code>, <code>errFile8.txt</code> and <code>errFile16.txt</code>.</p> outFile8.txtoutFile16.txt <pre><code>Job Name: example\nNumber of nodes: 1\nNumber of tasks per node: 8\nOutput text: hello world\n</code></pre> <pre><code>Job Name: example\nNumber of nodes: 1\nNumber of tasks per node: 16\nOutput text: hello world\n</code></pre> <p>This example shows that the <code>#SBATCH</code> commands in the <code>example.sh</code> script produces  environmental variables with a <code>SLURM</code> prefix which can then be accessed. These variables cannot be accessed from the <code>example.sh</code> script itself though e.g. <code>echo $SLURM_SLURM_JOB_NAME</code> would not print anything if it was included in <code>example.sh</code>.</p>"},{"location":"hpc_basics/slurm/#debugging","title":"Debugging","text":"<p>It can be annoying if you are running a small job, to submit it to a <code>SLURM</code> queue and then wait for  it to get to the front.</p> <p>As an alternative, you can just login to kennedy and run the <code>example_print.py</code> script (after activating the relevant CONDA environment). To do this, you first need to set the environmental variables that the script uses by running <code>source example_params.sh</code> where the <code>example_params.sh</code> script is given below:</p> <pre><code>export SLURM_JOB_NAME=example\nexport SLURM_NTASKS_PER_NODE=8\nexport OUTPUT_TEXT=hello world\n</code></pre> <p>Doing this, you are limited to one node and 8 tasks per node, so you may get problems  with Isca if you try to run a simulation with more than 8 tasks per node.</p>"},{"location":"namelists/","title":"Namelists","text":"<p>To run a simulation, a namelist .nml file is required,  specifying the value of configuration options in a variety of categories or namelists. A diagnostic table  file is also required to specify what diagnostics to save for the experiment.</p> <p>Each page in this section is devoted to a particular namelist, indicating the source code where it is specified  and any relevant pages on Isca's website. There are then two sections,  Options and Diagnostics:</p>"},{"location":"namelists/#options","title":"Options","text":"<p>This section is related to the namelist .nml file and indicates the available options that can be configured in that particular namelist, as well as their default values.</p> <p>These options can be found, by looking at the module source code (e.g.  idealized_moist_phys.F90 for <code>idealized_moist_phys_nml</code>) and searching for the word namelist. You should then find some code like that below, indicating all the options in the namelist. <pre><code>namelist / idealized_moist_phys_nml / turb, lwet_convection, do_bm, do_ras, &amp;\n                                      roughness_heat, do_cloud_simple,      &amp;\n                                      two_stream_gray, do_rrtm_radiation,   &amp;\n                                      do_damping\n</code></pre> Then just above this code, the options will be initialized with their default values.</p>"},{"location":"namelists/#diagnostics","title":"Diagnostics","text":"<p>This section is related to the diagnostic table file and indicates the diagnostics relevant to the namelist that  can be saved to the data directory.</p> <p>These diagnostics can be found, by looking at the module source code (e.g.  idealized_moist_phys.F90 for <code>idealized_moist_phys_nml</code>) and searching for the word mod_name. You should then find some code like that below, indicating the available diagnostics for the namelist. <pre><code>id_cond_dt_qg = register_diag_field(mod_name, 'dt_qg_condensation',        &amp;\n     axes(1:3), Time, 'Moisture tendency from condensation','kg/kg/s')\nid_cond_dt_tg = register_diag_field(mod_name, 'dt_tg_condensation',        &amp;\n     axes(1:3), Time, 'Temperature tendency from condensation','K/s')\n</code></pre></p>"},{"location":"namelists/#mod_name","title":"<code>mod_name</code>","text":"<p>The <code>mod_name</code> used in the diagnostic table file is not always the same as the corresponding namelist associated with it. The table below gives the <code>mod_name</code> associated with each  namelist. There is a flowchart on Isca's website  which is quite useful for understanding how the modules are related to each other.</p> Module namelist <code>mod_name</code> Main <code>atmos_model.F90</code> <code>main_nml</code> N/A <code>atmosphere.F90</code> <code>atmosphere_nml</code> N/A <code>hs_forcing.F90</code> <code>hs_forcing_nml</code> <code>hs_forcing</code> <code>idealized_moist_phys.F90</code> <code>idealized_moist_phys_nml</code> <code>atmosphere</code> <code>spectral_dynamics.F90</code> <code>spectral_dynamics_nml</code> <code>dynamics</code> Convection <code>qe_moist_convection.F90</code> <code>qe_moist_convection_nml</code> N/A <code>betts_miller.f90</code> <code>betts_miller_nml</code> N/A <code>ras.F90</code> <code>ras_nml</code> <code>ras</code> Condensation <code>lscale_cond.F90</code> <code>lscale_cond_nml</code> N/A <code>sat_vapor_pres.F90</code> <code>sat_vapor_pres_nml</code> N/A Radiation <code>two_stream_gray_rad.F90</code> <code>two_stream_gray_rad_nml</code> <code>two_stream</code> <code>rrtm_radiation.F90</code> <code>rrtm_radiation_nml</code> <code>rrtm_radiation</code> <code>socrates_config_mod.F90</code> <code>socrates_rad_nml</code> <code>socrates</code> Surface <code>mixed_layer.F90</code> <code>mixed_layer_nml</code> <code>mixed_layer</code> <code>surface_flux.F90</code> <code>surface_flux_nml</code> N/A <code>spectral_init_cond.F90</code> <code>spectral_init_cond_nml</code> N/A Damping <code>damping_driver.F90</code> <code>damping_driver_nml</code> <code>damping</code> Turbulence <code>vert_turb_driver.F90</code> <code>vert_turb_driver_nml</code> <code>vert_turb</code> <code>diffusivity.F90</code> <code>diffusivitiy_nml</code> N/A"},{"location":"namelists/condensation/","title":"Condensation","text":"<p>The namelists for configuring condensation are <code>lscale_cond_nml</code> and  <code>sat_vapor_pres_nml</code>.</p> <p>The relevant diagnostics are given through the <code>atmosphere</code>  module name in the <code>idealized_moist_phys</code> module.</p>"},{"location":"namelists/condensation/lscale_cond/","title":"Large Scale Condensation and Precipitation","text":"<p>The <code>lscale_cond_nml</code>  only ever needs to be specified if  <code>convection_scheme</code> in  <code>idealized_moist_phys_nml</code> is either <code>SIMPLE_BETTS_CONV</code>, <code>FULL_BETTS_MILLER_CONV</code> or <code>RAS_CONV</code>. This namelist is described on Isca's website  and is used in the  Frierson example script. All the options are also described below:</p>"},{"location":"namelists/condensation/lscale_cond/#options","title":"Options","text":""},{"location":"namelists/condensation/lscale_cond/#hc","title":"<code>hc</code>","text":"<p>float The relative humidity at which large scale condensation occurs.  \\(0.0 \\leq hc \\leq 1.0\\). Default: <code>1.0</code></p>"},{"location":"namelists/condensation/lscale_cond/#do_evap","title":"<code>do_evap</code>","text":"<p>bool  The flag for the re-evaporation of moisture in sub-saturated layers below,  if <code>True</code> then re-evaporation is performed.  Default: <code>False</code></p>"},{"location":"namelists/condensation/lscale_cond/#do_simple","title":"<code>do_simple</code>","text":"<p>bool  If <code>True</code> then all precipitation is rain/liquid precipitation, there is no snow/frozen precipitation.  Default: <code>False</code></p>"},{"location":"namelists/condensation/sat_vapor_pres/","title":"Saturation Vapor Pressure","text":"<p>The <code>sat_vapor_pres_nml</code>  namelist specifies options for computing the saturation vapor pressure, specific humidity and vapor mixing ratio at a specified relative humidity.  Some of the most common options are described below:</p>"},{"location":"namelists/condensation/sat_vapor_pres/#options","title":"Options","text":""},{"location":"namelists/condensation/sat_vapor_pres/#do_simple","title":"<code>do_simple</code>","text":"<p>bool  If <code>True</code>, the calculation that is performed is simplified.  This option seems to be set to <code>True</code> for most example experiments e.g.  bucket_hydrology and frierson.  Default: <code>False</code></p>"},{"location":"namelists/convection/","title":"Convection","text":"<p>The namelist for configuring convection is either <code>qe_moist_convection_nml</code>, <code>betts_miller_nml</code> or <code>ras_nml</code> depending on the choice of the <code>convection_scheme</code> option.</p> <p>The relevant diagnostics are given through the <code>atmosphere</code>  module name in the <code>idealized_moist_phys</code> module.</p> <p>If <code>convection_scheme</code> is <code>ras_nml</code>,  then there are some additional diagnostics that can be specified with the <code>ras</code> module name.</p> <p>If <code>convection_scheme</code> is <code>DRY</code>, <code>gamma</code> and <code>tau</code> in  [<code>dry_convection_nml</code>] must be specified  because they have no default value. An error will be raised otherwise.</p>"},{"location":"namelists/convection/betts_miller/","title":"Betts-Miller Convection","text":"<p>The <code>betts_miller_nml</code>  only ever needs to be specified if  <code>convection_scheme = FULL_BETTS_MILLER_CONV</code> in  <code>idealized_moist_phys_nml</code>. Isca give an  example script using this namelist. Some of the most common options for configuring this convection scheme are described below:</p>"},{"location":"namelists/convection/betts_miller/#options","title":"Options","text":""},{"location":"namelists/convection/betts_miller/#tau_bm","title":"<code>tau_bm</code>","text":"<p>float Relaxation timescale, \\(\\tau\\) (seconds). Default: <code>7200</code></p>"},{"location":"namelists/convection/dry/","title":"Dry Convection","text":"<p>The <code>dry_convection_nml</code>  only ever needs to be specified if  <code>convection_scheme = DRY</code> in  <code>idealized_moist_phys_nml</code>. The key options are also described below:</p>"},{"location":"namelists/convection/dry/#options","title":"Options","text":""},{"location":"namelists/convection/dry/#tau","title":"<code>tau</code>","text":"<p>float Relaxation timescale, \\(\\tau\\) (seconds).  The equivalent parameter for moist convection is set to <code>7200</code>. Default: <code>N/A</code></p>"},{"location":"namelists/convection/dry/#gamma","title":"<code>gamma</code>","text":"<p>float Prescribed lapse rate. Set to <code>1</code> for dry lapse rate. Default: <code>N/A</code></p>"},{"location":"namelists/convection/qe_moist_convection/","title":"Quasi Equilibrium Moist Convection","text":"<p>The <code>qe_moist_convection_nml</code>  only ever needs to be specified if  <code>convection_scheme = SIMPLE_BETTS_MILLER_CONV</code> in  <code>idealized_moist_phys_nml</code>. This convection scheme is described on Isca's website  and is used in the  Frierson example script. Some of the most common options are also described below:</p>"},{"location":"namelists/convection/qe_moist_convection/#options","title":"Options","text":""},{"location":"namelists/convection/qe_moist_convection/#tau_bm","title":"<code>tau_bm</code>","text":"<p>float Relaxation timescale, \\(\\tau\\) (seconds). Default: <code>7200</code></p>"},{"location":"namelists/convection/ras/","title":"Relaxed Arakawa Schubert Convection","text":"<p>The <code>ras_nml</code>  only ever needs to be specified if  <code>convection_scheme = RAS</code> in  <code>idealized_moist_phys_nml</code>. Some of the most common options for configuring this convection scheme are described below:</p>"},{"location":"namelists/convection/ras/#options","title":"Options","text":""},{"location":"namelists/convection/ras/#fracs","title":"<code>fracs</code>","text":"<p>float Fraction of planetary boundary layer mass allowed to be used by a cloud-type in time \\(DT\\). Default: <code>0.25</code></p>"},{"location":"namelists/convection/ras/#diagnostics","title":"Diagnostics","text":"<p>The diagnostics for  this module  can be specified using the <code>module_name</code> of <code>ras</code> in the  diagnostic table file. I haven't been able to save any of these  diagnostics, get error when using the <code>ras</code> module name. Some available diagnostics are given below:</p>"},{"location":"namelists/convection/ras/#tdt_conv","title":"<code>tdt_conv</code>","text":"<p>Temperature tendency from RAS. Dimensions: time, lat, lon, pressure Units: \\(Ks^{-1}\\)</p>"},{"location":"namelists/convection/ras/#qdt_conv","title":"<code>qdt_conv</code>","text":"<p>Specific humidity tendency from RAS. Dimensions: time, lat, lon, pressure Units: \\(kgkg^{-1}s^{-1}\\)</p>"},{"location":"namelists/convection/ras/#prec_conv","title":"<code>prec_conv</code>","text":"<p>Precipitation rate from RAS. Dimensions: time, lat, lon, pressure Units: \\(kgm^{-2}s^{-1}\\)</p>"},{"location":"namelists/damping/","title":"Damping Driver","text":"<p>The <code>damping_driver_nml</code>  only ever needs to be specified if  <code>do_damping=.true.</code> in  <code>idealized_moist_phys_nml</code>. This namelist accounts for subgrid-scale processes which decelerate fast winds at upper levels.  It is described on Isca's website and is used in the  Frierson example script.  Some of the most common options are also described below:</p>"},{"location":"namelists/damping/#options","title":"Options","text":""},{"location":"namelists/damping/#do_rayleigh","title":"<code>do_rayleigh</code>","text":"<p>bool  On/off switch for Rayleigh friction.  Default: <code>False</code></p>"},{"location":"namelists/damping/#diagnostics","title":"Diagnostics","text":"<p>The diagnostics for  this module  can be specified using the <code>module_name</code> of <code>damping</code> in the  diagnostic table file. The list of available diagnostics is available on  Isca's website.  Some are also given below:</p>"},{"location":"namelists/damping/#udt_rdamp","title":"<code>udt_rdamp</code>","text":"<p>Zonal wind tendency for Rayleigh damping. Can only be returned if <code>do_rayleigh=True</code>. Dimensions: time, lat, lon, pressure Units: \\(ms^{-2}\\)</p>"},{"location":"namelists/main/","title":"Main","text":"<p>The <code>main_nml</code>  namelist contains options relating to the  frequency at which data is saved. Some of the most common options are described below.</p>"},{"location":"namelists/main/#options","title":"Options","text":""},{"location":"namelists/main/#days","title":"<code>days</code>","text":"<p>integer Data is saved after this many days. E.g. if <code>days=10</code>, the first output folder <code>run0001</code> will contain data for the first 10 days of the simulation. Can also use the options <code>hours</code>, <code>minutes</code> and <code>seconds</code> if you want to increase the precision in this duration. Must be specified, typical would be <code>30</code>. Default: <code>0</code></p>"},{"location":"namelists/main/#calendar","title":"<code>calendar</code>","text":"<p>string Specifies the calendar used for the experiment.  Options are <code>thirty_day</code>, <code>julian</code>, <code>noleap</code> or <code>no_calendar</code>. Typical is <code>thirty_day</code> which means months of \\(30\\) days each so a year would be \\(360\\) days. Default: N/A</p>"},{"location":"namelists/main/#dt_atmosphere","title":"<code>dt_atmosphere</code>","text":"<p>integer Duration of each time step in the simulation (seconds). Must be specified, typical would be <code>720</code>. Default: <code>0</code></p>"},{"location":"namelists/main/#current_date","title":"<code>current_date</code>","text":"<p>list - integer The start date of the simulation. Not exactly sure, but I think the 6 values are  <code>[Year, Month, Day, Hour, Minute, Second]</code>. In the example experiments, they use  <code>[1,1,1,0,0,0]</code> and  <code>[2000,1,1,0,0,0]</code>.  Default: <code>[0,0,0,0,0,0]</code></p>"},{"location":"namelists/main/atmosphere/","title":"Atmosphere","text":"<p>The <code>atmosphere_nml</code>  namelist contains the following options:</p>"},{"location":"namelists/main/atmosphere/#options","title":"Options","text":""},{"location":"namelists/main/atmosphere/#idealized_moist_model","title":"<code>idealized_moist_model</code>","text":"<p>bool If <code>False</code>, the <code>hs_forcing_nml</code> namelist needs to be specified to configure the Newtonian cooling thermal relaxation profile (simple and dry alternative to moist physics). If <code>True</code>, the <code>idealized_moist_phys_nml</code> namelist needs to  be specified. Default: <code>False</code></p>"},{"location":"namelists/main/experiment_details/","title":"Experiment details","text":"<p>The <code>experiment_details</code> namelist contains information on how to run the simulation, most of which is  relevant for submitting jobs to Slurm. It is an addition I made to the namelists in the Isca source code, and  all options must be specified to run the simulation using <code>isca_tools</code>.</p>"},{"location":"namelists/main/experiment_details/#options","title":"Options","text":""},{"location":"namelists/main/experiment_details/#name","title":"<code>name</code>","text":"<p>string Name of experiment e.g. data saved in folder <code>$GFDL_DATA/{name}</code>. You can use <code>name='exp/run1'</code> to set create a new <code>exp</code> directory and then save the <code>run1</code> data within it.  This may be useful for running similar experiments with different parameters. Default: N/A</p>"},{"location":"namelists/main/experiment_details/#input_dir","title":"<code>input_dir</code>","text":"<p>string Directory containing any input files required for the experiment such as <code>namelist.nml</code>, <code>diag_table</code> or  <code>co2.nc</code>. Default: N/A</p>"},{"location":"namelists/main/experiment_details/#n_months_total","title":"<code>n_months_total</code>","text":"<p>int Total duration of simulation in months. Default: N/A</p>"},{"location":"namelists/main/experiment_details/#n_months_job","title":"<code>n_months_job</code>","text":"<p>int Approximate duration of each job of the simulation in months. E.g. if <code>n_months_total=12</code> and <code>n_months_job=6</code>, the experiment would be split up into 2 jobs each of length 6 months. Default: N/A</p>"},{"location":"namelists/main/experiment_details/#n_nodes","title":"<code>n_nodes</code>","text":"<p>int Number of nodes to run job on (Slurm info). Default: N/A</p>"},{"location":"namelists/main/experiment_details/#n_cores","title":"<code>n_cores</code>","text":"<p>int Number of cores for each node to run job on (Slurm info). Must be a power of \\(2\\) so typically \\(8\\), \\(16\\) or \\(32\\). Default: N/A</p>"},{"location":"namelists/main/experiment_details/#resolution","title":"<code>resolution</code>","text":"<p>string Horizontal resolution of experiment. Options are <code>T21</code>, <code>T42</code> or <code>T85</code>. Default: N/A</p>"},{"location":"namelists/main/experiment_details/#partition","title":"<code>partition</code>","text":"<p>string Slurm queue to submit job to. Default: N/A</p>"},{"location":"namelists/main/experiment_details/#overwrite_data","title":"<code>overwrite_data</code>","text":"<p>bool If this is <code>True</code> and data already exists in <code>$GFDL_DATA/{name}</code>, then it will be overwritten.  If it is <code>False</code> and the data exists, an error will occur. If a previous run failed part way through, running the same experiment again with <code>overwrite_data=.false.</code> will start using the last restart file created in that previous run. Typically set to <code>False</code>.  Default: N/A</p>"},{"location":"namelists/main/experiment_details/#compile","title":"<code>compile</code>","text":"<p>bool If <code>True</code>, it will recompile the codebase before running the experiment. Typically set to <code>False</code>.  Set this to <code>True</code> if you have made any changes to the underlying source code of Isca. Default: N/A</p>"},{"location":"namelists/main/experiment_details/#max_walltime","title":"<code>max_walltime</code>","text":"<p>string Maximum time that job can run on Slurm. \\(1\\) hour would be <code>'01:00:00'</code> and \\(30\\) minutes would be <code>'30:00'</code>. Default: N/A</p>"},{"location":"namelists/main/experiment_details/#delete_restart_files","title":"<code>delete_restart_files</code>","text":"<p>bool If <code>True</code>, only the restart file for the final month will be kept. Otherwise, a restart file will be generated for every month. Default: N/A</p>"},{"location":"namelists/main/experiment_details/#nodelist","title":"<code>nodelist</code>","text":"<p>string Specify node to submit to. Options on <code>debug</code> partition on St Andrews kennedy are <code>kennedy20</code>, <code>kennedy21</code> or <code>kennedy22</code>. Number listed needs to match <code>n_nodes</code>. <code>kennedy[20-22]</code> would request the 3 nodes <code>20, 21, 22</code>. If not given, will just submit to default nodes. If multiple jobs, this will likely result in jobs being submit in the wrong order, and an error occuring. Default: N/A</p>"},{"location":"namelists/main/held_suarez/","title":"Held Suarez","text":"<p>The <code>hs_forcing_nml</code>  only ever needs to be specified if  <code>idealized_moist_model = False</code> in <code>atmosphere_nml</code>. It contains options which specify the Newtonian cooling thermal relaxation profile. Some of the most common ones are described below:</p> <p>Isca gives an  example script  indicating typical usage of the <code>hs_forcing_nml</code>.</p>"},{"location":"namelists/main/held_suarez/#options","title":"Options","text":""},{"location":"namelists/main/held_suarez/#t_zero","title":"<code>t_zero</code>","text":"<p>float Temperature at reference pressure at equator (Kelvin). Default: <code>315</code></p>"},{"location":"namelists/main/held_suarez/#t_strat","title":"<code>t_strat</code>","text":"<p>float Stratosphere temperature (Kelvin). Default: <code>200</code></p>"},{"location":"namelists/main/held_suarez/#delh","title":"<code>delh</code>","text":"<p>float Equator-pole temperature gradient (Kelvin). Default: <code>60</code></p>"},{"location":"namelists/main/held_suarez/#delv","title":"<code>delv</code>","text":"<p>float Lapse rate (Kelvin). Default: <code>60</code></p>"},{"location":"namelists/main/held_suarez/#eps","title":"<code>eps</code>","text":"<p>float Stratospheric latitudinal variation. Default: <code>0</code></p>"},{"location":"namelists/main/held_suarez/#sigma_b","title":"<code>sigma_b</code>","text":"<p>float Boundary layer friction height (\\(\\sigma =  p/p_s\\)). Default: <code>0.7</code></p>"},{"location":"namelists/main/held_suarez/#ka","title":"<code>ka</code>","text":"<p>float Constant Newtonian cooling timescale (negative sign is a flag indicating that the units are days). Default: <code>-40</code></p>"},{"location":"namelists/main/held_suarez/#ks","title":"<code>ks</code>","text":"<p>float Boundary layer dependent cooling timescale (negative sign is a flag indicating that the units are days). Default: <code>-4</code></p>"},{"location":"namelists/main/held_suarez/#kf","title":"<code>kf</code>","text":"<p>float BL momentum frictional timescalee (negative sign is a flag indicating that the units are days). Default: <code>-1</code></p>"},{"location":"namelists/main/held_suarez/#do_conserve_energy","title":"<code>do_conserve_energy</code>","text":"<p>bool Convert dissipated momentum into heat if <code>True</code>.  Default: <code>True</code></p>"},{"location":"namelists/main/held_suarez/#diagnostics","title":"Diagnostics","text":"<p>The diagnostics for  this module  can be specified using the <code>module_name</code> of <code>hs_forcing</code> in the  diagnostic table file. Some available diagnostics are given below:</p>"},{"location":"namelists/main/held_suarez/#h_trop","title":"<code>h_trop</code>","text":"<p>Height of tropopause. Dimensions: time, lat, lon Units: \\(km\\)</p>"},{"location":"namelists/main/idealized_moist_physics/","title":"Idealized Moist Physics","text":"<p>The <code>idealized_moist_phys_nml</code>  only ever needs to be specified if  <code>idealized_moist_model = True</code> in <code>atmosphere_nml</code>. It contains options which specify the various modules associated with Isca\u2019s moist physics configurations and is described on Isca's website. Isca also gives numerous example scripts  using the <code>idealized_moist_phys_nml</code> namelist. Some of the most common options are described below:</p>"},{"location":"namelists/main/idealized_moist_physics/#options","title":"Options","text":""},{"location":"namelists/main/idealized_moist_physics/#convection_scheme","title":"<code>convection_scheme</code>","text":"<p>string There are 4 choices of convection schemes, as well as no convection, in Isca:</p> <ul> <li><code>SIMPLE_BETTS_CONV</code> - Use Frierson Quasi-Equilibrium convection scheme  [Frierson2007]. If this is selected, the <code>lscale_cond_nml</code> and <code>qe_moist_convection_nml</code>  namelists needs to be specified.</li> <li><code>FULL_BETTS_MILLER_CONV</code> - Use the Betts-Miller convection scheme  [Betts1986], [BettsMiller1986].  If this is selected, the <code>lscale_cond_nml</code> and <code>betts_miller_nml</code> namelists needs to be specified.</li> <li><code>RAS</code> - Use the relaxed Arakawa Schubert convection scheme  [Moorthi1992].  If this is selected, the <code>lscale_cond_nml</code> and <code>ras_nml</code> namelists needs to be specified.</li> <li><code>DRY</code> - Use the dry convection scheme  [Schneider2006].  This module  needs the <code>dry_convection_nml</code> namelist specified.</li> <li><code>NONE</code> - Use no convection scheme.</li> <li><code>UNSET</code> - Will raise an error.</li> </ul> <p>Default: <code>UNSET</code></p>"},{"location":"namelists/main/idealized_moist_physics/#do_simple","title":"<code>do_simple</code>","text":"<p>bool If <code>True</code>, use a simplification when calculating relative humidity.  Default: <code>False</code></p>"},{"location":"namelists/main/idealized_moist_physics/#do_damping","title":"<code>do_damping</code>","text":"<p>bool If <code>True</code>, the  <code>damping_driver_nml</code> namelist needs to be specified.  Default: <code>False</code> </p>"},{"location":"namelists/main/idealized_moist_physics/#radiation","title":"Radiation","text":""},{"location":"namelists/main/idealized_moist_physics/#two_stream_gray","title":"<code>two_stream_gray</code>","text":"<p>bool This is one of three choices for radiation, the others being <code>do_rrtm_radiation</code> and  <code>do_socrates_radiation</code>. If <code>True</code>, the  <code>two_stream_gray_nml</code> namelist needs to be specified.  Default: <code>True</code></p>"},{"location":"namelists/main/idealized_moist_physics/#do_rrtm_radiation","title":"<code>do_rrtm_radiation</code>","text":"<p>bool This is one of three choices for radiation, the others being <code>two_stream_gray</code> and  <code>do_socrates_radiation</code>. If <code>True</code>, the  <code>rrtm_radiation_nml</code> namelist needs to be specified.  Default: <code>False</code></p>"},{"location":"namelists/main/idealized_moist_physics/#do_socrates_radiation","title":"<code>do_socrates_radiation</code>","text":"<p>bool This is one of three choices for radiation, the others being <code>two_stream_gray</code> and  <code>do_rrtm_radiation</code>. If <code>True</code>, the  <code>socrates_rad_nml</code> namelist needs to be specified.  Default: <code>False</code> </p>"},{"location":"namelists/main/idealized_moist_physics/#turbulence","title":"Turbulence","text":"<p>These options are all related to how Isca computes surface exchange of heat, momentum and humidity.</p>"},{"location":"namelists/main/idealized_moist_physics/#turb","title":"<code>turb</code>","text":"<p>bool If <code>True</code>, vertical diffusion is enabled and the <code>vert_turb_driver_nml</code>  namelist needs to be specified.  Default: <code>False</code></p>"},{"location":"namelists/main/idealized_moist_physics/#do_virtual","title":"<code>do_virtual</code>","text":"<p>bool If <code>True</code>, the virtual temperature is used in the  vertical diffusion module.  Default: <code>False</code></p>"},{"location":"namelists/main/idealized_moist_physics/#roughness_moist","title":"<code>roughness_moist</code>","text":"<p>float Roughness length for use in surface moisture exchange. Default: <code>0.05</code></p>"},{"location":"namelists/main/idealized_moist_physics/#roughness_mom","title":"<code>roughness_mom</code>","text":"<p>float Roughness length for use in surface momentum exchange. Default: <code>0.05</code></p>"},{"location":"namelists/main/idealized_moist_physics/#roughness_heat","title":"<code>roughness_heat</code>","text":"<p>float Roughness length for use in surface heat exchange. Default: <code>0.05</code> </p>"},{"location":"namelists/main/idealized_moist_physics/#land-and-hydrology","title":"Land and Hydrology","text":"<p>Land and hydrology processes are predominantly dealt with in the <code>surface_flux_nml</code> and  the <code>mixed_layer_nml</code> namelists, but land and bucket hydrology options are initialised  with the following namelist parameters.</p> <p>Land is implemented in Isca  via adjustment of the roughness length (larger over land),  evaporative flux (smaller over land),  albedo (larger over land) and  mixed layer depth/ heat capacity (smaller over land).</p> <p>Isca give an  example script using land.</p> <p>The file indicated by <code>land_file_name</code> is generated using the <code>write_land</code> function  within <code>isca_tools</code>.</p>"},{"location":"namelists/main/idealized_moist_physics/#mixed_layer_bc","title":"<code>mixed_layer_bc</code>","text":"<p>bool If <code>True</code>, the <code>mixed_layer</code> module is called and the <code>mixed_layer_nml</code> namelist needs to be specified.  Default: <code>False</code></p>"},{"location":"namelists/main/idealized_moist_physics/#land_option","title":"<code>land_option</code>","text":"<p>string There are 3 choices of the land mask in Isca:</p> <ul> <li><code>input</code> - Read land mask from input file.</li> <li><code>zsurf</code> - Define land where surface geopotential height at model initialisation exceeds a threshold of 10.</li> <li><code>none</code> - Do not apply land mask.</li> </ul> <p>This should be set to the same value as <code>land_option</code> in the  <code>mixed_layer_nml</code> namelist.</p> <p>Default: <code>none</code></p>"},{"location":"namelists/main/idealized_moist_physics/#land_file_name","title":"<code>land_file_name</code>","text":"<p>string Filename for the input land-mask.Only ever required if <code>land_option = 'input'</code>. If the file is called <code>land.nc</code> and is in the <code>input_dir</code> then <code>land_file_name</code> should be <code>INPUT/land.nc</code>. Default: <code>'INPUT/land.nc'</code></p>"},{"location":"namelists/main/idealized_moist_physics/#land_field_name","title":"<code>land_field_name</code>","text":"<p>string Field name in the input land-mask netcdf.Only ever required if <code>land_option = 'input'</code>. Default: <code>land_mask</code></p>"},{"location":"namelists/main/idealized_moist_physics/#land_roughness_prefactor","title":"<code>land_roughness_prefactor</code>","text":"<p>float Multiplier on the roughness lengths to allow land-ocean contrast. Expect this to be greater than <code>1</code> because land is rougher than ocean.  Only ever required if <code>land_option</code> is not <code>none</code>. Default: <code>1.0</code></p>"},{"location":"namelists/main/idealized_moist_physics/#bucket","title":"<code>bucket</code>","text":"<p>bool If <code>True</code>, use bucket hydrology.  Isca give an  example script  using bucket hydrology. Default: <code>False</code></p>"},{"location":"namelists/main/idealized_moist_physics/#init_bucket_depth","title":"<code>init_bucket_depth</code>","text":"<p>float Value at which to initialise bucket water depth over ocean (in \\(m\\)). Should be large.  Only ever required if <code>bucket = .true.</code>. Default: <code>1000</code></p>"},{"location":"namelists/main/idealized_moist_physics/#init_bucket_depth_land","title":"<code>init_bucket_depth_land</code>","text":"<p>float Value at which to initialise bucket water depth over land (in \\(m\\)).  Only ever required if <code>bucket = .true.</code>. Default: <code>20</code></p>"},{"location":"namelists/main/idealized_moist_physics/#max_bucket_depth_land","title":"<code>max_bucket_depth_land</code>","text":"<p>float Maximum depth of water in bucket over land following initialisation..  Only ever required if <code>bucket = .true.</code>. Default: <code>0.15</code></p>"},{"location":"namelists/main/idealized_moist_physics/#robert_bucket","title":"<code>robert_bucket</code>","text":"<p>float Robert coefficient for Roberts-Asselin-Williams filter  on bucket leapfrog timestepping.  Only ever required if <code>bucket = .true.</code>. Default: <code>0.04</code></p>"},{"location":"namelists/main/idealized_moist_physics/#raw_bucket","title":"<code>raw_bucket</code>","text":"<p>float RAW coefficient for Roberts-Asselin-Williams filter  on bucket leapfrog timestepping.  Only ever required if <code>bucket = .true.</code>. Default: <code>0.53</code></p>"},{"location":"namelists/main/idealized_moist_physics/#diagnostics","title":"Diagnostics","text":"<p>The diagnostics for  this module  can be specified using the <code>module_name</code> of <code>atmosphere</code> in the  diagnostic table file. The list of available diagnostics is available on  Isca's website.  Some are also given below:</p>"},{"location":"namelists/main/idealized_moist_physics/#rh","title":"<code>rh</code>","text":"<p>Relative humidity. Dimensions: time, lat, lon, pressure Units: \\(\\%\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#precipitation","title":"<code>precipitation</code>","text":"<p>Rain and Snow from resolved and parameterised condensation/convection. Dimensions: time, lat, lon Units: \\(kgm^{-2}s^{-1}\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#condensation","title":"Condensation","text":""},{"location":"namelists/main/idealized_moist_physics/#condensation_rain","title":"<code>condensation_rain</code>","text":"<p>Rain from condensation. Dimensions: time, lat, lon Units: \\(kgm^{-2}s^{-1}\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#dt_qg_condensation","title":"<code>dt_qg_condensation</code>","text":"<p>Moisture tendency from condensation. Dimensions: time, lat, lon, pressure Units: \\(kgkg^{-1}s^{-1}\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#dt_tg_condensation","title":"<code>dt_tg_condensation</code>","text":"<p>Temperature tendency from condensation. Dimensions: time, lat, lon, pressure Units: \\(Ks^{-1}\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#convection","title":"Convection","text":""},{"location":"namelists/main/idealized_moist_physics/#convection_rain","title":"<code>convection_rain</code>","text":"<p>Convective precipitation. Dimensions: time, lat, lon Units: \\(kgm^{-2}s^{-1}\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#dt_qg_convection","title":"<code>dt_qg_convection</code>","text":"<p>Moisture tendency from convection. Dimensions: time, lat, lon, pressure Units: \\(kgkg^{-1}s^{-1}\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#dt_tg_convection","title":"<code>dt_tg_convection</code>","text":"<p>Temperature tendency from convection. Dimensions: time, lat, lon, pressure Units: \\(Ks^{-1}\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#cape","title":"<code>cape</code>","text":"<p>Convective Available Potential Energy. Dimensions: time, lat, lon Units: \\(Jkg^{-1}\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#cin","title":"<code>cin</code>","text":"<p>Convective Inhibition. Dimensions: time, lat, lon Units: \\(Jkg^{-1}\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#near-surface-variables","title":"Near Surface Variables","text":""},{"location":"namelists/main/idealized_moist_physics/#temp_2m","title":"<code>temp_2m</code>","text":"<p>Air temperature \\(2m\\) above surface. Dimensions: time, lat, lon Units: \\(K\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#sphum_2m","title":"<code>sphum_2m</code>","text":"<p>Specific humidity \\(2m\\) above surface. Dimensions: time, lat, lon Units: \\(kg/kg\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#rh_2m","title":"<code>rh_2m</code>","text":"<p>Relative humidity \\(2m\\) above surface. Dimensions: time, lat, lon Units: \\(\\%\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#u_10m","title":"<code>u_10m</code>","text":"<p>Zonal wind \\(10m\\) above surface. Dimensions: time, lat, lon Units: \\(ms^{-1}\\)</p>"},{"location":"namelists/main/idealized_moist_physics/#v_10m","title":"<code>v_10m</code>","text":"<p>Meridional wind \\(10m\\) above surface. Dimensions: time, lat, lon Units: \\(ms^{-1}\\)</p>"},{"location":"namelists/main/spectral_dynamics/","title":"Spectral Dynamics","text":"<p>The <code>spectral_dynamics_nml</code>  contains options relating to Isca's spectral dynamical core.  The Frierson example script uses this namelist and some of the diagnostics. Some of the most common options are described below:</p>"},{"location":"namelists/main/spectral_dynamics/#options","title":"Options","text":""},{"location":"namelists/main/spectral_dynamics/#num_levels","title":"<code>num_levels</code>","text":"<p>integer The number of pressure coordinates i.e. number of vertical coordinates.  Default: <code>18</code></p>"},{"location":"namelists/main/spectral_dynamics/#vert_coord_option","title":"<code>vert_coord_option</code>","text":"<p>string How to specify the vertical coordinates, there are two options: </p> <ul> <li><code>even_sigma</code>: Levels are equally separated in \\(\\sigma\\) such that there are <code>num_levels</code> levels. </li> <li><code>uneven_sigma</code>: Not really sure what this does but it is an option.</li> <li> <p><code>input</code>: Each coordinate is explicitly specified using the  <code>vert_coordinate_nml</code>  namelist. </p> <code>vert_coordinate_nml</code> <p>In this case, the namelist   <code>vert_coordinate_nml</code>  needs to be specified through two options:</p> <ul> <li><code>bk</code> - The \\(\\sigma\\) coordinates. There should be <code>num_levels+1</code> of these including \\(0\\) and \\(1\\).</li> <li><code>pk</code> - The corresponding pressure coordinates. Again, there should be <code>num_levels+1</code> of these.  If they are all set to \\(0\\), they will be computed automatically.</li> </ul> <p>For an example using this , see the the   <code>frierson_test_case</code>.</p> </li> </ul> <p>Default: <code>even_sigma</code></p>"},{"location":"namelists/main/spectral_dynamics/#diagnostics","title":"Diagnostics","text":"<p>The diagnostics for  this module  can be specified using the <code>module_name</code> of <code>dynamics</code> in the  diagnostic table file. Some available diagnostics are listed  on  Isca's website. Some  of the more common ones are also given below:</p>"},{"location":"namelists/main/spectral_dynamics/#ps","title":"<code>ps</code>","text":"<p>Surface pressure. Dimensions: time, lat, lon Units: \\(Pa\\)</p>"},{"location":"namelists/main/spectral_dynamics/#bk","title":"<code>bk</code>","text":"<p>Vertical coordinate \\(\\sigma\\) values. Dimensions: pressure Units: \\(Pa\\)</p>"},{"location":"namelists/main/spectral_dynamics/#pk","title":"<code>pk</code>","text":"<p>Vertical coordinate pressure values. Dimensions: pressure Units: \\(Pa\\)</p>"},{"location":"namelists/main/spectral_dynamics/#sphum","title":"<code>sphum</code>","text":"<p>Specific humidity. Dimensions: time, lat, lon, pressure Units: \\(kg/kg\\)</p>"},{"location":"namelists/main/spectral_dynamics/#temp","title":"<code>temp</code>","text":"<p>Temperature. Dimensions: time, lat, lon, pressure Units: \\(K\\)</p>"},{"location":"namelists/main/spectral_dynamics/#ucomp","title":"<code>ucomp</code>","text":"<p>Zonal component of the horizontal winds. Dimensions: time, lat, lon, pressure Units: \\(ms^{-1}\\)</p>"},{"location":"namelists/main/spectral_dynamics/#vcomp","title":"<code>vcomp</code>","text":"<p>Meridional component of the horizontal winds. Dimensions: time, lat, lon, pressure Units: \\(ms^{-1}\\)</p>"},{"location":"namelists/main/spectral_dynamics/#omega","title":"<code>omega</code>","text":"<p>Vertical velocity. Dimensions: time, lat, lon, pressure Units: \\(Pas^{-1}\\)</p>"},{"location":"namelists/main/spectral_dynamics/#vor","title":"<code>vor</code>","text":"<p>Vorticity. Dimensions: time, lat, lon, pressure Units: \\(s^{-1}\\)</p>"},{"location":"namelists/main/spectral_dynamics/#div","title":"<code>div</code>","text":"<p>Divergence. Dimensions: time, lat, lon, pressure Units: \\(s^{-1}\\)</p>"},{"location":"namelists/main/spectral_dynamics/#height","title":"<code>height</code>","text":"<p>Geopotential height at full model levels. Dimensions: time, lat, lon, pressure Units: \\(m\\)</p>"},{"location":"namelists/radiation/rrtm/","title":"RRTM Radiation","text":"<p>The <code>rrtm_radiation_nml</code>  only ever needs to be specified if  <code>do_rrtm_radiation = .true.</code> in  <code>idealized_moist_phys_nml</code>. If this is the case, then the Rapid Radiative Transfer Model will be the radiation scheme that is used. Isca gives an  example script using this radiation scheme. Some of the most common options for configuring this radiation scheme are described below:</p>"},{"location":"namelists/radiation/rrtm/#options","title":"Options","text":""},{"location":"namelists/radiation/rrtm/#co2ppmv","title":"<code>co2ppmv</code>","text":"<p>float Concentration of \\(CO_2\\) (ppmv). Default: <code>300</code></p>"},{"location":"namelists/radiation/rrtm/#diagnostics","title":"Diagnostics","text":"<p>The diagnostics for  this module  can be specified using the <code>module_name</code> of <code>rrtm_radiation</code> in the  diagnostic table file. Some available diagnostics are given below:</p>"},{"location":"namelists/radiation/rrtm/#olr","title":"<code>olr</code>","text":"<p>Top of atmosphere longwave flux (up is positive). Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/rrtm/#toa_sw","title":"<code>toa_sw</code>","text":"<p>Top of atmosphere net shortwave flux (down is positive). Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/rrtm/#flux_lw","title":"<code>flux_lw</code>","text":"<p>Longwave flux at the surface (down only). Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/rrtm/#flux_sw","title":"<code>flux_sw</code>","text":"<p>Net shortwave flux at the surface (down is positive). Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/socrates/","title":"SOCRATES","text":"<p>The <code>socrates_rad_nml</code>  only ever needs to be specified if  <code>do_socrates_radiation = .true.</code> in  <code>idealized_moist_phys_nml</code>. If this is the case, then SOCRATES will be the radiation scheme that is used.  This is described on Isca's website.  Isca also includes an  example script. Some of the most common options are described below:</p>"},{"location":"namelists/radiation/socrates/#options","title":"Options","text":""},{"location":"namelists/radiation/socrates/#inc_co2","title":"<code>inc_co2</code>","text":"<p>bool Includes radiative effects of \\(CO_2\\) if <code>True</code>. Default: <code>True</code></p>"},{"location":"namelists/radiation/socrates/#diagnostics","title":"Diagnostics","text":"<p>The diagnostics for  this module  can be specified using the <code>module_name</code> of <code>socrates</code> in the  diagnostic table file. The list of available diagnostics is available on  Isca's website.  Some are also given below:</p>"},{"location":"namelists/radiation/socrates/#soc_olr","title":"<code>soc_olr</code>","text":"<p>Top of atmosphere longwave flux (up is positive). Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/socrates/#soc_toa_sw","title":"<code>soc_toa_sw</code>","text":"<p>Top of atmosphere net shortwave flux (down is positive). Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/socrates/#soc_surf_flux_lw","title":"<code>soc_surf_flux_lw</code>","text":"<p>Net longwave flux at the surface (up is positive). Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/socrates/#soc_surf_flux_sw","title":"<code>soc_surf_flux_sw</code>","text":"<p>Net shortwave flux at the surface (down is positive). Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/two_stream_gray/","title":"Gray Radiation","text":"<p>The <code>two_stream_gray_rad_nml</code>  namelist only ever needs to be specified if  <code>two_stream_gray = .true.</code> in  <code>idealized_moist_phys_nml</code>. It contains options which specify the configuration to use to solve the two stream radiative transfer equations, as well as configuring the incoming solar radiation. It is described on Isca's website and is used in the  Frierson example script.   Some of the most common options are described below:</p>"},{"location":"namelists/radiation/two_stream_gray/#options","title":"Options","text":""},{"location":"namelists/radiation/two_stream_gray/#rad_scheme","title":"<code>rad_scheme</code>","text":"<p>string There are 4 choices of configuration for solving the two stream radiative transfer equations in Isca:</p> <ul> <li><code>FRIERSON</code> - Semi-gray  scheme with prescribed longwave and shortwave optical depths.  Changing the \\(CO_2\\) concentration does not affect this scheme.</li> <li> <p><code>BYRNE</code> - Semi-gray scheme  with longwave optical depth dependent on water vapour content and \\(CO_2\\) concentration.  Shortwave optical depth is prescribed.</p> Convergence with Frierson example script <p>Just running the  Frierson example script but changing the <code>rad_scheme</code> from <code>frierson</code> to <code>byrne</code> did not converge for me - temperature kept rising. To make it converge, I had to increase the <code>albedo_value</code>  from \\(0.31\\) to \\(0.38\\). The later being the value used in the only  example script  using <code>byrne</code> radiation.</p> </li> <li> <p><code>GEEN</code> - Multi-band scheme with  two longwave bands and one shortwave band. One longwave band corresponds to an infrared window region (\\(8-14\\mu m\\))  and the second corresponds to all other infrared wavelengths (\\(&gt;4\\mu m\\)).  Longwave and shortwave optical depths depend on water vapour content and  concentration.</p> </li> <li><code>SCHNEIDER</code> - Semi-gray  scheme for use in giant planet simulations. Longwave and shortwave optical depths are prescribed.  Does not require a surface temperature as input, and allows specification of an interior heat flux.  Changing the \\(CO_2\\) concentration does not affect this scheme.</li> </ul> Reference Pressure, \\(P_0\\) <p>A reference pressure, \\(P_0\\), is used in the <code>FRIERSON</code>/<code>BYRNE</code>/<code>SCHNEIDER</code> shortwave optical depth, as well as in  the <code>FRIERSON</code>/<code>SCHNEIDER</code> longwave optical depth. The value of this is set to <code>pstd_mks</code> in the  <code>constants_nml</code> namelist. This has a default value of \\(10^5 Pa\\) i.e. surface pressure on Earth.</p> <p>Default: <code>FRIERSON</code> </p>"},{"location":"namelists/radiation/two_stream_gray/#longwave-radiation","title":"Longwave Radiation","text":""},{"location":"namelists/radiation/two_stream_gray/#odp","title":"<code>odp</code>","text":"<p>float Frierson longwave optical depth scaling parameter, \\(\\kappa\\). I.e. larger <code>opd</code> is used as a proxy for more \\(CO_2\\) in the <code>FRIERSON</code> radiation scheme. Only ever required if <code>rad_scheme = FRIERSON</code>. Default: <code>1.0</code></p>"},{"location":"namelists/radiation/two_stream_gray/#incoming-solar-radiation","title":"Incoming Solar Radiation","text":"<p>There is a specific section on  Isca's website that explains this.</p>"},{"location":"namelists/radiation/two_stream_gray/#do_seasonal","title":"<code>do_seasonal</code>","text":"<p>bool</p> <ul> <li> <p><code>False</code>: A diurnally and seasonally averaged insolation is selected. Incoming solar radiation takes the form:  $$ S = \\frac{S_{0}}{4}[1+\\Delta_{S}P_{2}(\\theta)+\\Delta_{\\text{sw}}\\sin\\theta] $$</p> <ul> <li>\\(P_{2} = (1 - 3\\sin^{2}\\theta)/4\\) is the second legendre polynomial.</li> <li>\\(S_0\\) is the <code>solar_constant</code>.</li> <li>\\(\\Delta_s\\) is <code>del_sol</code>.</li> <li>\\(\\Delta_{sw}\\) is <code>del_sw</code>.</li> </ul> Schneider Insolation Profile <p>If <code>rad_scheme</code> is <code>SCHNEIDER</code>, then the insolation with <code>do_seasonal = False</code> is: $$ S = \\frac{S_{0}}{\\pi}\\cos\\theta  $$</p> </li> <li> <p><code>True</code>: The time dependent insolation has the form: $$ S = S_{0}\\cos\\zeta\\left(\\frac{a}{r}\\right)^{2} $$</p> <ul> <li>\\(\\zeta\\) is the zenith angle.</li> <li>\\(a\\) is the semi-major axis of the orbital ellipse.</li> <li>\\(r\\) is the time-varying planet-star distance.</li> </ul> </li> </ul> <p>Default: <code>False</code></p>"},{"location":"namelists/radiation/two_stream_gray/#solar_constant","title":"<code>solar_constant</code>","text":"<p>float The solar constant, \\(S_0\\), in the insolation equation (\\(Wm^{-2}\\)). Default: <code>1360.0</code></p>"},{"location":"namelists/radiation/two_stream_gray/#del_sol","title":"<code>del_sol</code>","text":"<p>float Parameter, \\(\\Delta_s\\), in the insolation equation. It sets the amplitude of the \\(P_2\\) insolation profile between the equator and the pole. Only ever required if <code>do_seasonal = .false.</code> and <code>rad_scheme</code> is not <code>SCHNEIDER</code>. Default: <code>1.4</code></p>"},{"location":"namelists/radiation/two_stream_gray/#del_sw","title":"<code>del_sw</code>","text":"<p>float Parameter, \\(\\Delta_{sw}\\), in the insolation equation. It defines the magnitude of \\(\\sin \\theta\\) modification to the \\(P_2\\) insolation profile. Only ever required if <code>do_seasonal = .false.</code> and <code>rad_scheme</code> is not <code>SCHNEIDER</code>. Default: <code>0.0</code></p>"},{"location":"namelists/radiation/two_stream_gray/#use_time_average_coszen","title":"<code>use_time_average_coszen</code>","text":"<p>bool If <code>True</code>, average \\(\\cos\\zeta\\) over the period <code>dt_rad_avg</code>.  For example, for the Earth's diurnal period, <code>use_time_average_coszen=True</code> and <code>dt_rad_avg=86400.</code>  would achieve diurnally averaged insolation.  Only ever required if <code>do_seasonal = .true.</code>. Default: <code>False</code></p>"},{"location":"namelists/radiation/two_stream_gray/#dt_rad_avg","title":"<code>dt_rad_avg</code>","text":"<p>float Averaging period (seconds) for time-dependent insolation \\(\\Delta t_{\\text{avg}}\\).  If equal to <code>-1</code>, it sets averaging period to model timestep.  Only ever required if <code>do_seasonal = .true.</code>. Default: <code>-1</code></p>"},{"location":"namelists/radiation/two_stream_gray/#solday","title":"<code>solday</code>","text":"<p>integer Day of year to run time-dependent insolation perpetually.  If negative, the option to run perpetually on a specific day is not used.  Only ever required if <code>do_seasonal = .true.</code>. Default: <code>-10</code></p>"},{"location":"namelists/radiation/two_stream_gray/#equinox_day","title":"<code>equinox_day</code>","text":"<p>float Fraction of year (between \\(0\\) and \\(1\\)) where Northern Hemisphere autumn equinox occurs. A value of <code>0.75</code> would mean the end of September for 360 day year. Only ever required if <code>do_seasonal = .true.</code>. Default: <code>0.75</code> </p>"},{"location":"namelists/radiation/two_stream_gray/#co_2","title":"\\(CO_2\\)","text":"<p>Isca give an  example script using varying \\(CO_2\\) concentration.</p>"},{"location":"namelists/radiation/two_stream_gray/#do_read_co2","title":"<code>do_read_co2</code>","text":"<p>bool If <code>True</code>, reads time-varying \\(CO_2\\) concentration from an input file.   The input file needs to be 4D (3 spatial dimensions and time), but no spatial variation should be defined  (the code only reads in maximum value at a given time). </p> Compatible <code>rad_schemes</code> <p>Varying \\(CO_2\\) concentration can only be using if <code>rad_scheme</code> is <code>byrne</code> or <code>geen</code>.</p> <p>Default: <code>False</code></p>"},{"location":"namelists/radiation/two_stream_gray/#co2_file","title":"<code>co2_file</code>","text":"<p>string Name of \\(CO_2\\) file to read.  The file should be in the <code>input_dir</code> and have a .nc appendix but that should be left out here.  File is produced using the <code>create_time_series_file</code> function in <code>isca_tools</code> which is extended from a  python script  provided by Isca. Only ever required if <code>do_read_co2 = .true.</code>. Default: <code>co2</code></p>"},{"location":"namelists/radiation/two_stream_gray/#co2_variable_name","title":"<code>co2_variable_name</code>","text":"<p>string Name of \\(CO_2\\) variable in \\(CO_2\\) file. Only ever required if <code>do_read_co2 = .true.</code>. Default: <code>co2</code></p>"},{"location":"namelists/radiation/two_stream_gray/#carbon_conc","title":"<code>carbon_conc</code>","text":"<p>float Prescribed concentration (in \\(ppmv\\)) of \\(CO_2\\) which remains constant throughout the simulation. Only ever required if <code>do_read_co2 = .false.</code> and <code>rad_scheme</code> is either  <code>byrne</code> or <code>geen</code>. For other <code>rad_schemes</code>, optical depth is prescribed so \\(CO_2\\) concentration has no effect. Default: <code>360.0</code></p>"},{"location":"namelists/radiation/two_stream_gray/#diagnostics","title":"Diagnostics","text":"<p>The diagnostics for  this module  can be specified using the <code>module_name</code> of <code>two_stream</code> in the  diagnostic table file. The list of available diagnostics is available on  Isca's website. Some  of the more common ones are also given below.</p>"},{"location":"namelists/radiation/two_stream_gray/#co2","title":"<code>co2</code>","text":"<p>Carbon dioxide concentration. Dimensions: time Units: \\(ppmv\\)</p>"},{"location":"namelists/radiation/two_stream_gray/#radiation","title":"Radiation","text":""},{"location":"namelists/radiation/two_stream_gray/#olr","title":"<code>olr</code>","text":"<p>Outgoing Longwave radiation. May be useful, along with <code>swdn_toa</code> to investigate how long experiment takes  to spin up. Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/two_stream_gray/#swdn_toa","title":"<code>swdn_toa</code>","text":"<p>Shortwave flux down at top of atmosphere. May be useful, along with <code>olr</code> to investigate how long  experiment takes to spin up. Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/two_stream_gray/#swdn_sfc","title":"<code>swdn_sfc</code>","text":"<p>Absorbed shortwave flux at the surface. Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/two_stream_gray/#lwdn_sfc","title":"<code>lwdn_sfc</code>","text":"<p>Downward longwave flux at the surface. Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/radiation/two_stream_gray/#lwup_sfc","title":"<code>lwup_sfc</code>","text":"<p>Upward longwave flux at the surface. Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/surface/mixed_layer/","title":"Mixed Layer","text":"<p>The <code>mixed_layer_nml</code>  only ever needs to be specified if  <code>mixed_layer_bc = .true.</code> in  <code>idealized_moist_phys_nml</code>. It contains options which deal with the mixed layer boundary condition, including the difference  between ocean and land.  It is described on Isca's website and is  used in numerous  example scripts. Some of the most common options are described below:</p>"},{"location":"namelists/surface/mixed_layer/#options","title":"Options","text":""},{"location":"namelists/surface/mixed_layer/#evaporation","title":"<code>evaporation</code>","text":"<p>bool Switch for surface evaporation. Default: <code>True</code></p>"},{"location":"namelists/surface/mixed_layer/#depth","title":"<code>depth</code>","text":"<p>float Depth of mixed layer (\\(m\\)). Default: <code>40.0</code> </p>"},{"location":"namelists/surface/mixed_layer/#q-flux","title":"Q-flux","text":""},{"location":"namelists/surface/mixed_layer/#do_qflux","title":"<code>do_qflux</code>","text":"<p>bool Switch to calculate time-independent Q-flux.  Default: <code>False</code></p>"},{"location":"namelists/surface/mixed_layer/#qflux_amp","title":"<code>qflux_amp</code>","text":"<p>float Amplitude of time-independent Q-flux.   Only ever required if <code>do_qflux = .true.</code>. Default: <code>0.0</code></p>"},{"location":"namelists/surface/mixed_layer/#qflux_width","title":"<code>qflux_width</code>","text":"<p>float Width of time-independent Q-flux.   Only ever required if <code>do_qflux = .true.</code>. Default: <code>16.0</code></p>"},{"location":"namelists/surface/mixed_layer/#load_qflux","title":"<code>load_qflux</code>","text":"<p>bool Switch to use input file to load in a time-independent or time-dependent Q-flux.  Default: <code>False</code></p>"},{"location":"namelists/surface/mixed_layer/#qflux_file_name","title":"<code>qflux_file_name</code>","text":"<p>string  Name of file among input files, from which to get Q-flux.   Only ever required if <code>load_qflux = .true.</code>. Default: <code>16.0</code></p>"},{"location":"namelists/surface/mixed_layer/#time_varying_qflux","title":"<code>time_varying_qflux</code>","text":"<p>bool  Flag that determines whether input Q-flux file is time dependent.   Only ever required if <code>load_qflux = .true.</code>. Default: <code>False</code></p>"},{"location":"namelists/surface/mixed_layer/#qflux_field_name","title":"<code>qflux_field_name</code>","text":"<p>string  Name of field name in Q-flux file name, from which to get Q-flux.   Only ever required if <code>time_varying_qflux = .false.</code>, otherwise assumes  field_name=file_name. Default: <code>16.0</code> </p>"},{"location":"namelists/surface/mixed_layer/#surface-temperature","title":"Surface Temperature","text":""},{"location":"namelists/surface/mixed_layer/#prescribe_initial_dist","title":"<code>prescribe_initial_dist</code>","text":"<p>bool If <code>True</code>, an  initial surface temperature distribution is set  up which is then allowed to evolve based on the surface fluxes. Default: <code>False</code></p>"},{"location":"namelists/surface/mixed_layer/#tconst","title":"<code>tconst</code>","text":"<p>float The parameter \\(T_{surf}\\) for the initial temperature distribution which follows: \\(T_s = T_{surf} -\\frac{1}{3} dT \\left(3\\sin(\\lambda)^2-1\\right)\\) Only ever required if <code>prescribe_initial_dist = .true.</code>. Default: <code>305.0</code></p>"},{"location":"namelists/surface/mixed_layer/#delta_t","title":"<code>delta_T</code>","text":"<p>float The parameter \\(dT\\) for the initial temperature distribution which follows: \\(T_s = T_{surf} -\\frac{1}{3} dT \\left(3\\sin(\\lambda)^2-1\\right)\\) Only ever required if <code>prescribe_initial_dist = .true.</code>. Default: <code>40.0</code></p>"},{"location":"namelists/surface/mixed_layer/#do_read_sst","title":"<code>do_read_sst</code>","text":"<p>bool If <code>True</code>, surface temperatures will be prescribed from an input file and will be fixed.  Default: <code>False</code></p>"},{"location":"namelists/surface/mixed_layer/#do_sc_sst","title":"<code>do_sc_sst</code>","text":"<p>bool As far as I can tell, this is exactly the same as <code>do_read_sst</code>. Default: <code>False</code></p>"},{"location":"namelists/surface/mixed_layer/#sst_file","title":"<code>sst_file</code>","text":"<p>string Name of NetCDF file containing fixed surface temperatures.  The file can be time independent or vary with time.  Only ever required if <code>do_read_sst = .true.</code>. Default: N/A</p>"},{"location":"namelists/surface/mixed_layer/#specify_sst_over_ocean_only","title":"<code>specify_sst_over_ocean_only</code>","text":"<p>bool Flag to specify surface temperature only over ocean.  Only ever required if <code>do_read_sst = .true.</code>. Default: N/A</p>"},{"location":"namelists/surface/mixed_layer/#do_ape_sst","title":"<code>do_ape_sst</code>","text":"<p>bool If <code>True</code>, surface temperatures will be fixed and prescribed from the  APE aquaplanet equation:  \\(T_s = 27 \\left( 1 - \\sin^2\\left( \\frac{3}{2} \\lambda \\right) \\right)\\) Default: <code>False</code></p>"},{"location":"namelists/surface/mixed_layer/#add_latent_heat_flux_anom","title":"<code>add_latent_heat_flux_anom</code>","text":"<p>bool Flag to add an anomalous latent heat flux.  Default: <code>False</code></p>"},{"location":"namelists/surface/mixed_layer/#do_warmpool","title":"<code>do_warmpool</code>","text":"<p>bool Flag to call the <code>warmpool</code> routine from the  <code>qflux</code> module,  which returns <code>ocean_qflux</code>.  Default: <code>False</code> </p>"},{"location":"namelists/surface/mixed_layer/#surface-albedo","title":"Surface Albedo","text":""},{"location":"namelists/surface/mixed_layer/#albedo_choice","title":"<code>albedo_choice</code>","text":"<p>integer There are 5 choices of surface albedo, \\(\\alpha\\), which can be specified by the indices below:</p> <ol> <li>Constant\\(\\alpha\\) set to the constant indicated by <code>albedo_value</code>.</li> <li> <p>Glacier (One hemisphere)</p> <ul> <li> <p><code>lat_glacier&gt;0</code></p> <ul> <li>\\(\\lambda\\) &gt; <code>lat_glacier</code>: \\(\\alpha\\) = <code>higher_albedo</code></li> <li>\\(\\lambda\\) &lt; <code>lat_glacier</code>: \\(\\alpha\\) = <code>albedo_value</code></li> </ul> </li> <li> <p><code>lat_glacier&lt;0</code></p> <ul> <li>\\(\\lambda\\) &lt; <code>lat_glacier</code>: \\(\\alpha\\) = <code>higher_albedo</code></li> <li>\\(\\lambda\\) &gt; <code>lat_glacier</code>: \\(\\alpha\\) = <code>albedo_value</code></li> </ul> </li> </ul> </li> <li> <p>Glacier (Both hemispheres)</p> <ul> <li>\\(\\lambda\\) &gt; \\(|\\)<code>lat_glacier</code>\\(|\\): \\(\\alpha\\) = <code>higher_albedo</code></li> <li>\\(\\lambda\\) &lt; \\(|\\)<code>lat_glacier</code>\\(|\\): \\(\\alpha\\) = <code>albedo_value</code></li> </ul> </li> <li> <p>Exponent \\(\\alpha\\) = <code>albedo_value</code> + (<code>higher_albedo</code> - <code>albedo_value</code>)     \\(\\times (\\frac{\\lambda}{90})\\)^<code>albedo_exp</code></p> </li> <li>Tanh     This is an increase in \\(\\alpha\\) at latitude indicated by <code>albedo_cntr</code> with width  <code>albedo_wdth</code>.  <code>\u03b1(\u03bb) = albedo_value + (higher_albedo-albedo_value)* 0.5 *(1+tanh((\u03bb-albedo_cntr)/albedo_wdth))</code></li> </ol> <p>Default: <code>1</code></p>"},{"location":"namelists/surface/mixed_layer/#albedo_value","title":"<code>albedo_value</code>","text":"<p>float Parameter to determine the surface albedo. Required for every value of  <code>albedo_choice</code>. Default: <code>0.06</code></p>"},{"location":"namelists/surface/mixed_layer/#higher_albedo","title":"<code>higher_albedo</code>","text":"<p>float Parameter to determine the surface albedo. Only ever required if <code>albedo_choice</code> is <code>2</code>, <code>3</code>, <code>4</code> or <code>5</code>. Default: <code>0.10</code></p>"},{"location":"namelists/surface/mixed_layer/#lat_glacier","title":"<code>lat_glacier</code>","text":"<p>float Parameter that sets the glacier ice latitude for determining the surface albedo. Only ever required if <code>albedo_choice</code> is <code>2</code> or <code>3</code>. Default: <code>60.0</code></p>"},{"location":"namelists/surface/mixed_layer/#albedo_exp","title":"<code>albedo_exp</code>","text":"<p>float Parameter that sets the latitude dependence for determining the surface albedo. Only ever required if <code>albedo_choice</code> is <code>4</code>. Default: <code>2.</code></p>"},{"location":"namelists/surface/mixed_layer/#albedo_cntr","title":"<code>albedo_cntr</code>","text":"<p>float Parameter that sets the central latitude for determining the surface albedo. Only ever required if <code>albedo_choice</code> is <code>5</code>. Default: <code>45.</code></p>"},{"location":"namelists/surface/mixed_layer/#albedo_wdth","title":"<code>albedo_wdth</code>","text":"<p>float Parameter that sets the latitude width for determining the surface albedo. Only ever required if <code>albedo_choice</code> is <code>5</code>. Default: <code>10.</code> </p>"},{"location":"namelists/surface/mixed_layer/#ice","title":"Ice","text":""},{"location":"namelists/surface/mixed_layer/#update_albedo_from_ice","title":"<code>update_albedo_from_ice</code>","text":"<p>bool Flag to set the surface albedo to <code>ice_albedo_value</code>  where there is ice as specified by <code>ice_file_name</code>. Default: <code>False</code></p>"},{"location":"namelists/surface/mixed_layer/#ice_albedo_value","title":"<code>ice_albedo_value</code>","text":"<p>float Value for the ice albedo. Expect this to be much larger than <code>albedo_value</code> because ice is more reflective than ocean. Only ever required if <code>update_albedo_from_ice=.true.</code>. Default: <code>0.7</code></p>"},{"location":"namelists/surface/mixed_layer/#ice_file_name","title":"<code>ice_file_name</code>","text":"<p>string Name of file containing sea ice concentration. Only ever required if <code>update_albedo_from_ice=.true.</code>. Default: <code>siconc_clim_amip</code></p>"},{"location":"namelists/surface/mixed_layer/#ice_concentration_threshold","title":"<code>ice_concentration_threshold</code>","text":"<p>float Value of sea ice concentration above which albedo should be set to <code>ice_albedo_value</code>. Default: <code>0.5</code> </p>"},{"location":"namelists/surface/mixed_layer/#land","title":"Land","text":"<p>There are 4 ways that land is implemented in Isca.</p>"},{"location":"namelists/surface/mixed_layer/#land_option","title":"<code>land_option</code>","text":"<p>string There are 4 choices of the land mask in Isca given below. This parameter should be set to the same value as <code>land_option</code> in the  <code>idealized_moist_phys_nml</code> namelist. If it is not specified here but is  elsewhere, then none of the land parameters set in this module will be used.</p> Heat capacity calculation <p>The heat capacity calculation is different over land for the different options of <code>land_option</code>. If it is <code>input</code>, the heat capacity at a particular location is  set  to <code>land_h_capacity_prefactor</code> multiplied by the ocean heat capacity at that location. If it is either <code>zsurf</code> or <code>lonlat</code>, it is  computed  as for ocean but with a different depth.</p> <ul> <li><code>input</code> - Read land mask from input file indicated by the  <code>land_file_name</code> parameter in the  <code>idealized_moist_phys_nml</code> namelist.</li> <li><code>zsurf</code> - Define land where surface geopotential height at model initialisation exceeds a threshold of 10.</li> <li><code>lonlat</code> - Define land to be in the longitude/latitude box set by <code>slandlon[k]</code>, <code>elandlon[k]</code>, <code>slandlat[k]</code>, <code>elandlat[k]</code> for all \\(k\\).</li> <li><code>none</code> - Do not apply land mask.</li> </ul> <p>Default: <code>none</code></p>"},{"location":"namelists/surface/mixed_layer/#land_h_capacity_prefactor","title":"<code>land_h_capacity_prefactor</code>","text":"<p>float Factor by which to multiply ocean heat capacity to get land heat capacity.  Would expect this to be less than <code>1</code> as land has a smaller heat capacity than ocean.  Only ever required if <code>land_option='input'</code>. Default: <code>1.0</code></p>"},{"location":"namelists/surface/mixed_layer/#land_albedo_prefactor","title":"<code>land_albedo_prefactor</code>","text":"<p>float Factor by which to multiply ocean albedo to get land albedo.  Would expect this to be more than <code>1</code> as land is more reflective than ocean.  Only ever required if <code>land_option='input'</code>. Default: <code>1.0</code></p>"},{"location":"namelists/surface/mixed_layer/#land_depth","title":"<code>land_depth</code>","text":"<p>float Depth of land mixed layer (\\(m\\)). Only ever required if <code>land_option</code> is <code>'zsurf'</code> or <code>'lonlat'</code>. If it is  negative, it just uses the ocean <code>depth</code>. Default: <code>-1</code></p>"},{"location":"namelists/surface/mixed_layer/#slandlon","title":"<code>slandlon</code>","text":"<p>list - float <code>slandlon[k]</code> is the start longitude of land box \\(k\\). Only ever required if <code>land_option</code> is <code>'lonlat'</code>. Default: <code>0</code></p>"},{"location":"namelists/surface/mixed_layer/#slandlat","title":"<code>slandlat</code>","text":"<p>list - float <code>slandlat[k]</code> is the start latitude of land box \\(k\\). Only ever required if <code>land_option</code> is <code>'lonlat'</code>. Default: <code>0</code></p>"},{"location":"namelists/surface/mixed_layer/#elandlon","title":"<code>elandlon</code>","text":"<p>list - float <code>elandlon[k]</code> is the end longitude of land box \\(k\\). Only ever required if <code>land_option</code> is <code>'lonlat'</code>. Default: <code>-1</code></p>"},{"location":"namelists/surface/mixed_layer/#elandlat","title":"<code>elandlat</code>","text":"<p>list - float <code>elandlat[k]</code> is the end latitude of land box \\(k\\). Only ever required if <code>land_option</code> is <code>'lonlat'</code>. Default: <code>-1</code></p>"},{"location":"namelists/surface/mixed_layer/#diagnostics","title":"Diagnostics","text":"<p>The diagnostics for  this module  can be specified using the <code>module_name</code> of <code>mixed_layer</code> in the  diagnostic table file. The list of available diagnostics is available on  Isca's website.  They are also given below:</p>"},{"location":"namelists/surface/mixed_layer/#t_surf","title":"<code>t_surf</code>","text":"<p>Surface temperature. Dimensions: time, lat, lon Units: \\(K\\)</p>"},{"location":"namelists/surface/mixed_layer/#delta_t_surf","title":"<code>delta_t_surf</code>","text":"<p>Surface temperature change. Dimensions: time, lat, lon Units: \\(K\\)</p>"},{"location":"namelists/surface/mixed_layer/#flux_t","title":"<code>flux_t</code>","text":"<p>Surface sensible heat flux (up is positive). Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/surface/mixed_layer/#flux_lhe","title":"<code>flux_lhe</code>","text":"<p>Surface latent heat flux (up is positive). Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/surface/mixed_layer/#flux_oceanq","title":"<code>flux_oceanq</code>","text":"<p>Oceanic Q-flux (will be \\(0\\) if <code>do_qflux=False</code>). Dimensions: time, lat, lon Units: \\(Wm^{-2}\\)</p>"},{"location":"namelists/surface/mixed_layer/#ml_heat_cap","title":"<code>ml_heat_cap</code>","text":"<p>Mixed layer heat capacity. On the website, it calls this  <code>land_sea_heat_capacity</code> but in the  code,  I think it is <code>ml_heat_cap</code> so I am not sure which is correct. Dimensions: time, lat, lon Units: \\(Jm^{-2}K^{-1}\\)</p>"},{"location":"namelists/surface/mixed_layer/#albedo","title":"<code>albedo</code>","text":"<p>Surface albedo. I think this will remain constant in time unless <code>update_albedo_from_ice=True</code>. Dimensions: time, lat, lon Units: N/A</p>"},{"location":"namelists/surface/mixed_layer/#ice_conc","title":"<code>ice_conc</code>","text":"<p>Sea ice concentration. Can only be returned if <code>update_albedo_from_ice=True</code>. Dimensions: time, lat, lon Units: N/A</p>"},{"location":"namelists/surface/surface_flux/","title":"Surface Flux","text":"<p>The <code>surface_flux_nml</code>  only ever needs to be specified if  <code>mixed_layer_bc = .true.</code> in  <code>idealized_moist_phys_nml</code>. It contains options which deal with the exchange of heat, momentum at the surface, including the difference  between ocean and land. It is described on  Isca's website and is used in the  realistic continents example script. Some of the most common options are described below:</p>"},{"location":"namelists/surface/surface_flux/#options","title":"Options","text":""},{"location":"namelists/surface/surface_flux/#land","title":"Land","text":"<p>There are 4 ways that land is implemented in Isca.</p>"},{"location":"namelists/surface/surface_flux/#land_humidity_factor","title":"<code>land_humidity_factor</code>","text":"<p>float  Factor that multiplies the surface specific humidity over land. This is included to make land dry. If it is equal to 1, land behaves like ocean.  If it is between 0 and 1, this will decrease the evaporative heat flux in areas of land.  The evaporative flux formula is given on Isca's website.  Only ever required if <code>land_option</code> is not <code>none</code>.</p> Instability <p>Note that this can lead to sign changes in the evaporative flux,  and we find this becomes unstable over very shallow mixed layer depths.</p> <p>Default: <code>1.0</code></p>"},{"location":"namelists/surface/surface_flux/#land_evap_prefactor","title":"<code>land_evap_prefactor</code>","text":"<p>float  Factor that multiplies the evaporative flux over land. This is included to make land dry. If it is equal to 1, land behaves like ocean.  If it is between 0 and 1, this will decrease the evaporative heat flux in areas of land.  The evaporative flux formula is given on Isca's website.  Only ever required if <code>land_option</code> is not <code>none</code>.</p> Stability <p>This formulation avoids sign changes in the evaporative flux and remains stable over very  shallow mixed layer depths.</p> Using with bucket model <p>With my  adjustment  to the <code>surface_flux.F90</code> source code, you can use this prefactor when using the  bucket model.</p> <p>It acts like the vegetation prefactor, \\(C_V\\) in  pietschnig_2021.</p> <p>Default: <code>1.0</code></p>"},{"location":"namelists/surface/topography/","title":"Topography","text":"<p>The <code>spectral_init_cond_nml</code>  only ever needs to be specified if  <code>land_option</code> is <code>input</code> in  <code>idealized_moist_phys_nml</code>. It contains options which specify the topography of the land. It is described on  Isca's website and there is an example script using topography.</p> <p>The options are described below:</p>"},{"location":"namelists/surface/topography/#options","title":"Options","text":""},{"location":"namelists/surface/topography/#topography_option","title":"<code>topography_option</code>","text":"<p>string  This indicates how the topography is specified. There are 4 options:</p> <ul> <li><code>input</code> - Get topography from input file.</li> <li><code>flat</code> - Surface geopotential is 0 (not widely used).</li> <li><code>interpolated</code> - Not currently used.</li> <li><code>gaussian</code> - Simple Gaussian-shaped mountains are generated from specified parameters (not widely used).</li> </ul> <p>Default: <code>flat</code></p>"},{"location":"namelists/surface/topography/#topog_file_name","title":"<code>topog_file_name</code>","text":"<p>string  File that contains the topography information.  This should be the same as  <code>land_file_name</code> but without <code>INPUT</code> i.e. if the file is called <code>land.nc</code> and is in the <code>input_dir</code> then  <code>topog_file_name</code> should be <code>land.nc</code>. Default: <code>topography.data.nc</code></p>"},{"location":"namelists/surface/topography/#topog_field_name","title":"<code>topog_field_name</code>","text":"<p>string The height field name in the input file. Default: <code>zsurf</code></p>"},{"location":"namelists/surface/topography/#land_field_name","title":"<code>land_field_name</code>","text":"<p>string The land field name in the input file. This should be the same as  <code>land_field_name</code> in the <code>idealized_moist_phys_nml</code> namelist. Default: <code>land_mask</code></p>"},{"location":"namelists/turbulence/diffusivity/","title":"Diffusivity","text":"<p>The <code>diffusivity_nml</code>  namelist only ever needs to be specified if  <code>do_diffusivity = .true.</code> in  <code>vert_turb_nml</code>. The module computes the atmospheric diffusivities in the planetary boundary layer and in  the free atmosphere. Some of the most common options for configuring this are described below:</p>"},{"location":"namelists/turbulence/diffusivity/#options","title":"Options","text":""},{"location":"namelists/turbulence/diffusivity/#do_simple","title":"<code>do_simple</code>","text":"<p>bool If <code>True</code>, a simplified calculation is used. Default: <code>False</code></p>"},{"location":"namelists/turbulence/vert_turb_driver/","title":"Vertical Turbulence Driver","text":"<p>The <code>vert_turb_nml</code>  namelist only ever needs to be specified if  <code>turb = .true.</code> in  <code>idealized_moist_phys_nml</code>. The module computes the vertical diffusion coefficients. Some of the most common options for configuring this are described below:</p>"},{"location":"namelists/turbulence/vert_turb_driver/#options","title":"Options","text":""},{"location":"namelists/turbulence/vert_turb_driver/#do_diffusivity","title":"<code>do_diffusivity</code>","text":"<p>bool If <code>True</code>, the <code>diffusivity</code> routine in the <code>diffusivity</code> module is run and the  <code>diffusivity_nml</code> namelist needs to be specified. Default: <code>False</code></p>"},{"location":"namelists/turbulence/vert_turb_driver/#do_molecular_diffusion","title":"<code>do_molecular_diffusion</code>","text":"<p>bool If <code>True</code>, the <code>molecular_diffusion</code> routine in the <code>diffusivity</code> module is run.  <code>do_diffusivity</code> must be <code>True</code> for this variable to make any difference.  Default: <code>False</code></p>"},{"location":"namelists/turbulence/vert_turb_driver/#diagnostics","title":"Diagnostics","text":"<p>The diagnostics for  this module  can be specified using the <code>module_name</code> of <code>vert_turb</code> in the  diagnostic table file. Some available diagnostics are given below:</p>"},{"location":"namelists/turbulence/vert_turb_driver/#z_full","title":"<code>z_full</code>","text":"<p>Geopotential height relative to surface at full levels. Dimensions: time, lat, lon, pressure Units: \\(m\\)</p>"}]}