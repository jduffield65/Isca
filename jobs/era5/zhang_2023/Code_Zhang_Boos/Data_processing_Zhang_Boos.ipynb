{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d003389a-532f-42f5-add9-d4786fb2d7c0",
   "metadata": {},
   "source": [
    "## Data processing for \"An upper bound for extreme temperatures over midlatitude land\" by Zhang and Boos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661c736-b786-46c6-9d95-8bc0360330b8",
   "metadata": {},
   "source": [
    "This notebook details how data are processed when |data files in the other notebook are not raw data. This notebook doesn't run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3849f530-1869-4708-bf90-6dd828c406ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.core.options.set_options at 0x155550aef0d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "from glob import glob \n",
    "import warnings\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.util import add_cyclic_point\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"]=\"FALSE\"\n",
    "xr.set_options(display_style='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b463b20-0f97-4ba8-9727-7968d6a777a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Western North America (45N, 55N, 116W, 123W)\n",
    "lat_slice_pn = slice(55,45)\n",
    "lon_slice_pn =slice(237,244)\n",
    "\n",
    "## Russia (50N, 57N, 35E, 45E)\n",
    "lat_slice_rs = slice(57,50)\n",
    "lon_slice_rs =slice(35,45)\n",
    "\n",
    "## Western europe (45N, 53N, 3E, 13E)\n",
    "lat_slice_fr = slice(53,46)\n",
    "lon_slice_fr =slice(3,13)\n",
    "\n",
    "## Time slices of three heatwaves\n",
    "time_pn = slice('2021-06-26','2021-07-01')\n",
    "time_fr = slice('2019-07-23','2019-07-25')\n",
    "time_rs = slice('2010-07-31','2010-08-11')\n",
    "\n",
    "# Land masks\n",
    "lsm = xr.open_dataset('./e5.oper.invariant.128_172_lsm.ll025sc.1979010100_1979010100.nc').LSM.squeeze()\n",
    "land = lsm.where(lsm>0.5)*0+1\n",
    "ocean = lsm.where(lsm<=0.5)*0+1\n",
    "weights = np.cos(np.deg2rad(land.latitude))\n",
    "\n",
    "# Physical constants\n",
    "cp = 1.0047090 \n",
    "L = 2.5008e3\n",
    "ep = 0.621981\n",
    "e = 1e-6\n",
    "R = 287.058 \n",
    "g = 9.81\n",
    "\n",
    "\n",
    "\n",
    "def e_sat(t): # Clausius Clapeyron [K] [Pa]\n",
    "    return 611.21*np.exp(17.502*((t-273.16)/(t-32.19)))\n",
    "\n",
    "def alpha(t): # de_sat/dT [K] [Pa]\n",
    "    return 4217.457/(t-32.19)**2# Constants followed ECMWF\n",
    "\n",
    "def e2q(e,sp):\n",
    "    return 0.621981*e/(sp-(1-0.621981)*e)\n",
    "\n",
    "z500_mean =55.74162890625\n",
    "t500_mean = 258.82523\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa975857-dca2-474e-9c43-4737f179696e",
   "metadata": {},
   "source": [
    "------------\n",
    "### Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7ca44-34f8-48a5-b63e-5decc250766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t500 = xr.open_mfdataset('/global/cscratch1/sd/y-zhang/ERA5/t500_*.nc').t500me # Daily mean T500 from ERA5 1979-2021\n",
    "t500_clim = t500.sel(time=slice('1979','1998')).groupby('time.dayofyear').mean('time').compute() # Taking the average of the first 20 years as climatology\n",
    "t500_anom = t500.groupby('time.dayofyear')-t500_clim\n",
    "\n",
    "TX = xr.open_mfdataset('/global/cscratch1/sd/y-zhang/ERA5/TX_*.nc').TX # Daily maximum 2-m temperature from ERA5 1979-2021\n",
    "TX_clim = TX.sel(time=slice('1979','1998')).groupby('time.dayofyear').mean('time').compute() # Taking the average of the first 20 years as climatology\n",
    "TX_anom = TX.groupby('time.dayofyear')-TX_clim\n",
    "\n",
    "\n",
    "t500_pn_anom = t500_anom.sel(time=time_pn).mean('time').compute()\n",
    "t500_fr_anom = t500_anom.sel(time=time_fr).mean('time').compute()\n",
    "t500_rs_anom = t500_anom.sel(time=time_rs).mean('time').compute()\n",
    "\n",
    "TX_pn_anom = TX_anom.sel(time=time_pn).mean('time').compute()\n",
    "TX_fr_anom = TX_anom.sel(time=time_fr).mean('time').compute()\n",
    "TX_rs_anom = TX_anom.sel(time=time_rs).mean('time').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d3656-5b6b-46e9-995f-90e928a62a9e",
   "metadata": {},
   "source": [
    "------------\n",
    "### Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f177b-5691-4491-a355-32bd6a8e28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TX = xr.open_dataset('/global/cscratch1/sd/y-zhang/ERA5/TX_2010.nc').TX.sel(latitude=slice(65,40)) # Daily maximum temperature from ERA5\n",
    "t2m = xr.open_dataset('/global/cscratch1/sd/y-zhang/ERA5/2m_temperature_2010.nc').t2m.sel(latitude=slice(65,40)) # hourly 2-m temperature from ERA5\n",
    "pr = xr.open_dataset('/global/cscratch1/sd/y-zhang/ERA5/pr_GPM_2010.nc').rename({'lat':'latitude', 'lon':'longitude'}).sel(latitude=slice(65,40)).pr    # GPM daily mean precipitation\n",
    "cape = xr.open_dataset('/global/cscratch1/sd/y-zhang/ERA5/cape_2010.nc').cape.sel(latitude=slice(65,40)) # hourly CAPE from ERA5\n",
    "\n",
    "# Calculation of 500-hPa saturation MSE\n",
    "t500 = xr.open_dataset('/global/cscratch1/sd/y-zhang/ERA5/t500_2010.nc').t500 # hourly 500-hPa temperature from ERA5\n",
    "z500 = xr.open_dataset('/global/cscratch1/sd/y-zhang/ERA5/z500_2010.nc').z500/1e3 # hourly 500-hPa geopotential from ERA5\n",
    "hsat500 = cp*t500+L*e2q(e_sat(t500), 50000)+z500\n",
    "\n",
    "# Calculaiton of 2-m MSE\n",
    "gzs = xr.open_dataset('/global/homes/y/y-zhang/cmip6/ERA5/e5.oper.invariant/197901/e5.oper.invariant.128_129_z.ll025sc.1979010100_1979010100.nc').Z.squeeze()/1e3\n",
    "h2m = cp*t2m+L*e2q(e_sat(d2m), sp)+gzs\n",
    "h2m = xr.open_dataset('/global/cscratch1/sd/y-zhang/ERA5/h2m_2010.nc').h2m.sel(latitude=slice(65,40))\n",
    "\n",
    "from numba import jit\n",
    "length=481\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _lag_ufunc(da1, da2): \n",
    "    result = np.ones(length)*np.nan \n",
    "    result[max((length-1)/2-da1.argmax(),0):min((length-1)/2-da1.argmax()+len(da1),length)] = da2[max(da1.argmax()-(length-1)/2,0):min(da1.argmax()+(length+1)/2,len(da1))]\n",
    "    return result\n",
    "                    \n",
    "def xr_lag(da1, da2, convert_to_dataset=True, dim='time'):\n",
    "    result = xr.apply_ufunc(_lag_ufunc, da1,da2,\n",
    "                            input_core_dims=[[dim],[dim]],\n",
    "                            dask ='parallelized',\n",
    "                            vectorize=True,\n",
    "                            output_dtypes=[np.float64], \n",
    "                            output_core_dims=[['lag']], \n",
    "                            output_sizes={'lag':length})\n",
    "    result['lag'] = np.arange(-(length-1)/2,(length+1)/2)/24\n",
    "    return result\n",
    "\n",
    "\n",
    "def xr_lag_year(ds):    \n",
    "    return xr_lag(ds[x], ds[y])\n",
    "\n",
    "d = xr.Dataset({'TX': t2m})\n",
    "x='TX'\n",
    "y='TX'\n",
    "TX_TX_year = d.groupby('time.year').apply(xr_lag_year).compute()\n",
    "\n",
    "d = xr.Dataset({'TX': t2m, 'cape': cape})\n",
    "x='TX'\n",
    "y='cape'\n",
    "cape_TX_year = d.groupby('time.year').apply(xr_lag_year).compute()\n",
    "\n",
    "d = xr.Dataset({'TX': t2m, 'h2m': h2m})\n",
    "x='TX'\n",
    "y='h2m'\n",
    "h2m_TX_year = d.groupby('time.year').apply(xr_lag_year).compute()\n",
    "\n",
    "d = xr.Dataset({'TX': t2m, 'hsat500': hsat500})\n",
    "x='TX'\n",
    "y='hsat500'\n",
    "hsat500_TX_year = d.groupby('time.year').apply(xr_lag_year).compute()\n",
    "\n",
    "d = xr.Dataset({'TX': t2m, 'pr': pr})\n",
    "x='TX'\n",
    "y='pr'\n",
    "pr_TX_year = d.groupby('time.year').apply(xr_lag_year).compute()\n",
    "pr_TX_year = pr_TX_year.sel(lag=slice(-10/24,10/24))\n",
    "pr_TX_year = pr_TX_year.assign_coords(lag = np.arange(-10,10.01,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f3dcc-6b27-481c-8dc3-4bd2f4f2be37",
   "metadata": {},
   "source": [
    "------------\n",
    "### Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b1af6-ca5b-4402-aa6f-d90efe9c8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t500 = xr.open_mfdataset('/global/cscratch1/sd/y-zhang/ERA5/t500_*.nc').t500me.sel(time=slice('2001','2020'))\n",
    "TX = xr.open_mfdataset('/global/cscratch1/sd/y-zhang/ERA5/TX_*.nc').TX.sel(time=slice('2001','2020'))\n",
    "sm = xr.open_mfdataset('/global/cscratch1/sd/y-zhang/ERA5/swvl1_30daybefore_*.nc').swvl1.sel(time=slice('2001','2020'))\n",
    "vo = xr.open_mfdataset('/global/cscratch1/sd/y-zhang/ERA5/500hpa_vorticity_*_summer-daily-mean.nc').vome.sel(time=slice('2001','2020')).isel(expver=0)\n",
    "gzs = xr.open_dataset('/global/homes/y/y-zhang/cmip6/ERA5/e5.oper.invariant/197901/e5.oper.invariant.128_129_z.ll025sc.1979010100_1979010100.nc').Z.squeeze()/1e3\n",
    "\n",
    "@jit\n",
    "def _histogram2d_ufunc(x,y):\n",
    "    h2d, x_bins, y_bins = np.histogram2d(x.ravel(),y.ravel(), bins = (np.arange(200,284.01,0.25), np.arange(210,345.01,0.5)))\n",
    "    return h2d\n",
    "\n",
    "   \n",
    "def xr_histogram2d(x,y): # 2-D histogram \n",
    "    h2d = xr.apply_ufunc(_histogram2d_ufunc, x, y, \n",
    "                             input_core_dims=[['latitude','longitude'],['latitude','longitude']],\n",
    "                            dask ='parallelized',\n",
    "                            vectorize=True,\n",
    "                            output_dtypes=[x.dtype], \n",
    "                          output_core_dims = [['x_bins','y_bins']],\n",
    "                         output_sizes = {'x_bins': len(np.arange(200,284.01,0.25)), \n",
    "                                         'y_bins': len(np.arange(210,345.01,0.5))})\n",
    "\n",
    "    return xr.DataArray(data = h2d, dims=['time', 'x_bins', 'y_bins'], \n",
    "                       coords={'x_bins': np.arange(200,284.01,0.25)[0:-1]+0.125, \n",
    "                              'y_bins': np.arange(210,345.01,0.5)[0:-1]+0.25,\n",
    "                              'time': x.time.values})\n",
    "\n",
    "@jit\n",
    "def _histogram2d_weight_ufunc(x,y,z):\n",
    "    h2d, x_bins, y_bins = np.histogram2d(x.ravel(),y.ravel(), \n",
    "                                         bins = (np.arange(200,284.01,0.25), np.arange(210,345.01,0.5))\n",
    "                                        ,weights=z.ravel())\n",
    "    return h2d\n",
    "\n",
    "   \n",
    "def xr_histogram2d_weight(x,y,z): # weighted 2-D histogram; For example, weighting by soil moisture gives the sum of soil moisture in each bin, then dividing by the count in each bin gives mean soil moisture in each bin\n",
    "    h2d = xr.apply_ufunc(_histogram2d_weight_ufunc, x, y, z,\n",
    "                             input_core_dims=[['latitude','longitude'],['latitude','longitude'],['latitude','longitude']],\n",
    "                            dask ='parallelized',\n",
    "                            vectorize=True,\n",
    "                            output_dtypes=[x.dtype], \n",
    "                          output_core_dims = [['x_bins','y_bins']],\n",
    "                         output_sizes = {'x_bins': len(np.arange(200,284.01,0.25)), \n",
    "                                         'y_bins': len(np.arange(210,345.01,0.5))})\n",
    "\n",
    "    return xr.DataArray(data = h2d, dims=['time', 'x_bins', 'y_bins'], \n",
    "                       coords={'x_bins': np.arange(200,284.01,0.25)[0:-1]+0.125, \n",
    "                              'y_bins': np.arange(210,345.01,0.5)[0:-1]+0.25,\n",
    "                              'time': x.time.values})\n",
    "\n",
    "h2d_era5 = xr_histogram2d(t500.sel(latitude=slice(65,40))*land, \n",
    "                     (TX+gzs/cp).sel(latitude=slice(65,40))*land)\n",
    "xr.Dataset({'histogram': h2d_era5}).to_netcdf('/global/cscratch1/sd/y-zhang/ERA5/histogram2d_TX_t500_1979-2021.nc')\n",
    "\n",
    "# Note that all data are interpolated to HadGHCND grid first\n",
    "TX_obs = xr.open_dataset('/global/cscratch1/sd/y-zhang/Had/HadGHCND_TX_ERA5.nc').TX\n",
    "t500_obs = xr.open_dataset('/global/cscratch1/sd/y-zhang/AIRS/t500_AIRS.nc').t500.sel(time=TX_obs.time)\n",
    "gzs_obs = xr.open_dataset('/global/cscratch1/sd/y-zhang/Had/gzs_ERA5.nc').gzs\n",
    "h2d_obs = xr_histogram2d(t500_obs.sel(latitude=slice(65,40))*land, \n",
    "                     (TX_obs+gzs_obs/cp).sel(latitude=slice(65,40))*land)\n",
    "xr.Dataset({'histogram': h2d_obs}).to_netcdf('/global/cscratch1/sd/y-zhang/Had/histogram2d_TX_t500_Had_AIRS_2003-2014.nc')\n",
    "\n",
    "\n",
    "h2d_sm = xr_histogram2d_weight(t500.sel(latitude=slice(65,40), time=t500['time.season']=='JJA')*land, \n",
    "                               (TX+gzs/cp).sel(latitude=slice(65,40), time=TX['time.season']=='JJA')*land,\n",
    "                               sm.sel(latitude=slice(65,40), time=sm['time.season']=='JJA')*land)\n",
    "xr.Dataset({'histogram': h2d_sm}).to_netcdf('/global/cscratch1/sd/y-zhang/Had/histogram2dweighted_sm_TX_t500_2001-2020.nc')\n",
    "\n",
    "h2d_vo = xr_histogram2d_weight(t500.sel(latitude=slice(65,40), time=t500['time.season']=='JJA')*land, \n",
    "                               (TX+gzs/cp).sel(latitude=slice(65,40), time=TX['time.season']=='JJA')*land,\n",
    "                               vo.sel(latitude=slice(65,40), time=vo['time.season']=='JJA')*land)\n",
    "xr.Dataset({'histogram': h2d_vo}).to_netcdf('/global/cscratch1/sd/y-zhang/Had/histogram2dweighted_vo_TX_t500_2001-2020.nc')\n",
    "\n",
    "t500_pn = t500.sel(latitude=lat_slice_pn, longitude=lon_slice_pn).weighted(weights).mean(['latitude','longitude']).compute()\n",
    "t500_fr = t500.sel(latitude=lat_slice_fr, longitude=lon_slice_fr).weighted(weights).mean(['latitude','longitude']).compute()\n",
    "t500_rs = t500.sel(latitude=lat_slice_rs, longitude=lon_slice_rs).weighted(weights).mean(['latitude','longitude']).compute()\n",
    "TX_pn = TX.sel(latitude=lat_slice_pn, longitude=lon_slice_pn).weighted(weights).mean(['latitude','longitude']).compute()\n",
    "TX_fr = TX.sel(latitude=lat_slice_fr, longitude=lon_slice_fr).weighted(weights).mean(['latitude','longitude']).compute()\n",
    "TX_rs = TX.sel(latitude=lat_slice_rs, longitude=lon_slice_rs).weighted(weights).mean(['latitude','longitude']).compute()\n",
    "gzs_pn = gzs.sel(latitude=lat_slice_pn, longitude=lon_slice_pn).weighted(weights).mean(['latitude','longitude']).values\n",
    "gzs_fr = gzs.sel(latitude=lat_slice_fr, longitude=lon_slice_fr).weighted(weights).mean(['latitude','longitude']).values\n",
    "gzs_rs = gzs.sel(latitude=lat_slice_rs, longitude=lon_slice_rs).weighted(weights).mean(['latitude','longitude']).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15714b30-4971-4a2e-9ef9-017caa236cf3",
   "metadata": {},
   "source": [
    "------------\n",
    "### Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a36db-1851-453e-94a0-43d0fa870fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def _of_txx_ufunc(t2m,y):    \n",
    "    return y[np.argmax(t2m)]\n",
    "\n",
    "   \n",
    "def xr_of_txx(t2m,y): # Find the level of a variable on the annual hottest day for each location\n",
    "    return xr.apply_ufunc(_at_txx_ufunc, t2m, y, \n",
    "                             input_core_dims=[['time'],['time']],\n",
    "                            dask ='parallelized',\n",
    "                            vectorize=True,\n",
    "                            output_dtypes=[y.dtype])\n",
    " \n",
    "    \n",
    "# T500 on the annual hottest days\n",
    "for year in np.arange(1979,2022):\n",
    "    t500 = xr.open_dataset('/global/cscratch1/sd/y-zhang/ERA5/t500_'+str(year)+'.nc').t500me\n",
    "    TX = xr.open_dataset('/global/cscratch1/sd/y-zhang/ERA5/TX_'+str(year)+'.nc').TX\n",
    "    \n",
    "    res = xr_of_txx(TX, t500).compute()\n",
    "    xr.Dataset({'t500': res}).to_netcdf('/global/cscratch1/sd/y-zhang/ERA5/t500_of_TXx_'+str(year)+'.nc')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aos",
   "language": "python",
   "name": "aos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
